{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 | Timesteps 47 | Avg Loss: 943.8116 | Actor Loss: -0.0132 | Critic Loss: 943.8305 | Entropy: 5.6750\n",
      "Episode 1 | Timesteps 1000 | Avg Loss: 34.3401 | Actor Loss: 0.0156 | Critic Loss: 34.3301 | Entropy: 5.6756\n",
      "Episode 2 | Timesteps 3000 | Avg Loss: 1.6720 | Actor Loss: 0.0143 | Critic Loss: 1.6634 | Entropy: 5.6723\n",
      "Episode 3 | Timesteps 5000 | Avg Loss: 0.1187 | Actor Loss: -0.0267 | Critic Loss: 0.1510 | Entropy: 5.6485\n",
      "Episode 4 | Timesteps 6944 | Avg Loss: 15.0784 | Actor Loss: -0.0018 | Critic Loss: 15.0859 | Entropy: 5.6334\n",
      "Episode 5 | Timesteps 7003 | Avg Loss: 411.8330 | Actor Loss: 0.1043 | Critic Loss: 411.7344 | Entropy: 5.6330\n",
      "Episode 6 | Timesteps 9003 | Avg Loss: 1.6675 | Actor Loss: -0.0159 | Critic Loss: 1.6890 | Entropy: 5.6250\n",
      "Episode 7 | Timesteps 9070 | Avg Loss: 366.1867 | Actor Loss: 0.1499 | Critic Loss: 366.0424 | Entropy: 5.6036\n",
      "Episode 8 | Timesteps 11070 | Avg Loss: 0.1154 | Actor Loss: -0.0261 | Critic Loss: 0.1471 | Entropy: 5.5912\n",
      "Episode 9 | Timesteps 11159 | Avg Loss: 228.3021 | Actor Loss: 0.1054 | Critic Loss: 228.2023 | Entropy: 5.5729\n",
      "Episode 10 | Average Reward (last 10 episodes): -122.18 | Average Length: 1115.90\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal, TransformedDistribution\n",
    "from torch.distributions.transforms import TanhTransform\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.init as init\n",
    "\n",
    "# Adjusted Hyperparameters\n",
    "ENV_NAME = 'BipedalWalkerHardcore-v3'\n",
    "HIDDEN_SIZE = 512\n",
    "LEARNING_RATE = 1e-4\n",
    "GAMMA = 0.99\n",
    "LAMBDA = 0.95\n",
    "CLIP_EPSILON = 0.2\n",
    "ENTROPY_COEF = 0.001\n",
    "VALUE_LOSS_COEF = 0.5\n",
    "MAX_GRAD_NORM = 0.5\n",
    "PPO_EPOCHS = 15\n",
    "MINI_BATCH_SIZE = 256\n",
    "TOTAL_EPISODES = 10000\n",
    "ROLLOUT_LENGTH = 4096\n",
    "EVAL_INTERVAL = 100       # Evaluate every 100 episodes\n",
    "EVAL_EPISODES = 5\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 10\n",
    "min_delta = 1e-3\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Initialize the environment\n",
    "env = gym.make(ENV_NAME)\n",
    "eval_env = gym.make(ENV_NAME, render_mode='human')\n",
    "\n",
    "env.action_space.seed(seed)\n",
    "eval_env.action_space.seed(seed + 1)\n",
    "\n",
    "obs_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.shape[0]\n",
    "\n",
    "# Running Mean and Std for observations\n",
    "class RunningMeanStd:\n",
    "    def __init__(self, epsilon=1e-4, shape=()):\n",
    "        self.mean = torch.zeros(shape, dtype=torch.float64).to(device)\n",
    "        self.var = torch.ones(shape, dtype=torch.float64).to(device)\n",
    "        self.count = epsilon\n",
    "\n",
    "    def update(self, x):\n",
    "        x = x.to(torch.float64)\n",
    "        batch_mean = torch.mean(x, dim=0)\n",
    "        batch_var = torch.var(x, dim=0)\n",
    "        batch_count = x.size(0)\n",
    "        self.update_from_moments(batch_mean, batch_var, batch_count)\n",
    "\n",
    "    def update_from_moments(self, batch_mean, batch_var, batch_count):\n",
    "        delta = batch_mean - self.mean\n",
    "        total_count = self.count + batch_count\n",
    "\n",
    "        new_mean = self.mean + delta * batch_count / total_count\n",
    "        m_a = self.var * self.count\n",
    "        m_b = batch_var * batch_count\n",
    "        M2 = m_a + m_b + delta ** 2 * self.count * batch_count / total_count\n",
    "        new_var = M2 / total_count\n",
    "\n",
    "        self.mean = new_mean\n",
    "        self.var = new_var\n",
    "        self.count = total_count\n",
    "\n",
    "    def normalize(self, x):\n",
    "        x = x.to(torch.float64)\n",
    "        return (x - self.mean) / (torch.sqrt(self.var) + 1e-8)\n",
    "\n",
    "obs_rms = RunningMeanStd(shape=obs_size)\n",
    "\n",
    "# Define the Actor-Critic Network\n",
    "class TanhNormal(TransformedDistribution):\n",
    "    def __init__(self, loc, scale):\n",
    "        self.normal = Normal(loc, scale)\n",
    "        transforms = [TanhTransform(cache_size=1)]\n",
    "        super(TanhNormal, self).__init__(self.normal, transforms)\n",
    "        self.loc = loc\n",
    "        self.scale = scale\n",
    "\n",
    "    @property\n",
    "    def mean(self):\n",
    "        mu = self.normal.mean\n",
    "        for transform in self.transforms:\n",
    "            mu = transform(mu)\n",
    "        return mu\n",
    "\n",
    "    def entropy(self):\n",
    "        return self.base_dist.entropy()\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, obs_size, action_size):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        # Common network\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(obs_size, HIDDEN_SIZE),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        # Actor network\n",
    "        self.actor_mean = nn.Linear(HIDDEN_SIZE, action_size)\n",
    "        # Initialize weights orthogonally\n",
    "        for layer in self.modules():\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                init.orthogonal_(layer.weight, gain=init.calculate_gain('relu'))\n",
    "                init.zeros_(layer.bias)\n",
    "        # Actor log_std (learned)\n",
    "        self.actor_log_std = nn.Parameter(torch.zeros(action_size))\n",
    "        # Critic network\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(HIDDEN_SIZE, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        shared_out = self.shared(x)\n",
    "        # Actor\n",
    "        mean = self.actor_mean(shared_out)\n",
    "        std = self.actor_log_std.exp().expand_as(mean)\n",
    "        dist = TanhNormal(mean, std)\n",
    "        # Critic\n",
    "        value = self.critic(shared_out)\n",
    "        return dist, value\n",
    "\n",
    "# Initialize the network and optimizer\n",
    "model = ActorCritic(obs_size, action_size).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Learning rate scheduler\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=lambda epoch: 1 - epoch / TOTAL_EPISODES)\n",
    "\n",
    "# Storage for rollouts\n",
    "class RolloutBuffer:\n",
    "    def __init__(self):\n",
    "        self.obs = []\n",
    "        self.actions = []\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        self.values = []\n",
    "\n",
    "    def clear(self):\n",
    "        self.obs = []\n",
    "        self.actions = []\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        self.values = []\n",
    "\n",
    "buffer = RolloutBuffer()\n",
    "\n",
    "# Function to compute Generalized Advantage Estimation (GAE)\n",
    "def compute_gae(next_value, rewards, dones, values):\n",
    "    values = values + [next_value]\n",
    "    gae = 0\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        delta = rewards[step] + GAMMA * values[step + 1] * (1 - dones[step]) - values[step]\n",
    "        gae = delta + GAMMA * LAMBDA * (1 - dones[step]) * gae\n",
    "        returns.insert(0, gae + values[step])\n",
    "    return returns\n",
    "\n",
    "# Function to evaluate the agent\n",
    "def evaluate_policy(model, eval_env, episodes=5):\n",
    "    model.eval()\n",
    "    total_rewards = []\n",
    "    for episode in range(episodes):\n",
    "        state, info = eval_env.reset(seed=seed + episode)\n",
    "        state = torch.FloatTensor(state).to(device)\n",
    "        state = obs_rms.normalize(state).to(torch.float32)\n",
    "        terminated = truncated = False\n",
    "        episode_reward = 0\n",
    "        while not (terminated or truncated):\n",
    "            with torch.no_grad():\n",
    "                dist, _ = model(state)\n",
    "                action = dist.mean\n",
    "            # Step the environment\n",
    "            next_state, reward, terminated, truncated, _ = eval_env.step(action.detach().cpu().numpy())\n",
    "            next_state = torch.FloatTensor(next_state).to(device)\n",
    "            state = obs_rms.normalize(next_state).to(torch.float32)\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            time.sleep(0.01)\n",
    "        total_rewards.append(episode_reward)\n",
    "    model.train()\n",
    "    avg_reward = np.mean(total_rewards)\n",
    "    print(f\"Evaluation over {episodes} episodes: Average Reward = {avg_reward}\")\n",
    "    return avg_reward\n",
    "\n",
    "# Initialize variables for early stopping and tracking\n",
    "best_avg_reward = -np.inf\n",
    "no_improvement_counter = 0\n",
    "all_episode_rewards = []\n",
    "all_episode_lengths = []\n",
    "all_losses = []\n",
    "all_actor_losses = []\n",
    "all_critic_losses = []\n",
    "all_entropies = []\n",
    "all_avg_rewards = []\n",
    "episode_rewards = []\n",
    "episode_lengths = []\n",
    "total_timesteps = 0\n",
    "next_eval = EVAL_INTERVAL\n",
    "episode_count = 0\n",
    "\n",
    "while episode_count < TOTAL_EPISODES:\n",
    "    # Reset the environment and get the initial state\n",
    "    state, info = env.reset(seed=seed + episode_count)\n",
    "    state = torch.FloatTensor(state).to(device)\n",
    "    state = obs_rms.normalize(state).to(torch.float32)\n",
    "    episode_reward = 0\n",
    "    episode_length = 0\n",
    "    terminated = truncated = False\n",
    "\n",
    "    while not (terminated or truncated):\n",
    "        dist, value = model(state)\n",
    "        action = dist.rsample()\n",
    "        log_prob = dist.log_prob(action).sum(dim=-1)\n",
    "        # Step the environment\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action.detach().cpu().numpy())\n",
    "        next_state = torch.FloatTensor(next_state).to(device)\n",
    "        next_state = obs_rms.normalize(next_state).to(torch.float32)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # Store experience in buffer (detach tensors)\n",
    "        buffer.obs.append(state)\n",
    "        buffer.actions.append(action.detach())\n",
    "        buffer.log_probs.append(log_prob.detach())\n",
    "        buffer.rewards.append(reward)\n",
    "        buffer.dones.append(done)\n",
    "        buffer.values.append(value.detach().squeeze())\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "        episode_length += 1\n",
    "        total_timesteps += 1\n",
    "\n",
    "        # Check if it's time to update the policy\n",
    "        if len(buffer.rewards) >= ROLLOUT_LENGTH or done:\n",
    "            # Compute next value\n",
    "            with torch.no_grad():\n",
    "                _, next_value = model(state)\n",
    "            next_value = next_value.detach().squeeze()\n",
    "\n",
    "            # Compute returns and advantages\n",
    "            returns = compute_gae(next_value, buffer.rewards, buffer.dones, buffer.values)\n",
    "            advantages = [ret - val for ret, val in zip(returns, buffer.values)]\n",
    "            advantages = torch.tensor(advantages, dtype=torch.float32).to(device)\n",
    "            returns = torch.tensor(returns, dtype=torch.float32).to(device)\n",
    "\n",
    "            # Update observation normalization\n",
    "            obs_batch = torch.stack(buffer.obs)\n",
    "            obs_rms.update(obs_batch)\n",
    "\n",
    "            # Normalize observations in the buffer\n",
    "            buffer.obs = [obs_rms.normalize(obs).to(torch.float32) for obs in buffer.obs]\n",
    "\n",
    "            # Flatten the buffers\n",
    "            obs_tensor = torch.stack(buffer.obs)\n",
    "            actions_tensor = torch.stack(buffer.actions)\n",
    "            log_probs_tensor = torch.stack(buffer.log_probs)\n",
    "            values_tensor = torch.stack(buffer.values).to(device)\n",
    "            # Clear buffer\n",
    "            buffer.clear()\n",
    "\n",
    "            # Normalize advantages\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "            # PPO Optimization step\n",
    "            total_loss = 0\n",
    "            total_actor_loss = 0\n",
    "            total_critic_loss = 0\n",
    "            total_entropy = 0\n",
    "            num_updates = 0\n",
    "            for _ in range(PPO_EPOCHS):\n",
    "                # Create mini-batches\n",
    "                indices = np.arange(len(obs_tensor))\n",
    "                np.random.shuffle(indices)\n",
    "                for start in range(0, len(obs_tensor), MINI_BATCH_SIZE):\n",
    "                    end = start + MINI_BATCH_SIZE\n",
    "                    mini_batch_indices = indices[start:end]\n",
    "                    mb_obs = obs_tensor[mini_batch_indices]\n",
    "                    mb_actions = actions_tensor[mini_batch_indices]\n",
    "                    mb_log_probs = log_probs_tensor[mini_batch_indices]\n",
    "                    mb_returns = returns[mini_batch_indices]\n",
    "                    mb_advantages = advantages[mini_batch_indices]\n",
    "                    # Forward pass\n",
    "                    dist, value = model(mb_obs)\n",
    "                    # Compute entropy using base distribution\n",
    "                    entropy = dist.base_dist.entropy().sum(dim=-1).mean()\n",
    "                    new_log_probs = dist.log_prob(mb_actions).sum(dim=-1)\n",
    "                    # Ratio for clipping\n",
    "                    ratio = (new_log_probs - mb_log_probs).exp()\n",
    "                    surr1 = ratio * mb_advantages\n",
    "                    surr2 = torch.clamp(ratio, 1.0 - CLIP_EPSILON, 1.0 + CLIP_EPSILON) * mb_advantages\n",
    "                    actor_loss = -torch.min(surr1, surr2).mean()\n",
    "                    critic_loss = VALUE_LOSS_COEF * (mb_returns - value.squeeze()).pow(2).mean()\n",
    "                    loss = actor_loss + critic_loss - ENTROPY_COEF * entropy\n",
    "                    # Backpropagation\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "                    # Accumulate losses\n",
    "                    total_loss += loss.item()\n",
    "                    total_actor_loss += actor_loss.item()\n",
    "                    total_critic_loss += critic_loss.item()\n",
    "                    total_entropy += entropy.item()\n",
    "                    num_updates += 1\n",
    "\n",
    "            # Compute average losses\n",
    "            avg_loss = total_loss / num_updates\n",
    "            avg_actor_loss = total_actor_loss / num_updates\n",
    "            avg_critic_loss = total_critic_loss / num_updates\n",
    "            avg_entropy = total_entropy / num_updates\n",
    "\n",
    "            # Store losses\n",
    "            all_losses.append(avg_loss)\n",
    "            all_actor_losses.append(avg_actor_loss)\n",
    "            all_critic_losses.append(avg_critic_loss)\n",
    "            all_entropies.append(avg_entropy)\n",
    "\n",
    "            # Verbose logging\n",
    "            print(f\"Episode {episode_count} | Timesteps {total_timesteps} | Avg Loss: {avg_loss:.4f} | \"\n",
    "                  f\"Actor Loss: {avg_actor_loss:.4f} | Critic Loss: {avg_critic_loss:.4f} | \"\n",
    "                  f\"Entropy: {avg_entropy:.4f}\")\n",
    "            break  # Exit loop after policy update\n",
    "\n",
    "    episode_rewards.append(episode_reward)\n",
    "    episode_lengths.append(episode_length)\n",
    "    episode_count += 1\n",
    "\n",
    "    # Print average reward every 10 episodes\n",
    "    if episode_count % 10 == 0:\n",
    "        avg_reward = np.mean(episode_rewards[-10:])\n",
    "        avg_length = np.mean(episode_lengths[-10:])\n",
    "        print(f\"Episode {episode_count} | Average Reward (last 10 episodes): {avg_reward:.2f} | \"\n",
    "              f\"Average Length: {avg_length:.2f}\")\n",
    "\n",
    "    # Evaluate the agent periodically\n",
    "    if episode_count % EVAL_INTERVAL == 0:\n",
    "        print(f\"\\nEvaluating at episode {episode_count}...\")\n",
    "        avg_reward = evaluate_policy(model, eval_env, episodes=EVAL_EPISODES)\n",
    "        all_avg_rewards.append(avg_reward)\n",
    "\n",
    "        # Early stopping and model saving\n",
    "        if avg_reward > best_avg_reward + min_delta:\n",
    "            best_avg_reward = avg_reward\n",
    "            no_improvement_counter = 0\n",
    "            # Save the model\n",
    "            torch.save(model.state_dict(), f'best_model_episode_{episode_count}.pth')\n",
    "            print(f\"Best model saved with average reward {best_avg_reward} at episode {episode_count}\")\n",
    "        else:\n",
    "            no_improvement_counter += 1\n",
    "            print(f\"No improvement for {no_improvement_counter} evaluation(s)\")\n",
    "\n",
    "        if no_improvement_counter >= patience:\n",
    "            print(f\"Early stopping at episode {episode_count} due to no improvement in average reward\")\n",
    "            break\n",
    "        print()\n",
    "\n",
    "env.close()\n",
    "eval_env.close()\n",
    "\n",
    "# Plotting the results\n",
    "episodes = range(len(episode_rewards))\n",
    "\n",
    "# Plot rewards\n",
    "plt.figure()\n",
    "plt.plot(episodes, episode_rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Episode Reward Over Time')\n",
    "plt.savefig('rewards.png')\n",
    "plt.show()\n",
    "\n",
    "# Plot losses\n",
    "plt.figure()\n",
    "plt.plot(range(len(all_losses)), all_losses, label='Total Loss')\n",
    "plt.plot(range(len(all_actor_losses)), all_actor_losses, label='Actor Loss')\n",
    "plt.plot(range(len(all_critic_losses)), all_critic_losses, label='Critic Loss')\n",
    "plt.xlabel('Policy Update')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Losses Over Time')\n",
    "plt.legend()\n",
    "plt.savefig('losses.png')\n",
    "plt.show()\n",
    "\n",
    "# Plot average rewards during evaluation\n",
    "eval_episodes = [EVAL_INTERVAL * i for i in range(1, len(all_avg_rewards)+1)]\n",
    "plt.figure()\n",
    "plt.plot(eval_episodes, all_avg_rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.title('Average Reward During Evaluation')\n",
    "plt.savefig('avg_rewards.png')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
