{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "\n",
    "# Hyperparameters\n",
    "env_name = 'BipedalWalkerHardcore-v3'\n",
    "seed = 123\n",
    "gamma = 0.99\n",
    "lam = 0.95\n",
    "clip_epsilon = 0.2\n",
    "learning_rate = 3e-4\n",
    "value_loss_coef = 0.5\n",
    "entropy_coef = 0.0\n",
    "max_grad_norm = 0.5\n",
    "num_steps_per_update = 2048\n",
    "num_epochs = 10\n",
    "minibatch_size = 64\n",
    "total_timesteps = 1_000_000\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make(env_name)\n",
    "# Seed action and observation spaces\n",
    "env.action_space.seed(seed)\n",
    "env.observation_space.seed(seed)\n",
    "\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "action_high = env.action_space.high\n",
    "action_low = env.action_space.low\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Actor-Critic Network\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, obs_dim, action_dim):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        hidden_size = 256\n",
    "\n",
    "        # Common feature layer\n",
    "        self.feature = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        # Actor network\n",
    "        self.actor_mean = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, action_dim)\n",
    "        )\n",
    "\n",
    "        # Log std parameter (state-independent)\n",
    "        self.log_std = nn.Parameter(torch.zeros(action_dim))\n",
    "\n",
    "        # Critic network\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        feature = self.feature(x)\n",
    "        action_mean = self.actor_mean(feature)\n",
    "        action_log_std = self.log_std.expand_as(action_mean)\n",
    "        value = self.critic(feature)\n",
    "        return action_mean, action_log_std, value\n",
    "\n",
    "# Initialize the network and optimizer\n",
    "model = ActorCritic(obs_dim, action_dim).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Storage for training data\n",
    "class RolloutBuffer:\n",
    "    def __init__(self):\n",
    "        self.observations = []\n",
    "        self.actions = []\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        self.masks = []\n",
    "        self.values = []\n",
    "        self.advantages = []\n",
    "        self.returns = []\n",
    "\n",
    "    def clear(self):\n",
    "        self.__init__()\n",
    "\n",
    "buffer = RolloutBuffer()\n",
    "\n",
    "def collect_trajectories(model, env, num_steps):\n",
    "    global_step = 0\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while global_step < num_steps:\n",
    "        # Convert observation to tensor and add batch dimension\n",
    "        obs_tensor = torch.FloatTensor(obs).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Get action from the policy network\n",
    "            action_mean, action_log_std, value = model(obs_tensor)\n",
    "            action_std = action_log_std.exp()\n",
    "            dist = Normal(action_mean, action_std)\n",
    "            action = dist.sample()\n",
    "            log_prob = dist.log_prob(action).sum(-1)\n",
    "            value = value.squeeze(-1)\n",
    "\n",
    "        # Remove batch dimension and convert to NumPy array\n",
    "        action_np = action.cpu().numpy()[0]\n",
    "        action_clipped = np.clip(action_np, action_low, action_high)\n",
    "\n",
    "        # Step the environment\n",
    "        next_obs, reward, done, truncated, _ = env.step(action_clipped)\n",
    "        mask = 0 if (done or truncated) else 1\n",
    "\n",
    "        # Store data in buffer\n",
    "        buffer.observations.append(obs)\n",
    "        buffer.actions.append(action_np)\n",
    "        buffer.log_probs.append(log_prob.item())\n",
    "        buffer.rewards.append(reward)\n",
    "        buffer.masks.append(mask)\n",
    "        buffer.values.append(value.item())\n",
    "\n",
    "        obs = next_obs\n",
    "        global_step += 1\n",
    "\n",
    "        if done or truncated:\n",
    "            obs, _ = env.reset()\n",
    "            done = False\n",
    "\n",
    "    # Compute the last value\n",
    "    obs_tensor = torch.FloatTensor(obs).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        _, _, last_value = model(obs_tensor)\n",
    "        last_value = last_value.squeeze(-1).item()\n",
    "\n",
    "    # Compute advantages and returns\n",
    "    compute_gae(last_value, buffer)\n",
    "\n",
    "def compute_gae(last_value, buffer):\n",
    "    rewards = buffer.rewards\n",
    "    masks = buffer.masks\n",
    "    values = buffer.values + [last_value]\n",
    "    advantages = []\n",
    "    gae = 0\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        delta = rewards[step] + gamma * values[step + 1] * masks[step] - values[step]\n",
    "        gae = delta + gamma * lam * masks[step] * gae\n",
    "        advantages.insert(0, gae)\n",
    "    buffer.advantages = advantages\n",
    "    buffer.returns = [adv + val for adv, val in zip(advantages, buffer.values)]\n",
    "\n",
    "def ppo_update(model, optimizer, buffer):\n",
    "    observations = torch.FloatTensor(buffer.observations).to(device)\n",
    "    actions = torch.FloatTensor(buffer.actions).to(device)\n",
    "    old_log_probs = torch.FloatTensor(buffer.log_probs).to(device)\n",
    "    returns = torch.FloatTensor(buffer.returns).to(device)\n",
    "    advantages = torch.FloatTensor(buffer.advantages).to(device)\n",
    "\n",
    "    # Normalize advantages\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = torch.utils.data.TensorDataset(\n",
    "        observations, actions, old_log_probs, returns, advantages\n",
    "    )\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=minibatch_size, shuffle=True\n",
    "    )\n",
    "\n",
    "    for _ in range(num_epochs):\n",
    "        for batch in data_loader:\n",
    "            obs_batch, actions_batch, old_log_probs_batch, returns_batch, advantages_batch = batch\n",
    "\n",
    "            # Forward pass\n",
    "            action_mean, action_log_std, value = model(obs_batch)\n",
    "            action_std = action_log_std.exp()\n",
    "            dist = Normal(action_mean, action_std)\n",
    "            log_probs = dist.log_prob(actions_batch).sum(-1)\n",
    "            entropy = dist.entropy().sum(-1)\n",
    "            value = value.squeeze(-1)\n",
    "\n",
    "            # Compute ratios\n",
    "            ratios = torch.exp(log_probs - old_log_probs_batch)\n",
    "\n",
    "            # Compute surrogate losses\n",
    "            surr1 = ratios * advantages_batch\n",
    "            surr2 = torch.clamp(ratios, 1.0 - clip_epsilon, 1.0 + clip_epsilon) * advantages_batch\n",
    "\n",
    "            # Compute actor and critic losses\n",
    "            actor_loss = -torch.min(surr1, surr2).mean()\n",
    "            critic_loss = (returns_batch - value).pow(2).mean()\n",
    "            entropy_loss = -entropy.mean()\n",
    "\n",
    "            # Total loss\n",
    "            loss = actor_loss + value_loss_coef * critic_loss + entropy_coef * entropy_loss\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            optimizer.step()\n",
    "\n",
    "def main():\n",
    "    num_updates = total_timesteps // num_steps_per_update\n",
    "    for update in range(1, num_updates + 1):\n",
    "        # Collect trajectories\n",
    "        buffer.clear()\n",
    "        collect_trajectories(model, env, num_steps_per_update)\n",
    "\n",
    "        # Update the policy\n",
    "        ppo_update(model, optimizer, buffer)\n",
    "\n",
    "        # Logging\n",
    "        if update % 10 == 0:\n",
    "            avg_reward = np.sum(buffer.rewards) / num_steps_per_update\n",
    "            print(f\"Update {update}, Average Reward per Step: {avg_reward:.2f}\")\n",
    "\n",
    "        # Save the model\n",
    "        if update % 50 == 0:\n",
    "            torch.save(model.state_dict(), f\"ppo_bipedalwalker_{update}.pth\")\n",
    "\n",
    "    # Save the final model\n",
    "    torch.save(model.state_dict(), \"ppo_bipedalwalker_final.pth\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "    # Test the trained model\n",
    "    model.load_state_dict(torch.load(\"ppo_bipedalwalker_final.pth\"))\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        obs_tensor = torch.FloatTensor(obs).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            action_mean, _, _ = model(obs_tensor)\n",
    "            action_np = action_mean.cpu().numpy()[0]\n",
    "        action_clipped = np.clip(action_np, action_low, action_high)\n",
    "        obs, reward, done, truncated, _ = env.step(action_clipped)\n",
    "        done = done or truncated\n",
    "        env.render()\n",
    "    env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
