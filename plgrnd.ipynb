{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\n",
      "Reward: -107.81\n",
      "Average Reward: -107.81\n",
      "--------------------------------------------------\n",
      "Evaluation Reward: -116.40\n",
      "Episode 2\n",
      "Reward: -127.53\n",
      "Average Reward: -117.67\n",
      "--------------------------------------------------\n",
      "Episode 3\n",
      "Reward: -113.95\n",
      "Average Reward: -116.43\n",
      "--------------------------------------------------\n",
      "Episode 4\n",
      "Reward: -106.04\n",
      "Average Reward: -113.83\n",
      "--------------------------------------------------\n",
      "Episode 5\n",
      "Reward: -107.97\n",
      "Average Reward: -112.66\n",
      "--------------------------------------------------\n",
      "Episode 6\n",
      "Reward: -100.51\n",
      "Average Reward: -110.63\n",
      "--------------------------------------------------\n",
      "Episode 7\n",
      "Reward: -129.39\n",
      "Average Reward: -113.31\n",
      "--------------------------------------------------\n",
      "Episode 8\n",
      "Reward: -113.95\n",
      "Average Reward: -113.39\n",
      "--------------------------------------------------\n",
      "Episode 9\n",
      "Reward: -104.02\n",
      "Average Reward: -112.35\n",
      "--------------------------------------------------\n",
      "Episode 10\n",
      "Reward: -99.52\n",
      "Average Reward: -111.07\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Normal\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# Actor Network for continuous actions\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.mean_layer = nn.Linear(256, action_dim)\n",
    "        self.log_std_layer = nn.Linear(256, action_dim)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.orthogonal_(module.weight, 1.0)\n",
    "            module.bias.data.zero_()\n",
    "    \n",
    "    def forward(self, state):\n",
    "        x = self.net(state)\n",
    "        mean = torch.tanh(self.mean_layer(x))  # Bound mean to [-1,1]\n",
    "        log_std = self.log_std_layer(x)\n",
    "        log_std = torch.clamp(log_std, -20, 2)  # Prevent too small or large std\n",
    "        return mean, log_std.exp()\n",
    "\n",
    "# Critic Network\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.orthogonal_(module.weight, 1.0)\n",
    "            module.bias.data.zero_()\n",
    "    \n",
    "    def forward(self, state):\n",
    "        return self.net(state)\n",
    "\n",
    "class PPOAgent:\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.actor = Actor(state_dim, action_dim)\n",
    "        self.critic = Critic(state_dim)\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=3e-4)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=3e-4)\n",
    "        \n",
    "        # PPO hyperparameters\n",
    "        self.gamma = 0.99\n",
    "        self.gae_lambda = 0.95\n",
    "        self.clip_epsilon = 0.2\n",
    "        self.entropy_coef = 0.01\n",
    "        self.value_clip_range = 0.2\n",
    "        self.max_grad_norm = 0.5\n",
    "        \n",
    "        # Initialize buffers\n",
    "        self.reset_buffers()\n",
    "        \n",
    "        # Training metrics\n",
    "        self.rewards_history = []\n",
    "        self.avg_rewards_history = []\n",
    "    \n",
    "    def reset_buffers(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.values = []\n",
    "        self.log_probs = []\n",
    "        self.dones = []\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        with torch.no_grad():\n",
    "            state = torch.FloatTensor(state)\n",
    "            mean, std = self.actor(state)\n",
    "            dist = Normal(mean, std)\n",
    "            action = dist.sample()\n",
    "            action = torch.clamp(action, -1.0, 1.0)\n",
    "            log_prob = dist.log_prob(action).sum(dim=-1)\n",
    "            value = self.critic(state)\n",
    "            return action.numpy(), value.item(), log_prob.item()\n",
    "    \n",
    "    def train_episode(self, env, render=False):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            # Get action\n",
    "            action, value, log_prob = self.get_action(state)\n",
    "            \n",
    "            # Take step in environment\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # Store transition\n",
    "            self.states.append(state)\n",
    "            self.actions.append(action)\n",
    "            self.rewards.append(reward)\n",
    "            self.values.append(value)\n",
    "            self.log_probs.append(log_prob)\n",
    "            self.dones.append(done)\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "        \n",
    "        return total_reward\n",
    "    \n",
    "    def update(self, batch_size=64):\n",
    "        # Convert buffers to tensors\n",
    "        states = torch.FloatTensor(np.array(self.states))\n",
    "        actions = torch.FloatTensor(np.array(self.actions))\n",
    "        old_log_probs = torch.FloatTensor(self.log_probs)\n",
    "        \n",
    "        # Compute GAE and returns\n",
    "        advantages = torch.zeros_like(torch.FloatTensor(self.rewards))\n",
    "        returns = torch.zeros_like(torch.FloatTensor(self.rewards))\n",
    "        \n",
    "        running_return = 0\n",
    "        running_advantage = 0\n",
    "        \n",
    "        for t in reversed(range(len(self.rewards))):\n",
    "            if t == len(self.rewards) - 1:\n",
    "                next_value = 0\n",
    "            else:\n",
    "                next_value = self.values[t + 1]\n",
    "            \n",
    "            running_return = self.rewards[t] + self.gamma * running_return * (1 - self.dones[t])\n",
    "            delta = self.rewards[t] + self.gamma * next_value * (1 - self.dones[t]) - self.values[t]\n",
    "            running_advantage = delta + self.gamma * self.gae_lambda * running_advantage * (1 - self.dones[t])\n",
    "            \n",
    "            returns[t] = running_return\n",
    "            advantages[t] = running_advantage\n",
    "        \n",
    "        # Normalize advantages\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        # PPO update\n",
    "        for _ in range(10):  # Number of epochs\n",
    "            # Generate random indices\n",
    "            indices = torch.randperm(len(states))\n",
    "            \n",
    "            # Mini-batch update\n",
    "            for start in range(0, len(states), batch_size):\n",
    "                end = start + batch_size\n",
    "                batch_indices = indices[start:end]\n",
    "                \n",
    "                # Get batch data\n",
    "                batch_states = states[batch_indices]\n",
    "                batch_actions = actions[batch_indices]\n",
    "                batch_log_probs = old_log_probs[batch_indices]\n",
    "                batch_advantages = advantages[batch_indices]\n",
    "                batch_returns = returns[batch_indices]\n",
    "                \n",
    "                # Get current policy distribution\n",
    "                means, stds = self.actor(batch_states)\n",
    "                dist = Normal(means, stds)\n",
    "                current_log_probs = dist.log_prob(batch_actions).sum(dim=-1)\n",
    "                entropy = dist.entropy().mean()\n",
    "                \n",
    "                # Compute policy loss\n",
    "                ratios = torch.exp(current_log_probs - batch_log_probs)\n",
    "                surr1 = ratios * batch_advantages\n",
    "                surr2 = torch.clamp(ratios, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * batch_advantages\n",
    "                policy_loss = -torch.min(surr1, surr2).mean()\n",
    "                \n",
    "                # Compute value loss\n",
    "                values = self.critic(batch_states).squeeze()\n",
    "                value_loss = 0.5 * ((values - batch_returns) ** 2).mean()\n",
    "                \n",
    "                # Total loss\n",
    "                loss = policy_loss + 0.5 * value_loss - self.entropy_coef * entropy\n",
    "                \n",
    "                # Update networks\n",
    "                self.actor_optimizer.zero_grad()\n",
    "                self.critic_optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                \n",
    "                # Clip gradients\n",
    "                torch.nn.utils.clip_grad_norm_(self.actor.parameters(), self.max_grad_norm)\n",
    "                torch.nn.utils.clip_grad_norm_(self.critic.parameters(), self.max_grad_norm)\n",
    "                \n",
    "                self.actor_optimizer.step()\n",
    "                self.critic_optimizer.step()\n",
    "        \n",
    "        self.reset_buffers()\n",
    "\n",
    "def plot_training_progress(rewards, avg_rewards, title=\"Training Progress\"):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(rewards, alpha=0.5, label='Rewards')\n",
    "    plt.plot(avg_rewards, label='Average Rewards')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('bipedal_training_progress.png')\n",
    "    plt.close()\n",
    "\n",
    "def train(env_name='BipedalWalker-v3', max_episodes=1000, render_freq=10):\n",
    "    env = gym.make(env_name)\n",
    "    eval_env = gym.make(env_name, render_mode='human')\n",
    "    \n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    \n",
    "    agent = PPOAgent(state_dim, action_dim)\n",
    "    \n",
    "    # Training loop\n",
    "    best_reward = float('-inf')\n",
    "    \n",
    "    try:\n",
    "        for episode in range(max_episodes):\n",
    "            # Training episode\n",
    "            total_reward = agent.train_episode(env)\n",
    "            agent.rewards_history.append(total_reward)\n",
    "            \n",
    "            # Calculate average reward\n",
    "            avg_reward = np.mean(agent.rewards_history[-100:])\n",
    "            agent.avg_rewards_history.append(avg_reward)\n",
    "            \n",
    "            # Update policy\n",
    "            agent.update()\n",
    "            \n",
    "            # Print progress\n",
    "            print(f\"Episode {episode + 1}\")\n",
    "            print(f\"Reward: {total_reward:.2f}\")\n",
    "            print(f\"Average Reward: {avg_reward:.2f}\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            # Plot progress\n",
    "            if episode % 10 == 0:\n",
    "                plot_training_progress(agent.rewards_history, agent.avg_rewards_history)\n",
    "            \n",
    "            # Render episode\n",
    "            if episode % render_freq == 0:\n",
    "                eval_reward = agent.train_episode(eval_env)\n",
    "                print(f\"Evaluation Reward: {eval_reward:.2f}\")\n",
    "            \n",
    "            # Save best model\n",
    "            if avg_reward > best_reward:\n",
    "                best_reward = avg_reward\n",
    "                torch.save({\n",
    "                    'actor_state_dict': agent.actor.state_dict(),\n",
    "                    'critic_state_dict': agent.critic.state_dict(),\n",
    "                    'reward': best_reward\n",
    "                }, 'best_bipedal_model.pth')\n",
    "            \n",
    "            # Early stopping criterion\n",
    "            if avg_reward > 300:  # BipedalWalker is considered solved at 300\n",
    "                print(f\"Environment solved in {episode + 1} episodes!\")\n",
    "                break\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nTraining interrupted by user\")\n",
    "    \n",
    "    env.close()\n",
    "    eval_env.close()\n",
    "    return agent\n",
    "\n",
    "def evaluate(agent, env_name='BipedalWalker-v3', episodes=5):\n",
    "    env = gym.make(env_name, render_mode='human')\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action, _, _ = agent.get_action(state)\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            total_reward += reward\n",
    "        \n",
    "        print(f\"Evaluation Episode {episode + 1}: Reward = {total_reward:.2f}\")\n",
    "    \n",
    "    env.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(0)\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    # Train agent\n",
    "    agent = train(render_freq=10)  # Render every 10 episodes\n",
    "    \n",
    "    # Evaluate trained agent\n",
    "    evaluate(agent)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
