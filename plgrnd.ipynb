{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mhfar\\AppData\\Local\\Temp\\ipykernel_12784\\57550191.py:96: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:277.)\n",
      "  states = torch.FloatTensor(states)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\n",
      "Reward: -124.50\n",
      "Average Reward (last 100): -124.50\n",
      "--------------------------------------------------\n",
      "Episode 2\n",
      "Reward: -116.26\n",
      "Average Reward (last 100): -120.38\n",
      "--------------------------------------------------\n",
      "Episode 3\n",
      "Reward: -100.28\n",
      "Average Reward (last 100): -113.68\n",
      "--------------------------------------------------\n",
      "Episode 4\n",
      "Reward: -111.77\n",
      "Average Reward (last 100): -113.20\n",
      "--------------------------------------------------\n",
      "Episode 5\n",
      "Reward: -109.13\n",
      "Average Reward (last 100): -112.39\n",
      "--------------------------------------------------\n",
      "Episode 6\n",
      "Reward: -111.60\n",
      "Average Reward (last 100): -112.26\n",
      "--------------------------------------------------\n",
      "Episode 7\n",
      "Reward: -112.87\n",
      "Average Reward (last 100): -112.34\n",
      "--------------------------------------------------\n",
      "Episode 8\n",
      "Reward: -109.17\n",
      "Average Reward (last 100): -111.95\n",
      "--------------------------------------------------\n",
      "Episode 9\n",
      "Reward: -111.70\n",
      "Average Reward (last 100): -111.92\n",
      "--------------------------------------------------\n",
      "Episode 10\n",
      "Reward: -114.36\n",
      "Average Reward (last 100): -112.16\n",
      "--------------------------------------------------\n",
      "Episode 11\n",
      "Reward: -115.31\n",
      "Average Reward (last 100): -112.45\n",
      "--------------------------------------------------\n",
      "Episode 12\n",
      "Reward: -85.60\n",
      "Average Reward (last 100): -110.21\n",
      "--------------------------------------------------\n",
      "Episode 13\n",
      "Reward: -114.31\n",
      "Average Reward (last 100): -110.53\n",
      "--------------------------------------------------\n",
      "Episode 14\n",
      "Reward: -106.83\n",
      "Average Reward (last 100): -110.26\n",
      "--------------------------------------------------\n",
      "Episode 15\n",
      "Reward: -114.25\n",
      "Average Reward (last 100): -110.53\n",
      "--------------------------------------------------\n",
      "Episode 16\n",
      "Reward: -114.02\n",
      "Average Reward (last 100): -110.75\n",
      "--------------------------------------------------\n",
      "Episode 17\n",
      "Reward: -109.10\n",
      "Average Reward (last 100): -110.65\n",
      "--------------------------------------------------\n",
      "Episode 18\n",
      "Reward: -109.94\n",
      "Average Reward (last 100): -110.61\n",
      "--------------------------------------------------\n",
      "Episode 19\n",
      "Reward: -109.58\n",
      "Average Reward (last 100): -110.56\n",
      "--------------------------------------------------\n",
      "Episode 20\n",
      "Reward: -103.76\n",
      "Average Reward (last 100): -110.22\n",
      "--------------------------------------------------\n",
      "Episode 21\n",
      "Reward: -2.61\n",
      "Average Reward (last 100): -105.09\n",
      "--------------------------------------------------\n",
      "Episode 22\n",
      "Reward: -113.34\n",
      "Average Reward (last 100): -105.47\n",
      "--------------------------------------------------\n",
      "Episode 23\n",
      "Reward: -106.65\n",
      "Average Reward (last 100): -105.52\n",
      "--------------------------------------------------\n",
      "Episode 24\n",
      "Reward: -104.22\n",
      "Average Reward (last 100): -105.46\n",
      "--------------------------------------------------\n",
      "Episode 25\n",
      "Reward: -104.24\n",
      "Average Reward (last 100): -105.42\n",
      "--------------------------------------------------\n",
      "Episode 26\n",
      "Reward: -115.04\n",
      "Average Reward (last 100): -105.79\n",
      "--------------------------------------------------\n",
      "Episode 27\n",
      "Reward: -106.98\n",
      "Average Reward (last 100): -105.83\n",
      "--------------------------------------------------\n",
      "Episode 28\n",
      "Reward: -104.46\n",
      "Average Reward (last 100): -105.78\n",
      "--------------------------------------------------\n",
      "Episode 29\n",
      "Reward: -112.91\n",
      "Average Reward (last 100): -106.03\n",
      "--------------------------------------------------\n",
      "Episode 30\n",
      "Reward: -113.61\n",
      "Average Reward (last 100): -106.28\n",
      "--------------------------------------------------\n",
      "Episode 31\n",
      "Reward: -106.16\n",
      "Average Reward (last 100): -106.28\n",
      "--------------------------------------------------\n",
      "Episode 32\n",
      "Reward: -106.43\n",
      "Average Reward (last 100): -106.28\n",
      "--------------------------------------------------\n",
      "Episode 33\n",
      "Reward: -117.14\n",
      "Average Reward (last 100): -106.61\n",
      "--------------------------------------------------\n",
      "Episode 34\n",
      "Reward: -114.57\n",
      "Average Reward (last 100): -106.84\n",
      "--------------------------------------------------\n",
      "Episode 35\n",
      "Reward: -104.83\n",
      "Average Reward (last 100): -106.79\n",
      "--------------------------------------------------\n",
      "Episode 36\n",
      "Reward: -107.28\n",
      "Average Reward (last 100): -106.80\n",
      "--------------------------------------------------\n",
      "Episode 37\n",
      "Reward: -118.38\n",
      "Average Reward (last 100): -107.11\n",
      "--------------------------------------------------\n",
      "Episode 38\n",
      "Reward: -116.63\n",
      "Average Reward (last 100): -107.36\n",
      "--------------------------------------------------\n",
      "Episode 39\n",
      "Reward: -112.34\n",
      "Average Reward (last 100): -107.49\n",
      "--------------------------------------------------\n",
      "Episode 40\n",
      "Reward: -100.83\n",
      "Average Reward (last 100): -107.32\n",
      "--------------------------------------------------\n",
      "Episode 41\n",
      "Reward: -100.42\n",
      "Average Reward (last 100): -107.16\n",
      "--------------------------------------------------\n",
      "Episode 42\n",
      "Reward: -106.19\n",
      "Average Reward (last 100): -107.13\n",
      "--------------------------------------------------\n",
      "Episode 43\n",
      "Reward: -102.25\n",
      "Average Reward (last 100): -107.02\n",
      "--------------------------------------------------\n",
      "Episode 44\n",
      "Reward: -113.18\n",
      "Average Reward (last 100): -107.16\n",
      "--------------------------------------------------\n",
      "Episode 45\n",
      "Reward: -99.68\n",
      "Average Reward (last 100): -106.99\n",
      "--------------------------------------------------\n",
      "Episode 46\n",
      "Reward: -105.26\n",
      "Average Reward (last 100): -106.96\n",
      "--------------------------------------------------\n",
      "Episode 47\n",
      "Reward: -110.41\n",
      "Average Reward (last 100): -107.03\n",
      "--------------------------------------------------\n",
      "Episode 48\n",
      "Reward: -110.44\n",
      "Average Reward (last 100): -107.10\n",
      "--------------------------------------------------\n",
      "Episode 49\n",
      "Reward: -98.94\n",
      "Average Reward (last 100): -106.93\n",
      "--------------------------------------------------\n",
      "Episode 50\n",
      "Reward: -100.16\n",
      "Average Reward (last 100): -106.80\n",
      "--------------------------------------------------\n",
      "Episode 51\n",
      "Reward: -101.83\n",
      "Average Reward (last 100): -106.70\n",
      "--------------------------------------------------\n",
      "Episode 52\n",
      "Reward: -129.38\n",
      "Average Reward (last 100): -107.14\n",
      "--------------------------------------------------\n",
      "Episode 53\n",
      "Reward: -97.87\n",
      "Average Reward (last 100): -106.96\n",
      "--------------------------------------------------\n",
      "Episode 54\n",
      "Reward: -101.33\n",
      "Average Reward (last 100): -106.86\n",
      "--------------------------------------------------\n",
      "Episode 55\n",
      "Reward: -108.84\n",
      "Average Reward (last 100): -106.89\n",
      "--------------------------------------------------\n",
      "Episode 56\n",
      "Reward: -100.33\n",
      "Average Reward (last 100): -106.78\n",
      "--------------------------------------------------\n",
      "Episode 57\n",
      "Reward: -117.01\n",
      "Average Reward (last 100): -106.96\n",
      "--------------------------------------------------\n",
      "Episode 58\n",
      "Reward: -111.85\n",
      "Average Reward (last 100): -107.04\n",
      "--------------------------------------------------\n",
      "Episode 59\n",
      "Reward: -108.69\n",
      "Average Reward (last 100): -107.07\n",
      "--------------------------------------------------\n",
      "Episode 60\n",
      "Reward: -111.77\n",
      "Average Reward (last 100): -107.15\n",
      "--------------------------------------------------\n",
      "Episode 61\n",
      "Reward: -102.62\n",
      "Average Reward (last 100): -107.07\n",
      "--------------------------------------------------\n",
      "Episode 62\n",
      "Reward: -110.39\n",
      "Average Reward (last 100): -107.13\n",
      "--------------------------------------------------\n",
      "Episode 63\n",
      "Reward: -108.81\n",
      "Average Reward (last 100): -107.15\n",
      "--------------------------------------------------\n",
      "Episode 64\n",
      "Reward: -95.79\n",
      "Average Reward (last 100): -106.98\n",
      "--------------------------------------------------\n",
      "Episode 65\n",
      "Reward: -112.57\n",
      "Average Reward (last 100): -107.06\n",
      "--------------------------------------------------\n",
      "Episode 66\n",
      "Reward: -110.97\n",
      "Average Reward (last 100): -107.12\n",
      "--------------------------------------------------\n",
      "Episode 67\n",
      "Reward: -96.52\n",
      "Average Reward (last 100): -106.96\n",
      "--------------------------------------------------\n",
      "Episode 68\n",
      "Reward: -127.11\n",
      "Average Reward (last 100): -107.26\n",
      "--------------------------------------------------\n",
      "Episode 69\n",
      "Reward: -107.78\n",
      "Average Reward (last 100): -107.27\n",
      "--------------------------------------------------\n",
      "Episode 70\n",
      "Reward: -115.65\n",
      "Average Reward (last 100): -107.39\n",
      "--------------------------------------------------\n",
      "Episode 71\n",
      "Reward: -96.28\n",
      "Average Reward (last 100): -107.23\n",
      "--------------------------------------------------\n",
      "Episode 72\n",
      "Reward: -156.11\n",
      "Average Reward (last 100): -107.91\n",
      "--------------------------------------------------\n",
      "Episode 73\n",
      "Reward: -113.11\n",
      "Average Reward (last 100): -107.98\n",
      "--------------------------------------------------\n",
      "Episode 74\n",
      "Reward: -113.57\n",
      "Average Reward (last 100): -108.06\n",
      "--------------------------------------------------\n",
      "Episode 75\n",
      "Reward: -116.13\n",
      "Average Reward (last 100): -108.16\n",
      "--------------------------------------------------\n",
      "Episode 76\n",
      "Reward: -138.14\n",
      "Average Reward (last 100): -108.56\n",
      "--------------------------------------------------\n",
      "Episode 77\n",
      "Reward: -110.23\n",
      "Average Reward (last 100): -108.58\n",
      "--------------------------------------------------\n",
      "Episode 78\n",
      "Reward: -147.70\n",
      "Average Reward (last 100): -109.08\n",
      "--------------------------------------------------\n",
      "Episode 79\n",
      "Reward: -206.76\n",
      "Average Reward (last 100): -110.32\n",
      "--------------------------------------------------\n",
      "Episode 80\n",
      "Reward: -198.49\n",
      "Average Reward (last 100): -111.42\n",
      "--------------------------------------------------\n",
      "Episode 81\n",
      "Reward: -118.02\n",
      "Average Reward (last 100): -111.50\n",
      "--------------------------------------------------\n",
      "Episode 82\n",
      "Reward: -122.20\n",
      "Average Reward (last 100): -111.63\n",
      "--------------------------------------------------\n",
      "Episode 83\n",
      "Reward: -123.29\n",
      "Average Reward (last 100): -111.77\n",
      "--------------------------------------------------\n",
      "Episode 84\n",
      "Reward: -121.28\n",
      "Average Reward (last 100): -111.88\n",
      "--------------------------------------------------\n",
      "Episode 85\n",
      "Reward: -113.16\n",
      "Average Reward (last 100): -111.90\n",
      "--------------------------------------------------\n",
      "Episode 86\n",
      "Reward: -84.88\n",
      "Average Reward (last 100): -111.59\n",
      "--------------------------------------------------\n",
      "Episode 87\n",
      "Reward: -100.50\n",
      "Average Reward (last 100): -111.46\n",
      "--------------------------------------------------\n",
      "Episode 88\n",
      "Reward: -109.89\n",
      "Average Reward (last 100): -111.44\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Normal\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Set environment variables to handle PyGame display better\n",
    "os.environ['SDL_VIDEODRIVER'] = 'windib'  # Use Windows driver\n",
    "os.environ['SDL_WINDOW_CENTERED'] = '1'\n",
    "\n",
    "# Actor Network\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.mean_layer = nn.Linear(256, action_dim)\n",
    "        self.log_std_layer = nn.Linear(256, action_dim)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = self.net(state)\n",
    "        mean = torch.tanh(self.mean_layer(x))\n",
    "        log_std = self.log_std_layer(x)\n",
    "        log_std = torch.clamp(log_std, -20, 2)\n",
    "        return mean, log_std.exp()\n",
    "\n",
    "# Critic Network\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, state):\n",
    "        return self.net(state)\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.actor = Actor(state_dim, action_dim)\n",
    "        self.critic = Critic(state_dim)\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=3e-4)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=3e-4)\n",
    "        \n",
    "        # PPO hyperparameters\n",
    "        self.gamma = 0.99\n",
    "        self.gae_lambda = 0.95\n",
    "        self.clip_ratio = 0.2\n",
    "        self.clip_rewards = True\n",
    "        self.entropy_coef = 0.01\n",
    "        self.max_grad_norm = 0.5\n",
    "        \n",
    "        # Training history\n",
    "        self.rewards_history = []\n",
    "        self.avg_rewards_history = []\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        with torch.no_grad():\n",
    "            state = torch.FloatTensor(state)\n",
    "            mean, std = self.actor(state)\n",
    "            dist = Normal(mean, std)\n",
    "            action = dist.sample()\n",
    "            action = torch.clamp(action, -1.0, 1.0)\n",
    "            log_prob = dist.log_prob(action).sum(dim=-1)\n",
    "            value = self.critic(state)\n",
    "            return action.numpy(), value.item(), log_prob.item()\n",
    "    \n",
    "    def compute_returns(self, rewards, dones, values):\n",
    "        returns = []\n",
    "        advantages = []\n",
    "        advantage = 0\n",
    "        next_value = 0\n",
    "        \n",
    "        for r, d, v in zip(reversed(rewards), reversed(dones), reversed(values)):\n",
    "            td_error = r + self.gamma * next_value * (1 - d) - v\n",
    "            advantage = td_error + self.gamma * self.gae_lambda * (1 - d) * advantage\n",
    "            next_value = v\n",
    "            \n",
    "            returns.insert(0, advantage + v)\n",
    "            advantages.insert(0, advantage)\n",
    "            \n",
    "        return torch.FloatTensor(returns), torch.FloatTensor(advantages)\n",
    "    \n",
    "    def update(self, states, actions, log_probs, returns, advantages, batch_size=64):\n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.FloatTensor(actions)\n",
    "        old_log_probs = torch.FloatTensor(log_probs)\n",
    "        \n",
    "        # Normalize advantages\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        for _ in range(10):  # PPO epochs\n",
    "            # Generate random indices\n",
    "            indices = torch.randperm(states.size(0))\n",
    "            \n",
    "            for start_idx in range(0, states.size(0), batch_size):\n",
    "                idx = indices[start_idx:start_idx + batch_size]\n",
    "                \n",
    "                batch_states = states[idx]\n",
    "                batch_actions = actions[idx]\n",
    "                batch_old_log_probs = old_log_probs[idx]\n",
    "                batch_returns = returns[idx]\n",
    "                batch_advantages = advantages[idx]\n",
    "                \n",
    "                # Get current policy distribution\n",
    "                means, stds = self.actor(batch_states)\n",
    "                dist = Normal(means, stds)\n",
    "                new_log_probs = dist.log_prob(batch_actions).sum(dim=-1)\n",
    "                entropy = dist.entropy().mean()\n",
    "                \n",
    "                # Calculate ratios\n",
    "                ratios = torch.exp(new_log_probs - batch_old_log_probs)\n",
    "                surr1 = ratios * batch_advantages\n",
    "                surr2 = torch.clamp(ratios, 1 - self.clip_ratio, 1 + self.clip_ratio) * batch_advantages\n",
    "                \n",
    "                # Calculate losses\n",
    "                actor_loss = -torch.min(surr1, surr2).mean()\n",
    "                critic_values = self.critic(batch_states).squeeze()\n",
    "                critic_loss = 0.5 * ((critic_values - batch_returns) ** 2).mean()\n",
    "                \n",
    "                # Total loss\n",
    "                loss = actor_loss + 0.5 * critic_loss - self.entropy_coef * entropy\n",
    "                \n",
    "                # Update networks\n",
    "                self.actor_optimizer.zero_grad()\n",
    "                self.critic_optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                \n",
    "                # Clip gradients\n",
    "                torch.nn.utils.clip_grad_norm_(self.actor.parameters(), self.max_grad_norm)\n",
    "                torch.nn.utils.clip_grad_norm_(self.critic.parameters(), self.max_grad_norm)\n",
    "                \n",
    "                self.actor_optimizer.step()\n",
    "                self.critic_optimizer.step()\n",
    "\n",
    "def train(render_every=20, total_episodes=1000):\n",
    "    # Create environments\n",
    "    env = gym.make('BipedalWalker-v3')\n",
    "    \n",
    "    # Initialize agent\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    agent = PPO(state_dim, action_dim)\n",
    "    \n",
    "    # Training loop\n",
    "    best_reward = float('-inf')\n",
    "    episode_rewards = []\n",
    "    \n",
    "    try:\n",
    "        for episode in range(total_episodes):\n",
    "            states, actions, rewards, dones, log_probs, values = [], [], [], [], [], []\n",
    "            \n",
    "            # Reset environment\n",
    "            state, _ = env.reset()\n",
    "            episode_reward = 0\n",
    "            \n",
    "            # Create display environment if needed\n",
    "            if episode % render_every == 0:\n",
    "                display_env = gym.make('BipedalWalker-v3', render_mode='human')\n",
    "                display_state, _ = display_env.reset()\n",
    "            \n",
    "            while True:\n",
    "                # Get action from agent\n",
    "                action, value, log_prob = agent.get_action(state)\n",
    "                \n",
    "                # Take step in environment\n",
    "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "                done = terminated or truncated\n",
    "                \n",
    "                # Store transition\n",
    "                states.append(state)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                dones.append(done)\n",
    "                log_probs.append(log_prob)\n",
    "                values.append(value)\n",
    "                \n",
    "                episode_reward += reward\n",
    "                state = next_state\n",
    "                \n",
    "                # Display if needed\n",
    "                if episode % render_every == 0:\n",
    "                    display_action, _, _ = agent.get_action(display_state)\n",
    "                    display_state, _, terminated, truncated, _ = display_env.step(display_action)\n",
    "                    if terminated or truncated:\n",
    "                        display_env.close()\n",
    "                        break\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            # Update policy\n",
    "            returns, advantages = agent.compute_returns(rewards, dones, values)\n",
    "            agent.update(states, actions, log_probs, returns, advantages)\n",
    "            \n",
    "            # Store reward\n",
    "            episode_rewards.append(episode_reward)\n",
    "            avg_reward = np.mean(episode_rewards[-100:])\n",
    "            \n",
    "            # Print progress\n",
    "            print(f\"Episode {episode + 1}\")\n",
    "            print(f\"Reward: {episode_reward:.2f}\")\n",
    "            print(f\"Average Reward (last 100): {avg_reward:.2f}\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            # Plot progress\n",
    "            if (episode + 1) % 10 == 0:\n",
    "                plt.figure(figsize=(10, 5))\n",
    "                plt.plot(episode_rewards)\n",
    "                plt.title(\"Training Progress\")\n",
    "                plt.xlabel(\"Episode\")\n",
    "                plt.ylabel(\"Reward\")\n",
    "                plt.savefig(\"training_progress.png\")\n",
    "                plt.close()\n",
    "            \n",
    "            # Save best model\n",
    "            if avg_reward > best_reward:\n",
    "                best_reward = avg_reward\n",
    "                torch.save({\n",
    "                    'actor_state_dict': agent.actor.state_dict(),\n",
    "                    'critic_state_dict': agent.critic.state_dict(),\n",
    "                    'reward': best_reward\n",
    "                }, 'best_model.pth')\n",
    "            \n",
    "            # Early stopping\n",
    "            if avg_reward >= 300:\n",
    "                print(\"Environment solved!\")\n",
    "                break\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nTraining interrupted by user\")\n",
    "    \n",
    "    finally:\n",
    "        env.close()\n",
    "        if 'display_env' in locals():\n",
    "            display_env.close()\n",
    "    \n",
    "    return agent\n",
    "\n",
    "def evaluate(agent, episodes=5):\n",
    "    env = gym.make('BipedalWalker-v3', render_mode='human')\n",
    "    \n",
    "    try:\n",
    "        for episode in range(episodes):\n",
    "            state, _ = env.reset()\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "            \n",
    "            while not done:\n",
    "                action, _, _ = agent.get_action(state)\n",
    "                state, reward, terminated, truncated, _ = env.step(action)\n",
    "                done = terminated or truncated\n",
    "                total_reward += reward\n",
    "                time.sleep(0.01)  # Slow down visualization\n",
    "            \n",
    "            print(f\"Episode {episode + 1} Reward: {total_reward:.2f}\")\n",
    "    \n",
    "    finally:\n",
    "        env.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set random seeds\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Train agent\n",
    "    print(\"Starting training...\")\n",
    "    agent = train(render_every=20)  # Show every 20 episodes\n",
    "    \n",
    "    # Evaluate agent\n",
    "    print(\"\\nEvaluating trained agent...\")\n",
    "    evaluate(agent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
