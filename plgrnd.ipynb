{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "\n",
      "Training interrupted by user\n",
      "\n",
      "Evaluating trained agent...\n",
      "Episode 1 Reward: -108.98\n",
      "Episode 2 Reward: -111.91\n",
      "Episode 3 Reward: -107.43\n",
      "Episode 4 Reward: -105.17\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Normal\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Set environment variables to handle PyGame display better\n",
    "os.environ['SDL_VIDEODRIVER'] = 'windib'  # Use Windows driver\n",
    "os.environ['SDL_WINDOW_CENTERED'] = '1'\n",
    "\n",
    "# Actor Network\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.mean_layer = nn.Linear(256, action_dim)\n",
    "        self.log_std_layer = nn.Linear(256, action_dim)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = self.net(state)\n",
    "        mean = torch.tanh(self.mean_layer(x))\n",
    "        log_std = self.log_std_layer(x)\n",
    "        log_std = torch.clamp(log_std, -20, 2)\n",
    "        return mean, log_std.exp()\n",
    "\n",
    "# Critic Network\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, state):\n",
    "        return self.net(state)\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.actor = Actor(state_dim, action_dim)\n",
    "        self.critic = Critic(state_dim)\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=3e-4)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=3e-4)\n",
    "        \n",
    "        # PPO hyperparameters\n",
    "        self.gamma = 0.99\n",
    "        self.gae_lambda = 0.95\n",
    "        self.clip_ratio = 0.2\n",
    "        self.clip_rewards = True\n",
    "        self.entropy_coef = 0.01\n",
    "        self.max_grad_norm = 0.5\n",
    "        \n",
    "        # Training history\n",
    "        self.rewards_history = []\n",
    "        self.avg_rewards_history = []\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        with torch.no_grad():\n",
    "            state = torch.FloatTensor(state)\n",
    "            mean, std = self.actor(state)\n",
    "            dist = Normal(mean, std)\n",
    "            action = dist.sample()\n",
    "            action = torch.clamp(action, -1.0, 1.0)\n",
    "            log_prob = dist.log_prob(action).sum(dim=-1)\n",
    "            value = self.critic(state)\n",
    "            return action.numpy(), value.item(), log_prob.item()\n",
    "    \n",
    "    def compute_returns(self, rewards, dones, values):\n",
    "        returns = []\n",
    "        advantages = []\n",
    "        advantage = 0\n",
    "        next_value = 0\n",
    "        \n",
    "        for r, d, v in zip(reversed(rewards), reversed(dones), reversed(values)):\n",
    "            td_error = r + self.gamma * next_value * (1 - d) - v\n",
    "            advantage = td_error + self.gamma * self.gae_lambda * (1 - d) * advantage\n",
    "            next_value = v\n",
    "            \n",
    "            returns.insert(0, advantage + v)\n",
    "            advantages.insert(0, advantage)\n",
    "            \n",
    "        return torch.FloatTensor(returns), torch.FloatTensor(advantages)\n",
    "    \n",
    "    def update(self, states, actions, log_probs, returns, advantages, batch_size=64):\n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.FloatTensor(actions)\n",
    "        old_log_probs = torch.FloatTensor(log_probs)\n",
    "        \n",
    "        # Normalize advantages\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        for _ in range(10):  # PPO epochs\n",
    "            # Generate random indices\n",
    "            indices = torch.randperm(states.size(0))\n",
    "            \n",
    "            for start_idx in range(0, states.size(0), batch_size):\n",
    "                idx = indices[start_idx:start_idx + batch_size]\n",
    "                \n",
    "                batch_states = states[idx]\n",
    "                batch_actions = actions[idx]\n",
    "                batch_old_log_probs = old_log_probs[idx]\n",
    "                batch_returns = returns[idx]\n",
    "                batch_advantages = advantages[idx]\n",
    "                \n",
    "                # Get current policy distribution\n",
    "                means, stds = self.actor(batch_states)\n",
    "                dist = Normal(means, stds)\n",
    "                new_log_probs = dist.log_prob(batch_actions).sum(dim=-1)\n",
    "                entropy = dist.entropy().mean()\n",
    "                \n",
    "                # Calculate ratios\n",
    "                ratios = torch.exp(new_log_probs - batch_old_log_probs)\n",
    "                surr1 = ratios * batch_advantages\n",
    "                surr2 = torch.clamp(ratios, 1 - self.clip_ratio, 1 + self.clip_ratio) * batch_advantages\n",
    "                \n",
    "                # Calculate losses\n",
    "                actor_loss = -torch.min(surr1, surr2).mean()\n",
    "                critic_values = self.critic(batch_states).squeeze()\n",
    "                critic_loss = 0.5 * ((critic_values - batch_returns) ** 2).mean()\n",
    "                \n",
    "                # Total loss\n",
    "                loss = actor_loss + 0.5 * critic_loss - self.entropy_coef * entropy\n",
    "                \n",
    "                # Update networks\n",
    "                self.actor_optimizer.zero_grad()\n",
    "                self.critic_optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                \n",
    "                # Clip gradients\n",
    "                torch.nn.utils.clip_grad_norm_(self.actor.parameters(), self.max_grad_norm)\n",
    "                torch.nn.utils.clip_grad_norm_(self.critic.parameters(), self.max_grad_norm)\n",
    "                \n",
    "                self.actor_optimizer.step()\n",
    "                self.critic_optimizer.step()\n",
    "\n",
    "def train(render_every=20, total_episodes=1000):\n",
    "    # Create environments\n",
    "    env = gym.make('BipedalWalker-v3')\n",
    "    \n",
    "    # Initialize agent\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    agent = PPO(state_dim, action_dim)\n",
    "    \n",
    "    # Training loop\n",
    "    best_reward = float('-inf')\n",
    "    episode_rewards = []\n",
    "    \n",
    "    try:\n",
    "        for episode in range(total_episodes):\n",
    "            states, actions, rewards, dones, log_probs, values = [], [], [], [], [], []\n",
    "            \n",
    "            # Reset environment\n",
    "            state, _ = env.reset()\n",
    "            episode_reward = 0\n",
    "            \n",
    "            # Create display environment if needed\n",
    "            if episode % render_every == 0:\n",
    "                display_env = gym.make('BipedalWalker-v3', render_mode='human')\n",
    "                display_state, _ = display_env.reset()\n",
    "            \n",
    "            while True:\n",
    "                # Get action from agent\n",
    "                action, value, log_prob = agent.get_action(state)\n",
    "                \n",
    "                # Take step in environment\n",
    "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "                done = terminated or truncated\n",
    "                \n",
    "                # Store transition\n",
    "                states.append(state)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                dones.append(done)\n",
    "                log_probs.append(log_prob)\n",
    "                values.append(value)\n",
    "                \n",
    "                episode_reward += reward\n",
    "                state = next_state\n",
    "                \n",
    "                # Display if needed\n",
    "                if episode % render_every == 0:\n",
    "                    display_action, _, _ = agent.get_action(display_state)\n",
    "                    display_state, _, terminated, truncated, _ = display_env.step(display_action)\n",
    "                    if terminated or truncated:\n",
    "                        display_env.close()\n",
    "                        break\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            # Update policy\n",
    "            returns, advantages = agent.compute_returns(rewards, dones, values)\n",
    "            agent.update(states, actions, log_probs, returns, advantages)\n",
    "            \n",
    "            # Store reward\n",
    "            episode_rewards.append(episode_reward)\n",
    "            avg_reward = np.mean(episode_rewards[-100:])\n",
    "            \n",
    "            # Print progress\n",
    "            print(f\"Episode {episode + 1}\")\n",
    "            print(f\"Reward: {episode_reward:.2f}\")\n",
    "            print(f\"Average Reward (last 100): {avg_reward:.2f}\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            # Plot progress\n",
    "            if (episode + 1) % 10 == 0:\n",
    "                plt.figure(figsize=(10, 5))\n",
    "                plt.plot(episode_rewards)\n",
    "                plt.title(\"Training Progress\")\n",
    "                plt.xlabel(\"Episode\")\n",
    "                plt.ylabel(\"Reward\")\n",
    "                plt.savefig(\"training_progress.png\")\n",
    "                plt.close()\n",
    "            \n",
    "            # Save best model\n",
    "            if avg_reward > best_reward:\n",
    "                best_reward = avg_reward\n",
    "                torch.save({\n",
    "                    'actor_state_dict': agent.actor.state_dict(),\n",
    "                    'critic_state_dict': agent.critic.state_dict(),\n",
    "                    'reward': best_reward\n",
    "                }, 'best_model.pth')\n",
    "            \n",
    "            # Early stopping\n",
    "            if avg_reward >= 300:\n",
    "                print(\"Environment solved!\")\n",
    "                break\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nTraining interrupted by user\")\n",
    "    \n",
    "    finally:\n",
    "        env.close()\n",
    "        if 'display_env' in locals():\n",
    "            display_env.close()\n",
    "    \n",
    "    return agent\n",
    "\n",
    "def evaluate(agent, episodes=5):\n",
    "    env = gym.make('BipedalWalker-v3', render_mode='human')\n",
    "    \n",
    "    try:\n",
    "        for episode in range(episodes):\n",
    "            state, _ = env.reset()\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "            \n",
    "            while not done:\n",
    "                action, _, _ = agent.get_action(state)\n",
    "                state, reward, terminated, truncated, _ = env.step(action)\n",
    "                done = terminated or truncated\n",
    "                total_reward += reward\n",
    "                time.sleep(0.01)  # Slow down visualization\n",
    "            \n",
    "            print(f\"Episode {episode + 1} Reward: {total_reward:.2f}\")\n",
    "    \n",
    "    finally:\n",
    "        env.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set random seeds\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Train agent\n",
    "    print(\"Starting training...\")\n",
    "    agent = train(render_every=20)  # Show every 20 episodes\n",
    "    \n",
    "    # Evaluate agent\n",
    "    print(\"\\nEvaluating trained agent...\")\n",
    "    evaluate(agent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
