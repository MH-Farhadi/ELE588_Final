{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tnlab\\AppData\\Local\\Temp\\ipykernel_8120\\1820079655.py:94: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  states = torch.FloatTensor(memory['states']).to(self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\n",
      "Reward: -4.69\n",
      "Average Reward (last 100): -4.69\n",
      "--------------------------------------------------\n",
      "Episode 2\n",
      "Reward: -112.97\n",
      "Average Reward (last 100): -58.83\n",
      "--------------------------------------------------\n",
      "Episode 3\n",
      "Reward: -110.51\n",
      "Average Reward (last 100): -76.06\n",
      "--------------------------------------------------\n",
      "Episode 4\n",
      "Reward: -150.60\n",
      "Average Reward (last 100): -94.69\n",
      "--------------------------------------------------\n",
      "Episode 5\n",
      "Reward: -106.54\n",
      "Average Reward (last 100): -97.06\n",
      "--------------------------------------------------\n",
      "Episode 6\n",
      "Reward: -178.10\n",
      "Average Reward (last 100): -110.57\n",
      "--------------------------------------------------\n",
      "Episode 7\n",
      "Reward: -164.93\n",
      "Average Reward (last 100): -118.33\n",
      "--------------------------------------------------\n",
      "Episode 8\n",
      "Reward: -120.05\n",
      "Average Reward (last 100): -118.55\n",
      "--------------------------------------------------\n",
      "Episode 9\n",
      "Reward: -189.57\n",
      "Average Reward (last 100): -126.44\n",
      "--------------------------------------------------\n",
      "Episode 10\n",
      "Reward: -156.73\n",
      "Average Reward (last 100): -129.47\n",
      "--------------------------------------------------\n",
      "Episode 11\n",
      "Reward: -183.21\n",
      "Average Reward (last 100): -134.35\n",
      "--------------------------------------------------\n",
      "Episode 12\n",
      "Reward: -195.22\n",
      "Average Reward (last 100): -139.43\n",
      "--------------------------------------------------\n",
      "Episode 13\n",
      "Reward: -201.45\n",
      "Average Reward (last 100): -144.20\n",
      "--------------------------------------------------\n",
      "Episode 14\n",
      "Reward: -115.51\n",
      "Average Reward (last 100): -142.15\n",
      "--------------------------------------------------\n",
      "Episode 15\n",
      "Reward: -131.55\n",
      "Average Reward (last 100): -141.44\n",
      "--------------------------------------------------\n",
      "Episode 16\n",
      "Reward: -107.84\n",
      "Average Reward (last 100): -139.34\n",
      "--------------------------------------------------\n",
      "Episode 17\n",
      "Reward: -129.67\n",
      "Average Reward (last 100): -138.77\n",
      "--------------------------------------------------\n",
      "Episode 18\n",
      "Reward: -129.06\n",
      "Average Reward (last 100): -138.23\n",
      "--------------------------------------------------\n",
      "Episode 19\n",
      "Reward: -123.87\n",
      "Average Reward (last 100): -137.48\n",
      "--------------------------------------------------\n",
      "Episode 20\n",
      "Reward: -236.86\n",
      "Average Reward (last 100): -142.45\n",
      "--------------------------------------------------\n",
      "Episode 21\n",
      "Reward: -111.75\n",
      "Average Reward (last 100): -140.98\n",
      "--------------------------------------------------\n",
      "Episode 22\n",
      "Reward: -128.29\n",
      "Average Reward (last 100): -140.41\n",
      "--------------------------------------------------\n",
      "Episode 23\n",
      "Reward: -121.59\n",
      "Average Reward (last 100): -139.59\n",
      "--------------------------------------------------\n",
      "Episode 24\n",
      "Reward: -107.50\n",
      "Average Reward (last 100): -138.25\n",
      "--------------------------------------------------\n",
      "Episode 25\n",
      "Reward: -111.10\n",
      "Average Reward (last 100): -137.17\n",
      "--------------------------------------------------\n",
      "Episode 26\n",
      "Reward: -110.74\n",
      "Average Reward (last 100): -136.15\n",
      "--------------------------------------------------\n",
      "Episode 27\n",
      "Reward: -112.93\n",
      "Average Reward (last 100): -135.29\n",
      "--------------------------------------------------\n",
      "Episode 28\n",
      "Reward: -114.29\n",
      "Average Reward (last 100): -134.54\n",
      "--------------------------------------------------\n",
      "Episode 29\n",
      "Reward: -118.41\n",
      "Average Reward (last 100): -133.98\n",
      "--------------------------------------------------\n",
      "Episode 30\n",
      "Reward: -116.35\n",
      "Average Reward (last 100): -133.40\n",
      "--------------------------------------------------\n",
      "Episode 31\n",
      "Reward: -106.17\n",
      "Average Reward (last 100): -132.52\n",
      "--------------------------------------------------\n",
      "Episode 32\n",
      "Reward: -110.49\n",
      "Average Reward (last 100): -131.83\n",
      "--------------------------------------------------\n",
      "Episode 33\n",
      "Reward: -111.12\n",
      "Average Reward (last 100): -131.20\n",
      "--------------------------------------------------\n",
      "Episode 34\n",
      "Reward: -116.48\n",
      "Average Reward (last 100): -130.77\n",
      "--------------------------------------------------\n",
      "Episode 35\n",
      "Reward: -115.14\n",
      "Average Reward (last 100): -130.32\n",
      "--------------------------------------------------\n",
      "Episode 36\n",
      "Reward: -110.62\n",
      "Average Reward (last 100): -129.77\n",
      "--------------------------------------------------\n",
      "Episode 37\n",
      "Reward: -113.95\n",
      "Average Reward (last 100): -129.35\n",
      "--------------------------------------------------\n",
      "Episode 38\n",
      "Reward: -106.66\n",
      "Average Reward (last 100): -128.75\n",
      "--------------------------------------------------\n",
      "Episode 39\n",
      "Reward: -108.27\n",
      "Average Reward (last 100): -128.23\n",
      "--------------------------------------------------\n",
      "Episode 40\n",
      "Reward: -110.69\n",
      "Average Reward (last 100): -127.79\n",
      "--------------------------------------------------\n",
      "\n",
      "Training interrupted by user\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Normal\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Actor and Critic Network definitions remain the same\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(128)  # Layer normalization for stable learning\n",
    "        )\n",
    "        self.mean_layer = nn.Linear(128, action_dim)\n",
    "        self.log_std_layer = nn.Linear(128, action_dim)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = self.net(state)\n",
    "        mean = torch.tanh(self.mean_layer(x))\n",
    "        log_std = self.log_std_layer(x)\n",
    "        log_std = torch.clamp(log_std, -20, 2)\n",
    "        return mean, log_std.exp()\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(128)  # Layer normalization\n",
    "        )\n",
    "        self.value_layer = nn.Linear(128, 1)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = self.net(state)\n",
    "        value = self.value_layer(x)\n",
    "        return value\n",
    "\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.actor = Actor(state_dim, action_dim).to(self.device)\n",
    "        self.critic = Critic(state_dim).to(self.device)\n",
    "        \n",
    "        # Optimizers with initial learning rates\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=3e-4)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=1e-3)\n",
    "\n",
    "        # PPO hyperparameters with adjustments\n",
    "        self.gamma = 0.98\n",
    "        self.gae_lambda = 0.95\n",
    "        self.clip_ratio = 0.1  # Reduce to encourage policy stability\n",
    "        self.entropy_coef = 0.005  # Decreased entropy coefficient for later stages\n",
    "        self.max_grad_norm = 0.5\n",
    "\n",
    "        # Memory management\n",
    "        self.max_memory_size = 2048\n",
    "\n",
    "        # Learning rate scheduler\n",
    "        self.scheduler_actor = torch.optim.lr_scheduler.ExponentialLR(self.actor_optimizer, gamma=0.99)\n",
    "        self.scheduler_critic = torch.optim.lr_scheduler.ExponentialLR(self.critic_optimizer, gamma=0.99)\n",
    "\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        with torch.no_grad():\n",
    "            state = torch.FloatTensor(state).to(self.device)\n",
    "            mean, std = self.actor(state)\n",
    "            dist = Normal(mean, std)\n",
    "            action = dist.sample()\n",
    "            action = torch.clamp(action, -1.0, 1.0)\n",
    "            log_prob = dist.log_prob(action).sum(dim=-1)\n",
    "            value = self.critic(state)\n",
    "            return action.cpu().numpy(), value.item(), log_prob.item()\n",
    "\n",
    "    def update(self, memory, batch_size=64):\n",
    "        states = torch.FloatTensor(memory['states']).to(self.device)\n",
    "        actions = torch.FloatTensor(memory['actions']).to(self.device)\n",
    "        old_log_probs = torch.FloatTensor(memory['log_probs']).to(self.device)\n",
    "        returns = torch.FloatTensor(memory['returns']).to(self.device)\n",
    "        advantages = torch.FloatTensor(memory['advantages']).to(self.device)\n",
    "        \n",
    "        # Normalize advantages\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        # Mini-batch updates\n",
    "        indices = torch.randperm(states.size(0))\n",
    "        \n",
    "        for start_idx in range(0, states.size(0), batch_size):\n",
    "            idx = indices[start_idx:start_idx + batch_size]\n",
    "            \n",
    "            batch_states = states[idx]\n",
    "            batch_actions = actions[idx]\n",
    "            batch_old_log_probs = old_log_probs[idx]\n",
    "            batch_returns = returns[idx]\n",
    "            batch_advantages = advantages[idx]\n",
    "            \n",
    "            # Get current policy distribution\n",
    "            means, stds = self.actor(batch_states)\n",
    "            dist = Normal(means, stds)\n",
    "            new_log_probs = dist.log_prob(batch_actions).sum(dim=-1)\n",
    "            entropy = dist.entropy().mean()\n",
    "            \n",
    "            # Calculate ratios and losses\n",
    "            ratios = torch.exp(new_log_probs - batch_old_log_probs)\n",
    "            surr1 = ratios * batch_advantages\n",
    "            surr2 = torch.clamp(ratios, 1 - self.clip_ratio, 1 + self.clip_ratio) * batch_advantages\n",
    "            \n",
    "            actor_loss = -torch.min(surr1, surr2).mean()\n",
    "            critic_values = self.critic(batch_states).squeeze()\n",
    "            critic_loss = 0.5 * ((critic_values - batch_returns) ** 2).mean()\n",
    "            \n",
    "            # Total loss\n",
    "            loss = actor_loss + 0.5 * critic_loss - self.entropy_coef * entropy\n",
    "            \n",
    "            # Update networks\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            self.critic_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Clip gradients\n",
    "            torch.nn.utils.clip_grad_norm_(self.actor.parameters(), self.max_grad_norm)\n",
    "            torch.nn.utils.clip_grad_norm_(self.critic.parameters(), self.max_grad_norm)\n",
    "            \n",
    "            self.actor_optimizer.step()\n",
    "            self.critic_optimizer.step()\n",
    "\n",
    "def train(render_every=20, total_episodes=10000):\n",
    "    # Create environment\n",
    "    env = gym.make('BipedalWalker-v3')\n",
    "    \n",
    "    # Initialize agent\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    agent = PPO(state_dim, action_dim)\n",
    "    \n",
    "    # Training variables\n",
    "    best_reward = float('-inf')  # Initialize best_reward to a very low value\n",
    "    episode_rewards = []\n",
    "    display_env = None\n",
    "    \n",
    "    try:\n",
    "        for episode in range(total_episodes):\n",
    "            # Initialize episode memory\n",
    "            memory = {\n",
    "                'states': [],\n",
    "                'actions': [],\n",
    "                'rewards': [],\n",
    "                'dones': [],\n",
    "                'log_probs': [],\n",
    "                'values': []\n",
    "            }\n",
    "            \n",
    "            state, _ = env.reset()\n",
    "            episode_reward = 0\n",
    "            \n",
    "            # Create display environment if needed\n",
    "            if episode % render_every == 0:\n",
    "                if display_env is not None:\n",
    "                    display_env.close()\n",
    "                display_env = gym.make('BipedalWalker-v3', render_mode='human')\n",
    "                display_state, _ = display_env.reset()\n",
    "            \n",
    "            # Episode loop\n",
    "            while True:\n",
    "                action, value, log_prob = agent.get_action(state)\n",
    "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "                done = terminated or truncated\n",
    "                \n",
    "                # Store transition\n",
    "                memory['states'].append(state)\n",
    "                memory['actions'].append(action)\n",
    "                memory['rewards'].append(reward)\n",
    "                memory['dones'].append(done)\n",
    "                memory['log_probs'].append(log_prob)\n",
    "                memory['values'].append(value)\n",
    "                \n",
    "                episode_reward += reward\n",
    "                state = next_state\n",
    "                \n",
    "                # Display if needed\n",
    "                if episode % render_every == 0 and display_env is not None:\n",
    "                    display_action, _, _ = agent.get_action(display_state)\n",
    "                    display_state, _, terminated, truncated, _ = display_env.step(display_action)\n",
    "                    if terminated or truncated:\n",
    "                        break\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "                \n",
    "                # Check memory limit\n",
    "                if len(memory['states']) >= agent.max_memory_size:\n",
    "                    break\n",
    "            \n",
    "            # Compute returns and advantages\n",
    "            returns, advantages = compute_returns(\n",
    "                memory['rewards'],\n",
    "                memory['dones'],\n",
    "                memory['values'],\n",
    "                agent.gamma,\n",
    "                agent.gae_lambda\n",
    "            )\n",
    "            \n",
    "            # Update memory\n",
    "            memory['returns'] = returns\n",
    "            memory['advantages'] = advantages\n",
    "            \n",
    "            # Update policy\n",
    "            agent.update(memory)\n",
    "            \n",
    "            # Store and print progress\n",
    "            episode_rewards.append(episode_reward)\n",
    "            avg_reward = np.mean(episode_rewards[-100:])\n",
    "            \n",
    "            print(f\"Episode {episode + 1}\")\n",
    "            print(f\"Reward: {episode_reward:.2f}\")\n",
    "            print(f\"Average Reward (last 100): {avg_reward:.2f}\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            # Save best model\n",
    "            if avg_reward > best_reward:\n",
    "                best_reward = avg_reward\n",
    "                torch.save({\n",
    "                    'actor_state_dict': agent.actor.state_dict(),\n",
    "                    'critic_state_dict': agent.critic.state_dict(),\n",
    "                    'reward': best_reward\n",
    "                }, 'best_model.pth')\n",
    "            \n",
    "            # Clear memory\n",
    "            del memory\n",
    "            \n",
    "            # Early stopping\n",
    "            if avg_reward >= 300:\n",
    "                print(\"Environment solved!\")\n",
    "                break\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nTraining interrupted by user\")\n",
    "    \n",
    "    finally:\n",
    "        env.close()\n",
    "        if display_env is not None:\n",
    "            display_env.close()\n",
    "    \n",
    "    return agent\n",
    "\n",
    "\n",
    "\n",
    "def compute_returns(rewards, dones, values, gamma, gae_lambda):\n",
    "    returns = []\n",
    "    advantages = []\n",
    "    advantage = 0\n",
    "    next_value = 0\n",
    "    \n",
    "    for r, d, v in zip(reversed(rewards), reversed(dones), reversed(values)):\n",
    "        td_error = r + gamma * next_value * (1 - d) - v\n",
    "        advantage = td_error + gamma * gae_lambda * (1 - d) * advantage\n",
    "        next_value = v\n",
    "        \n",
    "        returns.insert(0, advantage + v)\n",
    "        advantages.insert(0, advantage)\n",
    "    \n",
    "    return returns, advantages\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set random seeds\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Train the agent\n",
    "    print(\"Starting training...\")\n",
    "    agent = train(render_every=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
