{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "New best model saved with eval reward: -101.28\n",
      "New best reward: -128.29\n",
      "Episode 1\n",
      "Reward: -128.29\n",
      "Average Reward (last 100): -128.29\n",
      "Steps: 143\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.95e-03\n",
      "--------------------------------------------------\n",
      "Episode 2\n",
      "Reward: -143.70\n",
      "Average Reward (last 100): -135.99\n",
      "Steps: 434\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.95e-03\n",
      "--------------------------------------------------\n",
      "New best reward: -124.99\n",
      "Episode 3\n",
      "Reward: -102.98\n",
      "Average Reward (last 100): -124.99\n",
      "Steps: 57\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.90e-03\n",
      "--------------------------------------------------\n",
      "New best reward: -119.17\n",
      "Episode 4\n",
      "Reward: -101.70\n",
      "Average Reward (last 100): -119.17\n",
      "Steps: 66\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.85e-03\n",
      "--------------------------------------------------\n",
      "New best reward: -114.88\n",
      "Episode 5\n",
      "Reward: -97.73\n",
      "Average Reward (last 100): -114.88\n",
      "Steps: 89\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 6\n",
      "Reward: -117.41\n",
      "Average Reward (last 100): -115.30\n",
      "Steps: 74\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 7\n",
      "Reward: -120.08\n",
      "Average Reward (last 100): -115.98\n",
      "Steps: 64\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 8\n",
      "Reward: -99.23\n",
      "Average Reward (last 100): -113.89\n",
      "Steps: 69\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "New best reward: -112.50\n",
      "Episode 9\n",
      "Reward: -101.39\n",
      "Average Reward (last 100): -112.50\n",
      "Steps: 80\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "New best reward: -111.45\n",
      "Episode 10\n",
      "Reward: -101.98\n",
      "Average Reward (last 100): -111.45\n",
      "Steps: 64\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 11\n",
      "Reward: -111.32\n",
      "Average Reward (last 100): -111.44\n",
      "Steps: 139\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "New best reward: -110.30\n",
      "Episode 12\n",
      "Reward: -97.79\n",
      "Average Reward (last 100): -110.30\n",
      "Steps: 192\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 13\n",
      "Reward: -103.81\n",
      "Average Reward (last 100): -109.80\n",
      "Steps: 101\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 14\n",
      "Reward: -108.05\n",
      "Average Reward (last 100): -109.68\n",
      "Steps: 103\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "New best reward: -109.25\n",
      "Episode 15\n",
      "Reward: -103.34\n",
      "Average Reward (last 100): -109.25\n",
      "Steps: 70\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 16\n",
      "Reward: -108.05\n",
      "Average Reward (last 100): -109.18\n",
      "Steps: 167\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 17\n",
      "Reward: -102.33\n",
      "Average Reward (last 100): -108.77\n",
      "Steps: 63\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 18\n",
      "Reward: -101.94\n",
      "Average Reward (last 100): -108.40\n",
      "Steps: 69\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 19\n",
      "Reward: -130.95\n",
      "Average Reward (last 100): -109.58\n",
      "Steps: 423\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 20\n",
      "Reward: -155.35\n",
      "Average Reward (last 100): -111.87\n",
      "Steps: 1600\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 21\n",
      "Reward: -106.03\n",
      "Average Reward (last 100): -111.59\n",
      "Steps: 61\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 22\n",
      "Reward: -107.56\n",
      "Average Reward (last 100): -111.41\n",
      "Steps: 90\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 23\n",
      "Reward: -103.32\n",
      "Average Reward (last 100): -111.06\n",
      "Steps: 86\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 24\n",
      "Reward: -111.62\n",
      "Average Reward (last 100): -111.08\n",
      "Steps: 49\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 25\n",
      "Reward: -108.59\n",
      "Average Reward (last 100): -110.98\n",
      "Steps: 171\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 26\n",
      "Reward: -116.68\n",
      "Average Reward (last 100): -111.20\n",
      "Steps: 58\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 27\n",
      "Reward: -121.08\n",
      "Average Reward (last 100): -111.57\n",
      "Steps: 78\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 28\n",
      "Reward: -104.16\n",
      "Average Reward (last 100): -111.30\n",
      "Steps: 63\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 29\n",
      "Reward: -118.65\n",
      "Average Reward (last 100): -111.56\n",
      "Steps: 57\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 30\n",
      "Reward: -104.65\n",
      "Average Reward (last 100): -111.33\n",
      "Steps: 108\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 31\n",
      "Reward: -109.57\n",
      "Average Reward (last 100): -111.27\n",
      "Steps: 130\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 32\n",
      "Reward: -104.72\n",
      "Average Reward (last 100): -111.06\n",
      "Steps: 63\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 33\n",
      "Reward: -124.84\n",
      "Average Reward (last 100): -111.48\n",
      "Steps: 66\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 34\n",
      "Reward: -103.28\n",
      "Average Reward (last 100): -111.24\n",
      "Steps: 64\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 35\n",
      "Reward: -102.46\n",
      "Average Reward (last 100): -110.99\n",
      "Steps: 88\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 36\n",
      "Reward: -105.22\n",
      "Average Reward (last 100): -110.83\n",
      "Steps: 54\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 37\n",
      "Reward: -105.44\n",
      "Average Reward (last 100): -110.68\n",
      "Steps: 56\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 38\n",
      "Reward: -104.04\n",
      "Average Reward (last 100): -110.51\n",
      "Steps: 43\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 39\n",
      "Reward: -105.07\n",
      "Average Reward (last 100): -110.37\n",
      "Steps: 54\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 40\n",
      "Reward: -130.27\n",
      "Average Reward (last 100): -110.87\n",
      "Steps: 113\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 41\n",
      "Reward: -103.87\n",
      "Average Reward (last 100): -110.70\n",
      "Steps: 67\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 42\n",
      "Reward: -102.43\n",
      "Average Reward (last 100): -110.50\n",
      "Steps: 73\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 43\n",
      "Reward: -103.01\n",
      "Average Reward (last 100): -110.33\n",
      "Steps: 61\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 44\n",
      "Reward: -101.79\n",
      "Average Reward (last 100): -110.13\n",
      "Steps: 55\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 45\n",
      "Reward: -101.14\n",
      "Average Reward (last 100): -109.93\n",
      "Steps: 60\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 46\n",
      "Reward: -105.26\n",
      "Average Reward (last 100): -109.83\n",
      "Steps: 53\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 47\n",
      "Reward: -121.46\n",
      "Average Reward (last 100): -110.08\n",
      "Steps: 66\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 48\n",
      "Reward: -124.81\n",
      "Average Reward (last 100): -110.38\n",
      "Steps: 79\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 49\n",
      "Reward: -125.55\n",
      "Average Reward (last 100): -110.69\n",
      "Steps: 79\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 50\n",
      "Reward: -105.33\n",
      "Average Reward (last 100): -110.59\n",
      "Steps: 47\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 51\n",
      "Reward: -106.74\n",
      "Average Reward (last 100): -110.51\n",
      "Steps: 55\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 52\n",
      "Reward: -103.44\n",
      "Average Reward (last 100): -110.37\n",
      "Steps: 63\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 53\n",
      "Reward: -124.24\n",
      "Average Reward (last 100): -110.64\n",
      "Steps: 67\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 54\n",
      "Reward: -106.69\n",
      "Average Reward (last 100): -110.56\n",
      "Steps: 58\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 55\n",
      "Reward: -106.59\n",
      "Average Reward (last 100): -110.49\n",
      "Steps: 87\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 56\n",
      "Reward: -101.72\n",
      "Average Reward (last 100): -110.33\n",
      "Steps: 86\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 57\n",
      "Reward: -102.06\n",
      "Average Reward (last 100): -110.19\n",
      "Steps: 56\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 58\n",
      "Reward: -110.97\n",
      "Average Reward (last 100): -110.20\n",
      "Steps: 86\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 59\n",
      "Reward: -104.22\n",
      "Average Reward (last 100): -110.10\n",
      "Steps: 73\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 60\n",
      "Reward: -105.05\n",
      "Average Reward (last 100): -110.02\n",
      "Steps: 58\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 61\n",
      "Reward: -104.29\n",
      "Average Reward (last 100): -109.92\n",
      "Steps: 42\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 62\n",
      "Reward: -122.72\n",
      "Average Reward (last 100): -110.13\n",
      "Steps: 73\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 63\n",
      "Reward: -122.62\n",
      "Average Reward (last 100): -110.33\n",
      "Steps: 92\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 64\n",
      "Reward: -104.49\n",
      "Average Reward (last 100): -110.24\n",
      "Steps: 41\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 65\n",
      "Reward: -105.27\n",
      "Average Reward (last 100): -110.16\n",
      "Steps: 52\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 66\n",
      "Reward: -105.29\n",
      "Average Reward (last 100): -110.09\n",
      "Steps: 61\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 67\n",
      "Reward: -103.70\n",
      "Average Reward (last 100): -109.99\n",
      "Steps: 60\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 68\n",
      "Reward: -129.08\n",
      "Average Reward (last 100): -110.27\n",
      "Steps: 182\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 69\n",
      "Reward: -105.00\n",
      "Average Reward (last 100): -110.20\n",
      "Steps: 87\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 70\n",
      "Reward: -147.30\n",
      "Average Reward (last 100): -110.73\n",
      "Steps: 223\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 71\n",
      "Reward: -100.64\n",
      "Average Reward (last 100): -110.58\n",
      "Steps: 87\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 72\n",
      "Reward: -103.58\n",
      "Average Reward (last 100): -110.49\n",
      "Steps: 58\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 73\n",
      "Reward: -116.05\n",
      "Average Reward (last 100): -110.56\n",
      "Steps: 45\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 74\n",
      "Reward: -106.58\n",
      "Average Reward (last 100): -110.51\n",
      "Steps: 89\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 75\n",
      "Reward: -119.33\n",
      "Average Reward (last 100): -110.63\n",
      "Steps: 62\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 76\n",
      "Reward: -124.60\n",
      "Average Reward (last 100): -110.81\n",
      "Steps: 81\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 77\n",
      "Reward: -117.88\n",
      "Average Reward (last 100): -110.90\n",
      "Steps: 59\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 78\n",
      "Reward: -126.89\n",
      "Average Reward (last 100): -111.11\n",
      "Steps: 94\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 79\n",
      "Reward: -107.22\n",
      "Average Reward (last 100): -111.06\n",
      "Steps: 59\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 80\n",
      "Reward: -123.38\n",
      "Average Reward (last 100): -111.21\n",
      "Steps: 66\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 81\n",
      "Reward: -101.11\n",
      "Average Reward (last 100): -111.09\n",
      "Steps: 73\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 82\n",
      "Reward: -121.50\n",
      "Average Reward (last 100): -111.21\n",
      "Steps: 80\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 83\n",
      "Reward: -104.88\n",
      "Average Reward (last 100): -111.14\n",
      "Steps: 92\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 84\n",
      "Reward: -108.96\n",
      "Average Reward (last 100): -111.11\n",
      "Steps: 108\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 85\n",
      "Reward: -111.66\n",
      "Average Reward (last 100): -111.12\n",
      "Steps: 119\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 86\n",
      "Reward: -127.12\n",
      "Average Reward (last 100): -111.30\n",
      "Steps: 113\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 87\n",
      "Reward: -105.82\n",
      "Average Reward (last 100): -111.24\n",
      "Steps: 90\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 88\n",
      "Reward: -104.58\n",
      "Average Reward (last 100): -111.17\n",
      "Steps: 56\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 89\n",
      "Reward: -114.87\n",
      "Average Reward (last 100): -111.21\n",
      "Steps: 44\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 90\n",
      "Reward: -104.92\n",
      "Average Reward (last 100): -111.14\n",
      "Steps: 53\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 91\n",
      "Reward: -128.35\n",
      "Average Reward (last 100): -111.33\n",
      "Steps: 106\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 92\n",
      "Reward: -123.13\n",
      "Average Reward (last 100): -111.45\n",
      "Steps: 73\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 93\n",
      "Reward: -106.18\n",
      "Average Reward (last 100): -111.40\n",
      "Steps: 58\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 94\n",
      "Reward: -118.28\n",
      "Average Reward (last 100): -111.47\n",
      "Steps: 62\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 95\n",
      "Reward: -130.51\n",
      "Average Reward (last 100): -111.67\n",
      "Steps: 175\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 96\n",
      "Reward: -102.61\n",
      "Average Reward (last 100): -111.58\n",
      "Steps: 58\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 97\n",
      "Reward: -127.16\n",
      "Average Reward (last 100): -111.74\n",
      "Steps: 67\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 98\n",
      "Reward: -107.68\n",
      "Average Reward (last 100): -111.70\n",
      "Steps: 66\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 99\n",
      "Reward: -104.33\n",
      "Average Reward (last 100): -111.62\n",
      "Steps: 84\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 100\n",
      "Reward: -132.05\n",
      "Average Reward (last 100): -111.83\n",
      "Steps: 83\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 101\n",
      "Reward: -102.46\n",
      "Average Reward (last 100): -111.57\n",
      "Steps: 78\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 102\n",
      "Reward: -102.69\n",
      "Average Reward (last 100): -111.16\n",
      "Steps: 95\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 103\n",
      "Reward: -102.55\n",
      "Average Reward (last 100): -111.15\n",
      "Steps: 65\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 104\n",
      "Reward: -103.89\n",
      "Average Reward (last 100): -111.18\n",
      "Steps: 66\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 105\n",
      "Reward: -120.73\n",
      "Average Reward (last 100): -111.41\n",
      "Steps: 59\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 106\n",
      "Reward: -112.07\n",
      "Average Reward (last 100): -111.35\n",
      "Steps: 119\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 107\n",
      "Reward: -103.92\n",
      "Average Reward (last 100): -111.19\n",
      "Steps: 69\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 108\n",
      "Reward: -123.09\n",
      "Average Reward (last 100): -111.43\n",
      "Steps: 80\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 109\n",
      "Reward: -126.27\n",
      "Average Reward (last 100): -111.68\n",
      "Steps: 98\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 110\n",
      "Reward: -111.45\n",
      "Average Reward (last 100): -111.77\n",
      "Steps: 99\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 111\n",
      "Reward: -118.47\n",
      "Average Reward (last 100): -111.84\n",
      "Steps: 50\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 112\n",
      "Reward: -106.15\n",
      "Average Reward (last 100): -111.93\n",
      "Steps: 74\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 113\n",
      "Reward: -132.72\n",
      "Average Reward (last 100): -112.22\n",
      "Steps: 130\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 114\n",
      "Reward: -101.23\n",
      "Average Reward (last 100): -112.15\n",
      "Steps: 57\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 115\n",
      "Reward: -102.12\n",
      "Average Reward (last 100): -112.14\n",
      "Steps: 66\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 116\n",
      "Reward: -183.18\n",
      "Average Reward (last 100): -112.89\n",
      "Steps: 635\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 117\n",
      "Reward: -108.33\n",
      "Average Reward (last 100): -112.95\n",
      "Steps: 61\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 118\n",
      "Reward: -105.55\n",
      "Average Reward (last 100): -112.98\n",
      "Steps: 73\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 119\n",
      "Reward: -101.52\n",
      "Average Reward (last 100): -112.69\n",
      "Steps: 107\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 120\n",
      "Reward: -103.86\n",
      "Average Reward (last 100): -112.17\n",
      "Steps: 83\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 121\n",
      "Reward: -104.08\n",
      "Average Reward (last 100): -112.16\n",
      "Steps: 65\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 122\n",
      "Reward: -121.90\n",
      "Average Reward (last 100): -112.30\n",
      "Steps: 66\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 123\n",
      "Reward: -106.76\n",
      "Average Reward (last 100): -112.33\n",
      "Steps: 40\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 124\n",
      "Reward: -126.65\n",
      "Average Reward (last 100): -112.48\n",
      "Steps: 70\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 125\n",
      "Reward: -142.45\n",
      "Average Reward (last 100): -112.82\n",
      "Steps: 184\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 126\n",
      "Reward: -105.98\n",
      "Average Reward (last 100): -112.71\n",
      "Steps: 68\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 127\n",
      "Reward: -103.36\n",
      "Average Reward (last 100): -112.54\n",
      "Steps: 59\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 128\n",
      "Reward: -132.76\n",
      "Average Reward (last 100): -112.82\n",
      "Steps: 93\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 129\n",
      "Reward: -104.44\n",
      "Average Reward (last 100): -112.68\n",
      "Steps: 67\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 130\n",
      "Reward: -121.14\n",
      "Average Reward (last 100): -112.85\n",
      "Steps: 57\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 131\n",
      "Reward: -104.22\n",
      "Average Reward (last 100): -112.79\n",
      "Steps: 60\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 132\n",
      "Reward: -123.22\n",
      "Average Reward (last 100): -112.98\n",
      "Steps: 59\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 133\n",
      "Reward: -104.70\n",
      "Average Reward (last 100): -112.78\n",
      "Steps: 63\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 134\n",
      "Reward: -108.18\n",
      "Average Reward (last 100): -112.83\n",
      "Steps: 73\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 135\n",
      "Reward: -115.85\n",
      "Average Reward (last 100): -112.96\n",
      "Steps: 47\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 136\n",
      "Reward: -126.54\n",
      "Average Reward (last 100): -113.17\n",
      "Steps: 89\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 137\n",
      "Reward: -219.09\n",
      "Average Reward (last 100): -114.31\n",
      "Steps: 927\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 138\n",
      "Reward: -105.56\n",
      "Average Reward (last 100): -114.32\n",
      "Steps: 105\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 139\n",
      "Reward: -105.13\n",
      "Average Reward (last 100): -114.32\n",
      "Steps: 63\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 140\n",
      "Reward: -110.95\n",
      "Average Reward (last 100): -114.13\n",
      "Steps: 92\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 141\n",
      "Reward: -101.64\n",
      "Average Reward (last 100): -114.11\n",
      "Steps: 54\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 142\n",
      "Reward: -128.81\n",
      "Average Reward (last 100): -114.37\n",
      "Steps: 79\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 143\n",
      "Reward: -103.48\n",
      "Average Reward (last 100): -114.38\n",
      "Steps: 58\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 144\n",
      "Reward: -102.47\n",
      "Average Reward (last 100): -114.38\n",
      "Steps: 61\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 145\n",
      "Reward: -103.13\n",
      "Average Reward (last 100): -114.40\n",
      "Steps: 53\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 146\n",
      "Reward: -103.88\n",
      "Average Reward (last 100): -114.39\n",
      "Steps: 74\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 147\n",
      "Reward: -104.16\n",
      "Average Reward (last 100): -114.22\n",
      "Steps: 63\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 148\n",
      "Reward: -102.61\n",
      "Average Reward (last 100): -114.00\n",
      "Steps: 77\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 149\n",
      "Reward: -126.01\n",
      "Average Reward (last 100): -114.00\n",
      "Steps: 81\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 150\n",
      "Reward: -103.96\n",
      "Average Reward (last 100): -113.99\n",
      "Steps: 69\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 151\n",
      "Reward: -103.03\n",
      "Average Reward (last 100): -113.95\n",
      "Steps: 57\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 152\n",
      "Reward: -104.08\n",
      "Average Reward (last 100): -113.96\n",
      "Steps: 72\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 153\n",
      "Reward: -102.59\n",
      "Average Reward (last 100): -113.74\n",
      "Steps: 58\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 154\n",
      "Reward: -105.70\n",
      "Average Reward (last 100): -113.73\n",
      "Steps: 88\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 155\n",
      "Reward: -102.22\n",
      "Average Reward (last 100): -113.69\n",
      "Steps: 67\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 156\n",
      "Reward: -102.03\n",
      "Average Reward (last 100): -113.69\n",
      "Steps: 67\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 157\n",
      "Reward: -124.21\n",
      "Average Reward (last 100): -113.91\n",
      "Steps: 77\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 158\n",
      "Reward: -106.01\n",
      "Average Reward (last 100): -113.86\n",
      "Steps: 96\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 159\n",
      "Reward: -111.22\n",
      "Average Reward (last 100): -113.93\n",
      "Steps: 104\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 160\n",
      "Reward: -105.35\n",
      "Average Reward (last 100): -113.93\n",
      "Steps: 65\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 161\n",
      "Reward: -102.16\n",
      "Average Reward (last 100): -113.91\n",
      "Steps: 60\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 162\n",
      "Reward: -105.01\n",
      "Average Reward (last 100): -113.74\n",
      "Steps: 80\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 163\n",
      "Reward: -123.00\n",
      "Average Reward (last 100): -113.74\n",
      "Steps: 51\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 164\n",
      "Reward: -107.42\n",
      "Average Reward (last 100): -113.77\n",
      "Steps: 63\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 165\n",
      "Reward: -99.85\n",
      "Average Reward (last 100): -113.71\n",
      "Steps: 70\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 166\n",
      "Reward: -104.97\n",
      "Average Reward (last 100): -113.71\n",
      "Steps: 41\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 167\n",
      "Reward: -105.47\n",
      "Average Reward (last 100): -113.73\n",
      "Steps: 50\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 168\n",
      "Reward: -125.31\n",
      "Average Reward (last 100): -113.69\n",
      "Steps: 85\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 169\n",
      "Reward: -104.25\n",
      "Average Reward (last 100): -113.68\n",
      "Steps: 70\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 170\n",
      "Reward: -105.45\n",
      "Average Reward (last 100): -113.27\n",
      "Steps: 77\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 171\n",
      "Reward: -133.94\n",
      "Average Reward (last 100): -113.60\n",
      "Steps: 104\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 172\n",
      "Reward: -104.30\n",
      "Average Reward (last 100): -113.61\n",
      "Steps: 70\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 173\n",
      "Reward: -104.59\n",
      "Average Reward (last 100): -113.49\n",
      "Steps: 60\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 174\n",
      "Reward: -104.21\n",
      "Average Reward (last 100): -113.47\n",
      "Steps: 61\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 175\n",
      "Reward: -103.12\n",
      "Average Reward (last 100): -113.31\n",
      "Steps: 55\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 176\n",
      "Reward: -105.21\n",
      "Average Reward (last 100): -113.11\n",
      "Steps: 67\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 177\n",
      "Reward: -132.98\n",
      "Average Reward (last 100): -113.26\n",
      "Steps: 245\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 178\n",
      "Reward: -103.72\n",
      "Average Reward (last 100): -113.03\n",
      "Steps: 62\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 179\n",
      "Reward: -112.98\n",
      "Average Reward (last 100): -113.09\n",
      "Steps: 89\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 180\n",
      "Reward: -107.37\n",
      "Average Reward (last 100): -112.93\n",
      "Steps: 63\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 181\n",
      "Reward: -105.59\n",
      "Average Reward (last 100): -112.97\n",
      "Steps: 67\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 182\n",
      "Reward: -103.72\n",
      "Average Reward (last 100): -112.80\n",
      "Steps: 70\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 183\n",
      "Reward: -107.53\n",
      "Average Reward (last 100): -112.82\n",
      "Steps: 120\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 184\n",
      "Reward: -152.93\n",
      "Average Reward (last 100): -113.26\n",
      "Steps: 273\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 185\n",
      "Reward: -104.50\n",
      "Average Reward (last 100): -113.19\n",
      "Steps: 46\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 186\n",
      "Reward: -131.27\n",
      "Average Reward (last 100): -113.23\n",
      "Steps: 144\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 187\n",
      "Reward: -135.22\n",
      "Average Reward (last 100): -113.53\n",
      "Steps: 129\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 188\n",
      "Reward: -103.16\n",
      "Average Reward (last 100): -113.51\n",
      "Steps: 59\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 189\n",
      "Reward: -109.86\n",
      "Average Reward (last 100): -113.46\n",
      "Steps: 64\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 190\n",
      "Reward: -104.09\n",
      "Average Reward (last 100): -113.45\n",
      "Steps: 41\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 191\n",
      "Reward: -104.46\n",
      "Average Reward (last 100): -113.21\n",
      "Steps: 59\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 192\n",
      "Reward: -130.66\n",
      "Average Reward (last 100): -113.29\n",
      "Steps: 93\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 193\n",
      "Reward: -106.80\n",
      "Average Reward (last 100): -113.30\n",
      "Steps: 65\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 194\n",
      "Reward: -111.29\n",
      "Average Reward (last 100): -113.23\n",
      "Steps: 75\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 195\n",
      "Reward: -124.23\n",
      "Average Reward (last 100): -113.16\n",
      "Steps: 69\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 196\n",
      "Reward: -104.78\n",
      "Average Reward (last 100): -113.18\n",
      "Steps: 54\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 197\n",
      "Reward: -128.72\n",
      "Average Reward (last 100): -113.20\n",
      "Steps: 79\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 198\n",
      "Reward: -102.79\n",
      "Average Reward (last 100): -113.15\n",
      "Steps: 74\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 199\n",
      "Reward: -104.61\n",
      "Average Reward (last 100): -113.15\n",
      "Steps: 59\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 200\n",
      "Reward: -104.47\n",
      "Average Reward (last 100): -112.88\n",
      "Steps: 58\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 201\n",
      "Reward: -123.48\n",
      "Average Reward (last 100): -113.09\n",
      "Steps: 72\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 202\n",
      "Reward: -102.77\n",
      "Average Reward (last 100): -113.09\n",
      "Steps: 49\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 203\n",
      "Reward: -102.74\n",
      "Average Reward (last 100): -113.09\n",
      "Steps: 88\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 204\n",
      "Reward: -104.51\n",
      "Average Reward (last 100): -113.10\n",
      "Steps: 62\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 205\n",
      "Reward: -130.37\n",
      "Average Reward (last 100): -113.19\n",
      "Steps: 125\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 206\n",
      "Reward: -117.96\n",
      "Average Reward (last 100): -113.25\n",
      "Steps: 74\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 207\n",
      "Reward: -108.47\n",
      "Average Reward (last 100): -113.30\n",
      "Steps: 100\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 208\n",
      "Reward: -109.88\n",
      "Average Reward (last 100): -113.17\n",
      "Steps: 130\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 209\n",
      "Reward: -102.83\n",
      "Average Reward (last 100): -112.93\n",
      "Steps: 53\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 210\n",
      "Reward: -118.72\n",
      "Average Reward (last 100): -113.00\n",
      "Steps: 84\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 211\n",
      "Reward: -104.59\n",
      "Average Reward (last 100): -112.87\n",
      "Steps: 54\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 212\n",
      "Reward: -109.91\n",
      "Average Reward (last 100): -112.90\n",
      "Steps: 99\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 213\n",
      "Reward: -107.65\n",
      "Average Reward (last 100): -112.65\n",
      "Steps: 73\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 214\n",
      "Reward: -102.10\n",
      "Average Reward (last 100): -112.66\n",
      "Steps: 80\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Episode 215\n",
      "Reward: -107.80\n",
      "Average Reward (last 100): -112.72\n",
      "Steps: 68\n",
      "Learning Rate: 1.00e-04\n",
      "Entropy Coefficient: 9.80e-03\n",
      "--------------------------------------------------\n",
      "Early stopping triggered!\n",
      "\n",
      "Training Summary:\n",
      "Total Episodes: 215\n",
      "Best Average Reward: -109.25\n",
      "Best Evaluation Reward: -101.28\n",
      "Final Learning Rate: 1.00e-04\n",
      "Final Entropy Coefficient: 9.80e-03\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "# Set environment variable to handle OpenMP runtime conflict\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "\n",
    "# Rest of the imports\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Normal\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "\n",
    "class RunningMeanStd:\n",
    "    def __init__(self, epsilon=1e-4):\n",
    "        self.mean = 0\n",
    "        self.std = 1\n",
    "        self.var = 1\n",
    "        self.epsilon = epsilon\n",
    "        self.count = self.epsilon\n",
    "\n",
    "    def update(self, x):\n",
    "        batch_mean = np.mean(x)\n",
    "        batch_var = np.var(x)\n",
    "        batch_count = len(x)\n",
    "        \n",
    "        delta = batch_mean - self.mean\n",
    "        self.mean = self.mean + delta * batch_count / (self.count + batch_count)\n",
    "        m_a = self.var * self.count\n",
    "        m_b = batch_var * batch_count\n",
    "        M2 = m_a + m_b + np.square(delta) * self.count * batch_count / (self.count + batch_count)\n",
    "        self.var = M2 / (self.count + batch_count)\n",
    "        self.std = np.sqrt(self.var)\n",
    "        self.count += batch_count\n",
    "\n",
    "\n",
    "\n",
    "# Actor and Critic Network definitions remain the same\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        # More robust architecture with normalization and dropout\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            \n",
    "            nn.Linear(512, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            \n",
    "            nn.Linear(256, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Separate heads for mean and std\n",
    "        self.mean_layer = nn.Sequential(\n",
    "            nn.Linear(128, action_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        self.log_std_layer = nn.Sequential(\n",
    "            nn.Linear(128, action_dim),\n",
    "            nn.Tanh()  # Bound the std\n",
    "        )\n",
    "        \n",
    "    def forward(self, state):\n",
    "        features = self.net(state)\n",
    "        mean = self.mean_layer(features)\n",
    "        log_std = self.log_std_layer(features)\n",
    "        # Ensure std is positive\n",
    "        std = torch.exp(log_std)\n",
    "        return mean, std\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super().__init__()\n",
    "        # More robust architecture matching the actor\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            \n",
    "            nn.Linear(512, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            \n",
    "            nn.Linear(256, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, state):\n",
    "        return self.net(state)\n",
    "\n",
    "\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.actor = Actor(state_dim, action_dim).to(self.device)\n",
    "        self.critic = Critic(state_dim).to(self.device)\n",
    "        \n",
    "        # More conservative learning rates and optimizers\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=1e-4)  # Reduced from 3e-4\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=5e-4)  # Reduced from 1e-3\n",
    "        \n",
    "      \n",
    "\n",
    "        # PPO hyperparameters with conservative values\n",
    "        self.gamma = 0.99  # Increased from 0.98 for longer-term rewards\n",
    "        self.gae_lambda = 0.95\n",
    "        self.clip_ratio = 0.1  # More conservative clipping\n",
    "        self.max_grad_norm = 0.3  # Reduced from 0.5 for more stable updates\n",
    "        \n",
    "        # Adaptive entropy coefficient\n",
    "        self.initial_entropy_coef = 0.01\n",
    "        self.entropy_coef = self.initial_entropy_coef\n",
    "        self.min_entropy_coef = 0.001\n",
    "        self.entropy_decay = 0.995\n",
    "        \n",
    "        # Normalization components\n",
    "        self.reward_normalizer = RunningMeanStd()\n",
    "        self.state_normalizer = RunningMeanStd()\n",
    "        self.advantage_normalizer = RunningMeanStd()\n",
    "        \n",
    "        # Experience replay buffer\n",
    "        self.max_memory_size = 2048\n",
    "        self.batch_size = 64\n",
    "        \n",
    "        # Training tracking\n",
    "        self.best_reward = float('-inf')\n",
    "        self.training_step = 0\n",
    "        \n",
    "    def normalize_state(self, state):\n",
    "        \"\"\"Normalize states using running statistics\"\"\"\n",
    "        if isinstance(state, np.ndarray):\n",
    "            self.state_normalizer.update(state)\n",
    "            return (state - self.state_normalizer.mean) / (self.state_normalizer.std + 1e-8)\n",
    "        return state\n",
    "\n",
    "    def normalize_reward(self, reward):\n",
    "        \"\"\"Normalize rewards using running statistics\"\"\"\n",
    "        self.reward_normalizer.update(np.array([reward]))\n",
    "        return (reward - self.reward_normalizer.mean) / (self.reward_normalizer.std + 1e-8)\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        \"\"\"Get action with normalized states\"\"\"\n",
    "        with torch.no_grad():\n",
    "            state = torch.FloatTensor(self.normalize_state(state)).to(self.device)\n",
    "            mean, std = self.actor(state)\n",
    "            dist = Normal(mean, std)\n",
    "            action = dist.sample()\n",
    "            action = torch.clamp(action, -1.0, 1.0)\n",
    "            log_prob = dist.log_prob(action).sum(dim=-1)\n",
    "            value = self.critic(state)\n",
    "            return action.cpu().numpy(), value.item(), log_prob.item()\n",
    "\n",
    "    def update(self, memory, episode_reward):\n",
    "        \"\"\"Updated PPO training step with improvements\"\"\"\n",
    "        \n",
    "        \n",
    "        # Prepare data\n",
    "        states = torch.FloatTensor(self.normalize_state(memory['states'])).to(self.device)\n",
    "        actions = torch.FloatTensor(memory['actions']).to(self.device)\n",
    "        old_log_probs = torch.FloatTensor(memory['log_probs']).to(self.device)\n",
    "        rewards = torch.FloatTensor([self.normalize_reward(r) for r in memory['rewards']]).to(self.device)\n",
    "        returns = torch.FloatTensor(memory['returns']).to(self.device)\n",
    "        advantages = torch.FloatTensor(memory['advantages']).to(self.device)\n",
    "        \n",
    "        # Normalize advantages\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        # Mini-batch updates with multiple epochs\n",
    "        indices = torch.randperm(states.size(0))\n",
    "        n_epochs = 4  # Multiple epochs over the same data\n",
    "        batch_size = self.batch_size\n",
    "        \n",
    "        for _ in range(n_epochs):\n",
    "            indices = np.arange(states.size(0))\n",
    "            np.random.shuffle(indices)\n",
    "            \n",
    "            for start_idx in range(0, states.size(0), self.batch_size):\n",
    "                # Get mini-batch\n",
    "                idx = indices[start_idx:start_idx + self.batch_size]\n",
    "                batch_states = states[idx]\n",
    "                batch_actions = actions[idx]\n",
    "                batch_old_log_probs = old_log_probs[idx]\n",
    "                batch_returns = returns[idx]\n",
    "                batch_advantages = advantages[idx]\n",
    "                \n",
    "                # Get current policy distribution\n",
    "                means, stds = self.actor(batch_states)\n",
    "                dist = Normal(means, stds)\n",
    "                new_log_probs = dist.log_prob(batch_actions).sum(dim=-1)\n",
    "                entropy = dist.entropy().mean()\n",
    "                \n",
    "                # Calculate policy loss with clipping\n",
    "                ratios = torch.exp(new_log_probs - batch_old_log_probs)\n",
    "                surr1 = ratios * batch_advantages\n",
    "                surr2 = torch.clamp(ratios, 1 - self.clip_ratio, 1 + self.clip_ratio) * batch_advantages\n",
    "                policy_loss = -torch.min(surr1, surr2).mean()\n",
    "                \n",
    "                # Calculate value loss\n",
    "                current_values = self.critic(batch_states).squeeze()\n",
    "                value_loss = 0.5 * ((current_values - batch_returns) ** 2).mean()\n",
    "                \n",
    "                # Calculate entropy loss for exploration\n",
    "                entropy_loss = -self.entropy_coef * entropy\n",
    "                \n",
    "                # Total loss\n",
    "                total_loss = policy_loss + value_loss + entropy_loss\n",
    "                \n",
    "                # Update networks with gradient clipping\n",
    "                self.actor_optimizer.zero_grad()\n",
    "                self.critic_optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                \n",
    "                # Clip gradients\n",
    "                torch.nn.utils.clip_grad_norm_(self.actor.parameters(), self.max_grad_norm)\n",
    "                torch.nn.utils.clip_grad_norm_(self.critic.parameters(), self.max_grad_norm)\n",
    "                \n",
    "                self.actor_optimizer.step()\n",
    "                self.critic_optimizer.step()\n",
    "                \n",
    "        # Decay entropy coefficient\n",
    "        if episode_reward > self.best_reward:\n",
    "            self.best_reward = episode_reward\n",
    "            self.entropy_coef = max(self.entropy_coef * self.entropy_decay, self.min_entropy_coef)\n",
    "        \n",
    "        self.training_step += 1\n",
    "        \n",
    "        # Return losses for monitoring\n",
    "        return {\n",
    "            'policy_loss': policy_loss.item(),\n",
    "            'value_loss': value_loss.item(),\n",
    "            'entropy': entropy.item()\n",
    "        }\n",
    "\n",
    "    def save(self, path):\n",
    "        \"\"\"Save model with additional training state\"\"\"\n",
    "        torch.save({\n",
    "            'actor_state_dict': self.actor.state_dict(),\n",
    "            'critic_state_dict': self.critic.state_dict(),\n",
    "            'actor_optimizer_state_dict': self.actor_optimizer.state_dict(),\n",
    "            'critic_optimizer_state_dict': self.critic_optimizer.state_dict(),\n",
    "            'best_reward': self.best_reward,\n",
    "            'training_step': self.training_step,\n",
    "            'entropy_coef': self.entropy_coef,\n",
    "            'reward_normalizer_state': {\n",
    "                'mean': self.reward_normalizer.mean,\n",
    "                'std': self.reward_normalizer.std,\n",
    "                'var': self.reward_normalizer.var,\n",
    "                'count': self.reward_normalizer.count\n",
    "            },\n",
    "            'state_normalizer_state': {\n",
    "                'mean': self.state_normalizer.mean,\n",
    "                'std': self.state_normalizer.std,\n",
    "                'var': self.state_normalizer.var,\n",
    "                'count': self.state_normalizer.count\n",
    "            }\n",
    "        }, path)\n",
    "\n",
    "    def load(self, path):\n",
    "        \"\"\"Load model with additional training state\"\"\"\n",
    "        checkpoint = torch.load(path)\n",
    "        self.actor.load_state_dict(checkpoint['actor_state_dict'])\n",
    "        self.critic.load_state_dict(checkpoint['critic_state_dict'])\n",
    "        self.actor_optimizer.load_state_dict(checkpoint['actor_optimizer_state_dict'])\n",
    "        self.critic_optimizer.load_state_dict(checkpoint['critic_optimizer_state_dict'])\n",
    "        self.best_reward = checkpoint['best_reward']\n",
    "        self.training_step = checkpoint['training_step']\n",
    "        self.entropy_coef = checkpoint['entropy_coef']\n",
    "        \n",
    "        # Load normalizer states\n",
    "        self.reward_normalizer.mean = checkpoint['reward_normalizer_state']['mean']\n",
    "        self.reward_normalizer.std = checkpoint['reward_normalizer_state']['std']\n",
    "        self.reward_normalizer.var = checkpoint['reward_normalizer_state']['var']\n",
    "        self.reward_normalizer.count = checkpoint['reward_normalizer_state']['count']\n",
    "        \n",
    "        self.state_normalizer.mean = checkpoint['state_normalizer_state']['mean']\n",
    "        self.state_normalizer.std = checkpoint['state_normalizer_state']['std']\n",
    "        self.state_normalizer.var = checkpoint['state_normalizer_state']['var']\n",
    "        self.state_normalizer.count = checkpoint['state_normalizer_state']['count']\n",
    "\n",
    "\n",
    "def plot_training_results(rewards):\n",
    "    \"\"\"\n",
    "    Plot the episode rewards and moving average with thicker lines\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert to numpy array if it isn't already\n",
    "        rewards = np.array(rewards, dtype=np.float32)\n",
    "        \n",
    "        # Create figure and close any existing plots\n",
    "        plt.close('all')\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # Plot individual episode rewards with moderate thickness\n",
    "        plt.plot(rewards, alpha=0.3, color='blue', label='Episode Reward', linewidth=1.0)\n",
    "        \n",
    "        # Calculate and plot moving average with thicker line\n",
    "        window_size = 100\n",
    "        if len(rewards) >= window_size:\n",
    "            moving_avg = np.zeros(len(rewards) - window_size + 1)\n",
    "            for i in range(len(moving_avg)):\n",
    "                moving_avg[i] = np.mean(rewards[i:i+window_size])\n",
    "            plt.plot(range(window_size-1, len(rewards)), \n",
    "                    moving_avg, \n",
    "                    color='red', \n",
    "                    label='100-Episode Moving Average',\n",
    "                    linewidth=3.0)  # Increased linewidth for moving average\n",
    "        \n",
    "        plt.title('Training Progress', fontsize=14, pad=15)  # Larger title with padding\n",
    "        plt.xlabel('Episode', fontsize=12)\n",
    "        plt.ylabel('Reward', fontsize=12)\n",
    "        plt.legend(fontsize=10)\n",
    "        plt.grid(True, alpha=0.3)  # Lighter grid\n",
    "        \n",
    "        # Enhance overall appearance\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the plot with high DPI for crisp lines\n",
    "        plt.savefig('training_results.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in plotting: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        # Clean up\n",
    "        plt.close('all')\n",
    "\n",
    "\n",
    "\n",
    "def train(total_episodes=10000, eval_frequency=100, render_every=20, show_gui=False):\n",
    "    \"\"\"\n",
    "    Improved training function with better monitoring and stability\n",
    "    \n",
    "    Args:\n",
    "        total_episodes (int): Maximum number of episodes to train\n",
    "        eval_frequency (int): How often to run evaluation episodes\n",
    "        render_every (int): How often to render training episodes\n",
    "        show_gui (bool): Whether to show the GUI during training\n",
    "    \"\"\"\n",
    "    # Create environment\n",
    "    env = gym.make('BipedalWalker-v3')\n",
    "    eval_env = gym.make('BipedalWalker-v3')  # Separate env for evaluation\n",
    "    \n",
    "    # Initialize agent\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    agent = PPO(state_dim, action_dim)\n",
    "    \n",
    "    # Training variables\n",
    "    best_reward = float('-inf')\n",
    "    best_avg_reward = float('-inf')\n",
    "    episode_rewards = []\n",
    "    eval_rewards = []\n",
    "    display_env = None\n",
    "    \n",
    "    # Early stopping variables\n",
    "    patience = 200000\n",
    "    patience_counter = 0\n",
    "    min_improvement = 1.0  # Minimum improvement to reset patience\n",
    "    \n",
    "    # Progress tracking\n",
    "    training_info = {\n",
    "        'episode_rewards': [],\n",
    "        'eval_rewards': [],\n",
    "        'eval_episodes': [],\n",
    "        'actor_losses': [],\n",
    "        'critic_losses': [],\n",
    "        'entropy_values': [],\n",
    "        'learning_rates': []\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        for episode in range(total_episodes):\n",
    "            # Initialize episode memory\n",
    "            memory = {\n",
    "                'states': [],\n",
    "                'actions': [],\n",
    "                'rewards': [],\n",
    "                'dones': [],\n",
    "                'log_probs': [],\n",
    "                'values': []\n",
    "            }\n",
    "            \n",
    "            # Reset environment\n",
    "            state, _ = env.reset()\n",
    "            episode_reward = 0\n",
    "            steps = 0\n",
    "            \n",
    "            # Create display environment if needed\n",
    "            if show_gui and episode % render_every == 0:\n",
    "                if display_env is not None:\n",
    "                    display_env.close()\n",
    "                display_env = gym.make('BipedalWalker-v3', render_mode='human')\n",
    "                display_state, _ = display_env.reset()\n",
    "            \n",
    "            # Episode loop\n",
    "            while True:\n",
    "                # Get action from policy\n",
    "                action, value, log_prob = agent.get_action(state)\n",
    "                \n",
    "                # Take step in environment\n",
    "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "                done = terminated or truncated\n",
    "                \n",
    "                # Store transition\n",
    "                memory['states'].append(state)\n",
    "                memory['actions'].append(action)\n",
    "                memory['rewards'].append(reward)\n",
    "                memory['dones'].append(done)\n",
    "                memory['log_probs'].append(log_prob)\n",
    "                memory['values'].append(value)\n",
    "                \n",
    "                episode_reward += reward\n",
    "                state = next_state\n",
    "                steps += 1\n",
    "                \n",
    "                # Display if needed\n",
    "                if show_gui and episode % render_every == 0 and display_env is not None:\n",
    "                    display_action, _, _ = agent.get_action(display_state)\n",
    "                    display_state, _, terminated, truncated, _ = display_env.step(display_action)\n",
    "                    if terminated or truncated:\n",
    "                        break\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "                \n",
    "                # Check memory limit\n",
    "                if len(memory['states']) >= agent.max_memory_size:\n",
    "                    # Get final value estimate for partial trajectory\n",
    "                    _, final_value, _ = agent.get_action(state)\n",
    "                    memory['values'].append(final_value)\n",
    "                    break\n",
    "            \n",
    "            # Compute returns and advantages\n",
    "            final_value = 0 if done else agent.get_action(state)[1]\n",
    "            returns, advantages = compute_returns(\n",
    "                memory['rewards'],\n",
    "                memory['dones'],\n",
    "                memory['values'],\n",
    "                final_value,\n",
    "                agent.gamma,\n",
    "                agent.gae_lambda\n",
    "            )\n",
    "            \n",
    "            # Update memory\n",
    "            memory['returns'] = returns\n",
    "            memory['advantages'] = advantages\n",
    "            \n",
    "            \n",
    "            loss_info = agent.update(memory, episode_reward)\n",
    "            losses = [loss_info]\n",
    "           \n",
    "            \n",
    "            # Store episode results\n",
    "            episode_rewards.append(episode_reward)\n",
    "            training_info['episode_rewards'].append(episode_reward)\n",
    "            training_info['actor_losses'].append(np.mean([l['policy_loss'] for l in losses]))\n",
    "            training_info['critic_losses'].append(np.mean([l['value_loss'] for l in losses]))\n",
    "            training_info['entropy_values'].append(np.mean([l['entropy'] for l in losses]))\n",
    "            training_info['learning_rates'].append(agent.actor_optimizer.param_groups[0]['lr'])\n",
    "            \n",
    "            # Calculate average reward\n",
    "            avg_reward = np.mean(episode_rewards[-100:]) if len(episode_rewards) >= 100 else np.mean(episode_rewards)\n",
    "            \n",
    "            # Run evaluation episodes\n",
    "            if episode % eval_frequency == 0:\n",
    "                eval_reward = evaluate_policy(agent, eval_env, episodes=5)\n",
    "                eval_rewards.append(eval_reward)\n",
    "                training_info['eval_rewards'].append(eval_reward)\n",
    "                training_info['eval_episodes'].append(episode)\n",
    "                \n",
    "                # Update best models if improved\n",
    "                if eval_reward > best_avg_reward:\n",
    "                    best_avg_reward = eval_reward\n",
    "                    agent.save('best_model.pth')\n",
    "                    print(f\"New best model saved with eval reward: {eval_reward:.2f}\")\n",
    "            \n",
    "            # Early stopping check\n",
    "            if avg_reward > best_reward + min_improvement:\n",
    "                best_reward = avg_reward\n",
    "                patience_counter = 0\n",
    "                print(f\"New best reward: {best_reward:.2f}\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            # Print progress\n",
    "            print(f\"Episode {episode + 1}\")\n",
    "            print(f\"Reward: {episode_reward:.2f}\")\n",
    "            print(f\"Average Reward (last 100): {avg_reward:.2f}\")\n",
    "            print(f\"Steps: {steps}\")\n",
    "            print(f\"Learning Rate: {agent.actor_optimizer.param_groups[0]['lr']:.2e}\")\n",
    "            print(f\"Entropy Coefficient: {agent.entropy_coef:.2e}\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            # Save training state periodically\n",
    "            if episode % 100 == 0:\n",
    "                agent.save(f'checkpoint_episode_{episode}.pth')\n",
    "                \n",
    "                # Plot and save training progress\n",
    "                plot_training_progress(training_info, save_path=f'training_progress_episode_{episode}.png')\n",
    "            \n",
    "            # Early stopping\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered!\")\n",
    "                break\n",
    "            \n",
    "            # Success criterion\n",
    "            if avg_reward >= 300:  # You can adjust this threshold\n",
    "                print(\"Environment solved!\")\n",
    "                break\n",
    "            \n",
    "            # Clear memory\n",
    "            del memory\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nTraining interrupted by user\")\n",
    "    \n",
    "    finally:\n",
    "        # Cleanup\n",
    "        env.close()\n",
    "        eval_env.close()\n",
    "        if display_env is not None:\n",
    "            display_env.close()\n",
    "        \n",
    "        # Save final model and progress\n",
    "        agent.save('final_model.pth')\n",
    "        plot_training_progress(training_info, save_path='final_training_progress.png')\n",
    "        \n",
    "        # Print final statistics\n",
    "        print(\"\\nTraining Summary:\")\n",
    "        print(f\"Total Episodes: {episode + 1}\")\n",
    "        print(f\"Best Average Reward: {best_reward:.2f}\")\n",
    "        print(f\"Best Evaluation Reward: {best_avg_reward:.2f}\")\n",
    "        print(f\"Final Learning Rate: {agent.actor_optimizer.param_groups[0]['lr']:.2e}\")\n",
    "        print(f\"Final Entropy Coefficient: {agent.entropy_coef:.2e}\")\n",
    "    \n",
    "    return agent, training_info\n",
    "\n",
    "\n",
    "def evaluate_policy(agent, env, episodes=5):\n",
    "    \"\"\"\n",
    "    Evaluate the policy without exploration\n",
    "    \"\"\"\n",
    "    eval_rewards = []\n",
    "    \n",
    "    for _ in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            with torch.no_grad():\n",
    "                # Use mean action (no sampling)\n",
    "                state_tensor = torch.FloatTensor(agent.normalize_state(state)).to(agent.device)\n",
    "                mean, _ = agent.actor(state_tensor)\n",
    "                action = torch.clamp(mean, -1.0, 1.0).cpu().numpy()\n",
    "            \n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            done = terminated or truncated\n",
    "        \n",
    "        eval_rewards.append(total_reward)\n",
    "    \n",
    "    return np.mean(eval_rewards)\n",
    "\n",
    "def plot_training_progress(info, save_path='training_progress.png'):\n",
    "    \"\"\"\n",
    "    Plot comprehensive training progress\n",
    "    \"\"\"\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Plot rewards\n",
    "    ax1.plot(info['episode_rewards'], alpha=0.3, color='blue', label='Episode Reward')\n",
    "    window_size = min(100, len(info['episode_rewards']))\n",
    "    if window_size > 0:\n",
    "        moving_avg = np.convolve(info['episode_rewards'], \n",
    "                               np.ones(window_size)/window_size, \n",
    "                               mode='valid')\n",
    "        ax1.plot(range(window_size-1, len(info['episode_rewards'])), \n",
    "                moving_avg, color='red', label='Moving Average')\n",
    "    ax1.set_title('Training Rewards')\n",
    "    ax1.set_xlabel('Episode')\n",
    "    ax1.set_ylabel('Reward')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot evaluation rewards\n",
    "    if info['eval_rewards']:\n",
    "        ax2.plot(\n",
    "        info['eval_episodes'],\n",
    "        info['eval_rewards'],\n",
    "        marker='o',\n",
    "        label='Eval Reward'\n",
    "    )\n",
    "    ax2.set_title('Evaluation Rewards')\n",
    "    ax2.set_xlabel('Episode')\n",
    "    ax2.set_ylabel('Reward')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot losses\n",
    "    ax3.plot(info['actor_losses'], label='Actor Loss', alpha=0.7)\n",
    "    ax3.plot(info['critic_losses'], label='Critic Loss', alpha=0.7)\n",
    "    ax3.set_title('Losses')\n",
    "    ax3.set_xlabel('Episode')\n",
    "    ax3.set_ylabel('Loss')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot learning rate and entropy\n",
    "    ax4.plot(info['learning_rates'], label='Learning Rate', color='green')\n",
    "    ax4.set_title('Learning Rate')\n",
    "    ax4.set_xlabel('Episode')\n",
    "    ax4.set_ylabel('Learning Rate')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def compute_returns(rewards, dones, values, next_value, gamma, gae_lambda):\n",
    "    returns = []\n",
    "    advantages = []\n",
    "    gae = 0\n",
    "    values = values + [next_value]  # Append the next value\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        delta = rewards[step] + gamma * values[step + 1] * (1 - dones[step]) - values[step]\n",
    "        gae = delta + gamma * gae_lambda * (1 - dones[step]) * gae\n",
    "        returns.insert(0, gae + values[step])\n",
    "        advantages.insert(0, gae)\n",
    "    return returns, advantages\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set random seeds\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Train the agent\n",
    "    print(\"Starting training...\")\n",
    "    agent, training_info = train(render_every=100, total_episodes=10000, show_gui=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
