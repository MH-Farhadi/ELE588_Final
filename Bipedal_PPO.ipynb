{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 | Timesteps 1600 | Avg Loss: 0.6301 | Actor Loss: -0.0211 | Critic Loss: 0.6794 | Entropy: 1.4158\n",
      "Episode 1 | Timesteps 1705 | Avg Loss: 948.8256 | Actor Loss: 0.0028 | Critic Loss: 948.8510 | Entropy: 1.4126\n",
      "Episode 2 | Timesteps 3305 | Avg Loss: 0.3269 | Actor Loss: -0.0187 | Critic Loss: 0.3738 | Entropy: 1.4098\n",
      "Episode 3 | Timesteps 3386 | Avg Loss: 1289.9883 | Actor Loss: -0.0012 | Critic Loss: 1290.0175 | Entropy: 1.4062\n",
      "Episode 4 | Timesteps 4986 | Avg Loss: 0.4889 | Actor Loss: -0.0190 | Critic Loss: 0.5360 | Entropy: 1.4057\n",
      "Episode 5 | Timesteps 6586 | Avg Loss: 0.3456 | Actor Loss: -0.0158 | Critic Loss: 0.3895 | Entropy: 1.4038\n",
      "Episode 6 | Timesteps 6631 | Avg Loss: 2168.1223 | Actor Loss: 0.0038 | Critic Loss: 2168.1466 | Entropy: 1.4027\n",
      "Episode 7 | Timesteps 6701 | Avg Loss: 1423.9998 | Actor Loss: 0.0002 | Critic Loss: 1424.0277 | Entropy: 1.4026\n",
      "Episode 8 | Timesteps 6760 | Avg Loss: 1665.0232 | Actor Loss: -0.0009 | Critic Loss: 1665.0521 | Entropy: 1.4026\n",
      "Episode 9 | Timesteps 6818 | Avg Loss: 1559.0867 | Actor Loss: -0.0020 | Critic Loss: 1559.1168 | Entropy: 1.4026\n",
      "Episode 10 | Average Reward (last 10 episodes): -112.20 | Average Length: 681.80\n",
      "Episode 10 | Timesteps 6889 | Avg Loss: 1252.5518 | Actor Loss: -0.0010 | Critic Loss: 1252.5809 | Entropy: 1.4026\n",
      "Episode 11 | Timesteps 6943 | Avg Loss: 1668.2383 | Actor Loss: 0.0004 | Critic Loss: 1668.2660 | Entropy: 1.4026\n",
      "Episode 12 | Timesteps 8543 | Avg Loss: 0.6244 | Actor Loss: -0.0171 | Critic Loss: 0.6695 | Entropy: 1.4017\n",
      "Episode 13 | Timesteps 8660 | Avg Loss: 778.9900 | Actor Loss: -0.0032 | Critic Loss: 779.0212 | Entropy: 1.4002\n",
      "Episode 14 | Timesteps 8715 | Avg Loss: 1500.3267 | Actor Loss: 0.0020 | Critic Loss: 1500.3527 | Entropy: 1.4001\n",
      "Episode 15 | Timesteps 10315 | Avg Loss: 0.5483 | Actor Loss: -0.0129 | Critic Loss: 0.5892 | Entropy: 1.3980\n",
      "Episode 16 | Timesteps 10403 | Avg Loss: 1166.3571 | Actor Loss: 0.0001 | Critic Loss: 1166.3849 | Entropy: 1.3946\n",
      "Episode 17 | Timesteps 10468 | Avg Loss: 1337.4340 | Actor Loss: -0.0003 | Critic Loss: 1337.4621 | Entropy: 1.3945\n",
      "Episode 18 | Timesteps 10538 | Avg Loss: 1079.4976 | Actor Loss: -0.0004 | Critic Loss: 1079.5259 | Entropy: 1.3944\n",
      "Episode 19 | Timesteps 10650 | Avg Loss: 723.2929 | Actor Loss: -0.0001 | Critic Loss: 723.3209 | Entropy: 1.3944\n",
      "Episode 20 | Average Reward (last 10 episodes): -111.49 | Average Length: 383.20\n",
      "Episode 20 | Timesteps 12250 | Avg Loss: 1.5269 | Actor Loss: -0.0116 | Critic Loss: 1.5663 | Entropy: 1.3947\n",
      "Episode 21 | Timesteps 12320 | Avg Loss: 1181.8301 | Actor Loss: -0.0049 | Critic Loss: 1181.8630 | Entropy: 1.3948\n",
      "Episode 22 | Timesteps 13920 | Avg Loss: 0.9023 | Actor Loss: -0.0085 | Critic Loss: 0.9386 | Entropy: 1.3941\n",
      "Episode 23 | Timesteps 15520 | Avg Loss: 0.6749 | Actor Loss: -0.0103 | Critic Loss: 0.7130 | Entropy: 1.3919\n",
      "Episode 24 | Timesteps 17120 | Avg Loss: 0.5133 | Actor Loss: -0.0149 | Critic Loss: 0.5560 | Entropy: 1.3923\n",
      "Episode 25 | Timesteps 17187 | Avg Loss: 1397.2823 | Actor Loss: 0.0012 | Critic Loss: 1397.3088 | Entropy: 1.3938\n",
      "Episode 26 | Timesteps 17268 | Avg Loss: 1069.2461 | Actor Loss: -0.0006 | Critic Loss: 1069.2745 | Entropy: 1.3939\n",
      "Episode 27 | Timesteps 18868 | Avg Loss: 0.7226 | Actor Loss: -0.0078 | Critic Loss: 0.7583 | Entropy: 1.3930\n",
      "Episode 28 | Timesteps 20468 | Avg Loss: 0.5481 | Actor Loss: -0.0114 | Critic Loss: 0.5874 | Entropy: 1.3913\n",
      "Episode 29 | Timesteps 20561 | Avg Loss: 1031.4647 | Actor Loss: -0.0013 | Critic Loss: 1031.4938 | Entropy: 1.3906\n",
      "Episode 30 | Average Reward (last 10 episodes): -108.65 | Average Length: 991.10\n",
      "Episode 30 | Timesteps 22161 | Avg Loss: 0.5443 | Actor Loss: -0.0094 | Critic Loss: 0.5816 | Entropy: 1.3902\n",
      "Episode 31 | Timesteps 22284 | Avg Loss: 746.8030 | Actor Loss: -0.0004 | Critic Loss: 746.8312 | Entropy: 1.3893\n",
      "Episode 32 | Timesteps 22385 | Avg Loss: 878.9498 | Actor Loss: -0.0015 | Critic Loss: 878.9791 | Entropy: 1.3892\n",
      "Episode 33 | Timesteps 23985 | Avg Loss: 0.7259 | Actor Loss: -0.0118 | Critic Loss: 0.7655 | Entropy: 1.3892\n",
      "Episode 34 | Timesteps 24058 | Avg Loss: 1187.7262 | Actor Loss: 0.0051 | Critic Loss: 1187.7489 | Entropy: 1.3888\n",
      "Episode 35 | Timesteps 24111 | Avg Loss: 1701.0319 | Actor Loss: -0.0004 | Critic Loss: 1701.0601 | Entropy: 1.3888\n",
      "Episode 36 | Timesteps 24180 | Avg Loss: 1290.0532 | Actor Loss: -0.0016 | Critic Loss: 1290.0826 | Entropy: 1.3888\n",
      "Episode 37 | Timesteps 24222 | Avg Loss: 1647.3941 | Actor Loss: -0.0005 | Critic Loss: 1647.4224 | Entropy: 1.3888\n",
      "Episode 38 | Timesteps 24282 | Avg Loss: 1220.4455 | Actor Loss: -0.0008 | Critic Loss: 1220.4741 | Entropy: 1.3888\n",
      "Episode 39 | Timesteps 24387 | Avg Loss: 566.5375 | Actor Loss: 0.0000 | Critic Loss: 566.5652 | Entropy: 1.3888\n",
      "Episode 40 | Average Reward (last 10 episodes): -106.32 | Average Length: 382.60\n",
      "Episode 40 | Timesteps 25987 | Avg Loss: 1.7780 | Actor Loss: -0.0080 | Critic Loss: 1.8137 | Entropy: 1.3883\n",
      "Episode 41 | Timesteps 26043 | Avg Loss: 1091.0913 | Actor Loss: 0.0001 | Critic Loss: 1091.1189 | Entropy: 1.3870\n",
      "Episode 42 | Timesteps 27643 | Avg Loss: 1.2986 | Actor Loss: -0.0085 | Critic Loss: 1.3348 | Entropy: 1.3866\n",
      "Episode 43 | Timesteps 29243 | Avg Loss: 0.7848 | Actor Loss: -0.0096 | Critic Loss: 0.8221 | Entropy: 1.3856\n",
      "Episode 44 | Timesteps 30843 | Avg Loss: 0.6017 | Actor Loss: -0.0074 | Critic Loss: 0.6368 | Entropy: 1.3846\n",
      "Episode 45 | Timesteps 32443 | Avg Loss: 0.6265 | Actor Loss: -0.0134 | Critic Loss: 0.6675 | Entropy: 1.3816\n",
      "Episode 46 | Timesteps 34043 | Avg Loss: 0.3538 | Actor Loss: -0.0150 | Critic Loss: 0.3964 | Entropy: 1.3771\n",
      "Episode 47 | Timesteps 34140 | Avg Loss: 1059.7935 | Actor Loss: -0.0034 | Critic Loss: 1059.8243 | Entropy: 1.3741\n",
      "Episode 48 | Timesteps 34194 | Avg Loss: 1590.6192 | Actor Loss: -0.0012 | Critic Loss: 1590.6479 | Entropy: 1.3740\n",
      "Episode 49 | Timesteps 34316 | Avg Loss: 618.4142 | Actor Loss: 0.0000 | Critic Loss: 618.4416 | Entropy: 1.3740\n",
      "Episode 50 | Average Reward (last 10 episodes): -91.43 | Average Length: 992.90\n",
      "Episode 50 | Timesteps 35916 | Avg Loss: 0.7744 | Actor Loss: -0.0118 | Critic Loss: 0.8137 | Entropy: 1.3727\n",
      "Episode 51 | Timesteps 37516 | Avg Loss: 0.4394 | Actor Loss: -0.0121 | Critic Loss: 0.4789 | Entropy: 1.3695\n",
      "Episode 52 | Timesteps 37678 | Avg Loss: 491.7175 | Actor Loss: 0.0102 | Critic Loss: 491.7346 | Entropy: 1.3678\n",
      "Episode 53 | Timesteps 37767 | Avg Loss: 809.7450 | Actor Loss: -0.0012 | Critic Loss: 809.7736 | Entropy: 1.3678\n",
      "Episode 54 | Timesteps 39367 | Avg Loss: 0.9058 | Actor Loss: -0.0094 | Critic Loss: 0.9425 | Entropy: 1.3662\n",
      "Episode 55 | Timesteps 40967 | Avg Loss: 0.6860 | Actor Loss: -0.0117 | Critic Loss: 0.7250 | Entropy: 1.3625\n",
      "Episode 56 | Timesteps 41036 | Avg Loss: 1357.3402 | Actor Loss: 0.0004 | Critic Loss: 1357.3670 | Entropy: 1.3612\n",
      "Episode 57 | Timesteps 42011 | Avg Loss: 60.1514 | Actor Loss: -0.0027 | Critic Loss: 60.1813 | Entropy: 1.3612\n",
      "Episode 58 | Timesteps 43611 | Avg Loss: 0.6335 | Actor Loss: -0.0129 | Critic Loss: 0.6736 | Entropy: 1.3612\n",
      "Episode 59 | Timesteps 45211 | Avg Loss: 0.7769 | Actor Loss: -0.0102 | Critic Loss: 0.8144 | Entropy: 1.3605\n",
      "Episode 60 | Average Reward (last 10 episodes): -83.29 | Average Length: 1089.50\n",
      "Episode 60 | Timesteps 45405 | Avg Loss: 351.1416 | Actor Loss: 0.0097 | Critic Loss: 351.1590 | Entropy: 1.3599\n",
      "Episode 61 | Timesteps 45461 | Avg Loss: 1364.7079 | Actor Loss: -0.0007 | Critic Loss: 1364.7359 | Entropy: 1.3599\n",
      "Episode 62 | Timesteps 47061 | Avg Loss: 1.3819 | Actor Loss: -0.0080 | Critic Loss: 1.4171 | Entropy: 1.3594\n",
      "Episode 63 | Timesteps 47127 | Avg Loss: 1652.5845 | Actor Loss: -0.0003 | Critic Loss: 1652.6120 | Entropy: 1.3583\n",
      "Episode 64 | Timesteps 47237 | Avg Loss: 890.9450 | Actor Loss: 0.0008 | Critic Loss: 890.9713 | Entropy: 1.3583\n",
      "Episode 65 | Timesteps 47288 | Avg Loss: 1429.7926 | Actor Loss: 0.0007 | Critic Loss: 1429.8191 | Entropy: 1.3583\n",
      "Episode 66 | Timesteps 47391 | Avg Loss: 570.2967 | Actor Loss: -0.0024 | Critic Loss: 570.3262 | Entropy: 1.3583\n",
      "Episode 67 | Timesteps 48991 | Avg Loss: 1.3164 | Actor Loss: -0.0129 | Critic Loss: 1.3564 | Entropy: 1.3572\n",
      "Episode 68 | Timesteps 50591 | Avg Loss: 0.7324 | Actor Loss: -0.0111 | Critic Loss: 0.7706 | Entropy: 1.3542\n",
      "Episode 69 | Timesteps 52191 | Avg Loss: 0.4391 | Actor Loss: -0.0132 | Critic Loss: 0.4793 | Entropy: 1.3499\n",
      "Episode 70 | Average Reward (last 10 episodes): -84.07 | Average Length: 698.00\n",
      "Episode 70 | Timesteps 53791 | Avg Loss: 0.4117 | Actor Loss: -0.0144 | Critic Loss: 0.4530 | Entropy: 1.3451\n",
      "Episode 71 | Timesteps 55391 | Avg Loss: 0.3681 | Actor Loss: -0.0154 | Critic Loss: 0.4104 | Entropy: 1.3423\n",
      "Episode 72 | Timesteps 55457 | Avg Loss: 1473.8346 | Actor Loss: -0.0048 | Critic Loss: 1473.8663 | Entropy: 1.3409\n",
      "Episode 73 | Timesteps 55539 | Avg Loss: 933.6807 | Actor Loss: 0.0002 | Critic Loss: 933.7074 | Entropy: 1.3409\n",
      "Episode 74 | Timesteps 55622 | Avg Loss: 858.6307 | Actor Loss: 0.0004 | Critic Loss: 858.6571 | Entropy: 1.3409\n",
      "Episode 75 | Timesteps 55697 | Avg Loss: 872.6020 | Actor Loss: 0.0000 | Critic Loss: 872.6288 | Entropy: 1.3409\n",
      "Episode 76 | Timesteps 56289 | Avg Loss: 101.3033 | Actor Loss: 0.0001 | Critic Loss: 101.3300 | Entropy: 1.3409\n",
      "Episode 77 | Timesteps 57889 | Avg Loss: 0.9025 | Actor Loss: -0.0119 | Critic Loss: 0.9412 | Entropy: 1.3402\n",
      "Episode 78 | Timesteps 57951 | Avg Loss: 1486.6312 | Actor Loss: 0.0007 | Critic Loss: 1486.6572 | Entropy: 1.3387\n",
      "Episode 79 | Timesteps 58309 | Avg Loss: 129.3141 | Actor Loss: -0.0118 | Critic Loss: 129.3527 | Entropy: 1.3386\n",
      "Episode 80 | Average Reward (last 10 episodes): -88.39 | Average Length: 611.80\n",
      "Episode 80 | Timesteps 58724 | Avg Loss: 102.4626 | Actor Loss: -0.0198 | Critic Loss: 102.5093 | Entropy: 1.3386\n",
      "Episode 81 | Timesteps 59130 | Avg Loss: 89.3374 | Actor Loss: 0.0062 | Critic Loss: 89.3579 | Entropy: 1.3386\n",
      "Episode 82 | Timesteps 59682 | Avg Loss: 34.4770 | Actor Loss: -0.0052 | Critic Loss: 34.5089 | Entropy: 1.3386\n",
      "Episode 83 | Timesteps 59744 | Avg Loss: 449.3793 | Actor Loss: 0.0004 | Critic Loss: 449.4056 | Entropy: 1.3386\n",
      "Episode 84 | Timesteps 60028 | Avg Loss: 103.8364 | Actor Loss: -0.0073 | Critic Loss: 103.8704 | Entropy: 1.3386\n",
      "Episode 85 | Timesteps 60374 | Avg Loss: 44.1533 | Actor Loss: -0.0053 | Critic Loss: 44.1854 | Entropy: 1.3386\n",
      "Episode 86 | Timesteps 61974 | Avg Loss: 5.9722 | Actor Loss: -0.0095 | Critic Loss: 6.0084 | Entropy: 1.3383\n",
      "Episode 87 | Timesteps 63376 | Avg Loss: 13.6393 | Actor Loss: -0.0082 | Critic Loss: 13.6742 | Entropy: 1.3375\n",
      "Episode 88 | Timesteps 63451 | Avg Loss: 987.2573 | Actor Loss: -0.0018 | Critic Loss: 987.2859 | Entropy: 1.3373\n",
      "Episode 89 | Timesteps 65051 | Avg Loss: 10.1750 | Actor Loss: -0.0083 | Critic Loss: 10.2101 | Entropy: 1.3372\n",
      "Episode 90 | Average Reward (last 10 episodes): -94.21 | Average Length: 674.20\n",
      "Episode 90 | Timesteps 66576 | Avg Loss: 10.9116 | Actor Loss: -0.0083 | Critic Loss: 10.9466 | Entropy: 1.3371\n",
      "Episode 91 | Timesteps 68176 | Avg Loss: 1.4435 | Actor Loss: -0.0109 | Critic Loss: 1.4811 | Entropy: 1.3365\n",
      "Episode 92 | Timesteps 69776 | Avg Loss: 0.3832 | Actor Loss: -0.0170 | Critic Loss: 0.4268 | Entropy: 1.3323\n",
      "Episode 93 | Timesteps 69848 | Avg Loss: 1718.8669 | Actor Loss: 0.0073 | Critic Loss: 1718.8862 | Entropy: 1.3288\n",
      "Episode 94 | Timesteps 71448 | Avg Loss: 0.7452 | Actor Loss: -0.0173 | Critic Loss: 0.7891 | Entropy: 1.3273\n",
      "Episode 95 | Timesteps 71499 | Avg Loss: 897.3671 | Actor Loss: -0.0112 | Critic Loss: 897.4048 | Entropy: 1.3256\n",
      "Episode 96 | Timesteps 72049 | Avg Loss: 32.2666 | Actor Loss: -0.0211 | Critic Loss: 32.3142 | Entropy: 1.3256\n",
      "Episode 97 | Timesteps 72854 | Avg Loss: 19.4289 | Actor Loss: -0.0095 | Critic Loss: 19.4648 | Entropy: 1.3256\n",
      "Episode 98 | Timesteps 72921 | Avg Loss: 318.9775 | Actor Loss: 0.0005 | Critic Loss: 319.0035 | Entropy: 1.3257\n",
      "Episode 99 | Timesteps 74448 | Avg Loss: 29.5070 | Actor Loss: -0.0084 | Critic Loss: 29.5419 | Entropy: 1.3256\n",
      "Episode 100 | Average Reward (last 10 episodes): -71.32 | Average Length: 939.70\n",
      "\n",
      "Evaluating at episode 100...\n",
      "Evaluation over 3 episodes: Average Reward = 94.21850274928107\n",
      "Best model saved with average reward 94.21850274928107 at episode 100\n",
      "\n",
      "Episode 100 | Timesteps 74976 | Avg Loss: 53.1536 | Actor Loss: -0.0252 | Critic Loss: 53.2053 | Entropy: 1.3255\n",
      "Episode 101 | Timesteps 76576 | Avg Loss: 1.6832 | Actor Loss: -0.0097 | Critic Loss: 1.7195 | Entropy: 1.3243\n",
      "Episode 102 | Timesteps 78176 | Avg Loss: 2.4768 | Actor Loss: -0.0123 | Critic Loss: 2.5155 | Entropy: 1.3223\n",
      "Episode 103 | Timesteps 79382 | Avg Loss: 15.9619 | Actor Loss: -0.0088 | Critic Loss: 15.9972 | Entropy: 1.3222\n",
      "Episode 104 | Timesteps 80573 | Avg Loss: 12.7898 | Actor Loss: -0.0083 | Critic Loss: 12.8246 | Entropy: 1.3227\n",
      "Episode 105 | Timesteps 82173 | Avg Loss: 1.4959 | Actor Loss: -0.0126 | Critic Loss: 1.5350 | Entropy: 1.3234\n",
      "Episode 106 | Timesteps 82256 | Avg Loss: 409.3252 | Actor Loss: 0.0094 | Critic Loss: 409.3422 | Entropy: 1.3236\n",
      "Episode 107 | Timesteps 83856 | Avg Loss: 0.5010 | Actor Loss: -0.0098 | Critic Loss: 0.5372 | Entropy: 1.3219\n",
      "Episode 108 | Timesteps 83905 | Avg Loss: 507.0391 | Actor Loss: -0.0075 | Critic Loss: 507.0730 | Entropy: 1.3197\n",
      "Episode 109 | Timesteps 84836 | Avg Loss: 12.1503 | Actor Loss: -0.0127 | Critic Loss: 12.1894 | Entropy: 1.3198\n",
      "Episode 110 | Average Reward (last 10 episodes): -42.85 | Average Length: 1038.80\n",
      "Episode 110 | Timesteps 86436 | Avg Loss: 0.6589 | Actor Loss: -0.0117 | Critic Loss: 0.6970 | Entropy: 1.3181\n",
      "Episode 111 | Timesteps 88036 | Avg Loss: 0.7642 | Actor Loss: -0.0145 | Critic Loss: 0.8049 | Entropy: 1.3134\n",
      "Episode 112 | Timesteps 88206 | Avg Loss: 134.5522 | Actor Loss: 0.0098 | Critic Loss: 134.5686 | Entropy: 1.3117\n",
      "Episode 113 | Timesteps 88420 | Avg Loss: 510.7001 | Actor Loss: -0.0009 | Critic Loss: 510.7273 | Entropy: 1.3117\n",
      "Episode 114 | Timesteps 89638 | Avg Loss: 60.9445 | Actor Loss: -0.0096 | Critic Loss: 60.9803 | Entropy: 1.3115\n",
      "Episode 115 | Timesteps 89717 | Avg Loss: 588.4971 | Actor Loss: -0.0027 | Critic Loss: 588.5261 | Entropy: 1.3115\n",
      "Episode 116 | Timesteps 90450 | Avg Loss: 19.3793 | Actor Loss: -0.0086 | Critic Loss: 19.4142 | Entropy: 1.3115\n",
      "Episode 117 | Timesteps 90510 | Avg Loss: 1063.1479 | Actor Loss: -0.0016 | Critic Loss: 1063.1757 | Entropy: 1.3116\n",
      "Episode 118 | Timesteps 91237 | Avg Loss: 39.6872 | Actor Loss: -0.0102 | Critic Loss: 39.7236 | Entropy: 1.3116\n",
      "Episode 119 | Timesteps 91369 | Avg Loss: 83.5500 | Actor Loss: -0.0913 | Critic Loss: 83.6675 | Entropy: 1.3115\n",
      "Episode 120 | Average Reward (last 10 episodes): -56.26 | Average Length: 653.30\n",
      "Episode 120 | Timesteps 91456 | Avg Loss: 430.7958 | Actor Loss: 0.0003 | Critic Loss: 430.8217 | Entropy: 1.3116\n",
      "Episode 121 | Timesteps 92599 | Avg Loss: 44.1789 | Actor Loss: -0.0085 | Critic Loss: 44.2137 | Entropy: 1.3116\n",
      "Episode 122 | Timesteps 94199 | Avg Loss: 2.9699 | Actor Loss: -0.0112 | Critic Loss: 3.0073 | Entropy: 1.3116\n",
      "Episode 123 | Timesteps 95135 | Avg Loss: 36.6555 | Actor Loss: -0.0034 | Critic Loss: 36.6851 | Entropy: 1.3112\n",
      "Episode 124 | Timesteps 96735 | Avg Loss: 1.6139 | Actor Loss: -0.0098 | Critic Loss: 1.6499 | Entropy: 1.3112\n",
      "Episode 125 | Timesteps 98220 | Avg Loss: 4.1119 | Actor Loss: -0.0123 | Critic Loss: 4.1504 | Entropy: 1.3113\n",
      "Episode 126 | Timesteps 99820 | Avg Loss: 1.3401 | Actor Loss: -0.0070 | Critic Loss: 1.3734 | Entropy: 1.3107\n",
      "Episode 127 | Timesteps 99893 | Avg Loss: 307.5477 | Actor Loss: -0.0056 | Critic Loss: 307.5794 | Entropy: 1.3089\n",
      "Episode 128 | Timesteps 99979 | Avg Loss: 442.5032 | Actor Loss: -0.0024 | Critic Loss: 442.5318 | Entropy: 1.3088\n",
      "Episode 129 | Timesteps 101579 | Avg Loss: 1.3067 | Actor Loss: -0.0121 | Critic Loss: 1.3450 | Entropy: 1.3086\n",
      "Episode 130 | Average Reward (last 10 episodes): -31.90 | Average Length: 1021.00\n",
      "Episode 130 | Timesteps 102970 | Avg Loss: 7.7829 | Actor Loss: -0.0151 | Critic Loss: 7.8241 | Entropy: 1.3083\n",
      "Episode 131 | Timesteps 104570 | Avg Loss: 0.6952 | Actor Loss: -0.0105 | Critic Loss: 0.7318 | Entropy: 1.3060\n",
      "Episode 132 | Timesteps 106170 | Avg Loss: 2.5803 | Actor Loss: -0.0129 | Critic Loss: 2.6192 | Entropy: 1.3007\n",
      "Episode 133 | Timesteps 106638 | Avg Loss: 114.2690 | Actor Loss: -0.0040 | Critic Loss: 114.2990 | Entropy: 1.2993\n",
      "Episode 134 | Timesteps 108238 | Avg Loss: 0.6958 | Actor Loss: -0.0099 | Critic Loss: 0.7317 | Entropy: 1.3004\n",
      "Episode 135 | Timesteps 108767 | Avg Loss: 39.2124 | Actor Loss: 0.0077 | Critic Loss: 39.2307 | Entropy: 1.3015\n",
      "Episode 136 | Timesteps 110367 | Avg Loss: 0.9065 | Actor Loss: -0.0126 | Critic Loss: 0.9451 | Entropy: 1.2991\n",
      "Episode 137 | Timesteps 111967 | Avg Loss: 0.5288 | Actor Loss: -0.0145 | Critic Loss: 0.5692 | Entropy: 1.2918\n",
      "Episode 138 | Timesteps 113378 | Avg Loss: 5.5029 | Actor Loss: 0.0017 | Critic Loss: 5.5270 | Entropy: 1.2870\n",
      "Episode 139 | Timesteps 113476 | Avg Loss: 150.2672 | Actor Loss: -0.0019 | Critic Loss: 150.2948 | Entropy: 1.2863\n",
      "Episode 140 | Average Reward (last 10 episodes): -17.14 | Average Length: 1189.70\n",
      "Episode 140 | Timesteps 113528 | Avg Loss: 1275.6024 | Actor Loss: -0.0039 | Critic Loss: 1275.6321 | Entropy: 1.2863\n",
      "Episode 141 | Timesteps 115128 | Avg Loss: 1.0666 | Actor Loss: -0.0131 | Critic Loss: 1.1054 | Entropy: 1.2850\n",
      "Episode 142 | Timesteps 116728 | Avg Loss: 0.7425 | Actor Loss: -0.0115 | Critic Loss: 0.7796 | Entropy: 1.2807\n",
      "Episode 143 | Timesteps 118328 | Avg Loss: 0.7370 | Actor Loss: -0.0132 | Critic Loss: 0.7757 | Entropy: 1.2761\n",
      "Episode 144 | Timesteps 118420 | Avg Loss: 718.5836 | Actor Loss: 0.0028 | Critic Loss: 718.6063 | Entropy: 1.2740\n",
      "Episode 145 | Timesteps 118646 | Avg Loss: 82.6036 | Actor Loss: 0.0007 | Critic Loss: 82.6283 | Entropy: 1.2739\n",
      "Episode 146 | Timesteps 119443 | Avg Loss: 28.1133 | Actor Loss: -0.0108 | Critic Loss: 28.1496 | Entropy: 1.2738\n",
      "Episode 147 | Timesteps 121043 | Avg Loss: 0.9144 | Actor Loss: -0.0105 | Critic Loss: 0.9504 | Entropy: 1.2726\n",
      "Episode 148 | Timesteps 121540 | Avg Loss: 30.3180 | Actor Loss: -0.0109 | Critic Loss: 30.3543 | Entropy: 1.2705\n",
      "Episode 149 | Timesteps 122628 | Avg Loss: 20.5503 | Actor Loss: -0.0097 | Critic Loss: 20.5854 | Entropy: 1.2702\n",
      "Episode 150 | Average Reward (last 10 episodes): -16.99 | Average Length: 915.20\n",
      "Episode 150 | Timesteps 123876 | Avg Loss: 23.5697 | Actor Loss: -0.0092 | Critic Loss: 23.6043 | Entropy: 1.2700\n",
      "Episode 151 | Timesteps 125476 | Avg Loss: 0.5843 | Actor Loss: -0.0139 | Critic Loss: 0.6236 | Entropy: 1.2704\n",
      "Episode 152 | Timesteps 125857 | Avg Loss: 163.2813 | Actor Loss: -0.0054 | Critic Loss: 163.3121 | Entropy: 1.2703\n",
      "Episode 153 | Timesteps 127457 | Avg Loss: 0.6993 | Actor Loss: -0.0153 | Critic Loss: 0.7400 | Entropy: 1.2683\n",
      "Episode 154 | Timesteps 127561 | Avg Loss: 127.9121 | Actor Loss: -0.0101 | Critic Loss: 127.9474 | Entropy: 1.2651\n",
      "Episode 155 | Timesteps 128031 | Avg Loss: 34.9102 | Actor Loss: -0.0097 | Critic Loss: 34.9452 | Entropy: 1.2651\n",
      "Episode 156 | Timesteps 129631 | Avg Loss: 0.9993 | Actor Loss: -0.0117 | Critic Loss: 1.0363 | Entropy: 1.2635\n",
      "Episode 157 | Timesteps 131231 | Avg Loss: 0.8711 | Actor Loss: -0.0108 | Critic Loss: 0.9071 | Entropy: 1.2600\n",
      "Episode 158 | Timesteps 132556 | Avg Loss: 25.1598 | Actor Loss: -0.0026 | Critic Loss: 25.1875 | Entropy: 1.2581\n",
      "Episode 159 | Timesteps 134156 | Avg Loss: 1.1589 | Actor Loss: -0.0105 | Critic Loss: 1.1946 | Entropy: 1.2582\n",
      "Episode 160 | Average Reward (last 10 episodes): 2.87 | Average Length: 1152.80\n",
      "Episode 160 | Timesteps 135756 | Avg Loss: 2.7103 | Actor Loss: -0.0086 | Critic Loss: 2.7440 | Entropy: 1.2571\n",
      "Episode 161 | Timesteps 135840 | Avg Loss: 347.0782 | Actor Loss: -0.0011 | Critic Loss: 347.1044 | Entropy: 1.2557\n",
      "Episode 162 | Timesteps 135935 | Avg Loss: 173.2133 | Actor Loss: -0.0030 | Critic Loss: 173.2414 | Entropy: 1.2556\n",
      "Episode 163 | Timesteps 137535 | Avg Loss: 1.3196 | Actor Loss: -0.0101 | Critic Loss: 1.3547 | Entropy: 1.2525\n",
      "Episode 164 | Timesteps 139135 | Avg Loss: 0.4450 | Actor Loss: -0.0167 | Critic Loss: 0.4866 | Entropy: 1.2451\n",
      "Episode 165 | Timesteps 139231 | Avg Loss: 185.5419 | Actor Loss: 0.0031 | Critic Loss: 185.5637 | Entropy: 1.2417\n",
      "Episode 166 | Timesteps 140351 | Avg Loss: 31.6325 | Actor Loss: -0.0043 | Critic Loss: 31.6616 | Entropy: 1.2416\n",
      "Episode 167 | Timesteps 140480 | Avg Loss: 232.7311 | Actor Loss: 0.2249 | Critic Loss: 232.5310 | Entropy: 1.2417\n",
      "Episode 168 | Timesteps 140574 | Avg Loss: 222.9651 | Actor Loss: 0.0001 | Critic Loss: 222.9898 | Entropy: 1.2417\n",
      "Episode 169 | Timesteps 142174 | Avg Loss: 2.1351 | Actor Loss: -0.0115 | Critic Loss: 2.1715 | Entropy: 1.2409\n",
      "Episode 170 | Average Reward (last 10 episodes): -29.21 | Average Length: 801.80\n",
      "Episode 170 | Timesteps 143774 | Avg Loss: 1.4460 | Actor Loss: -0.0103 | Critic Loss: 1.4810 | Entropy: 1.2399\n",
      "Episode 171 | Timesteps 143817 | Avg Loss: 533.7989 | Actor Loss: 0.0011 | Critic Loss: 533.8227 | Entropy: 1.2404\n",
      "Episode 172 | Timesteps 145417 | Avg Loss: 0.8897 | Actor Loss: -0.0134 | Critic Loss: 0.9279 | Entropy: 1.2391\n",
      "Episode 173 | Timesteps 147017 | Avg Loss: 0.9106 | Actor Loss: -0.0169 | Critic Loss: 0.9522 | Entropy: 1.2341\n",
      "Episode 174 | Timesteps 148617 | Avg Loss: 0.8984 | Actor Loss: -0.0114 | Critic Loss: 0.9343 | Entropy: 1.2280\n",
      "Episode 175 | Timesteps 150217 | Avg Loss: 0.8086 | Actor Loss: -0.0158 | Critic Loss: 0.8488 | Entropy: 1.2244\n",
      "Episode 176 | Timesteps 150305 | Avg Loss: 193.8399 | Actor Loss: 0.0067 | Critic Loss: 193.8577 | Entropy: 1.2245\n",
      "Episode 177 | Timesteps 151905 | Avg Loss: 0.9169 | Actor Loss: -0.0110 | Critic Loss: 0.9524 | Entropy: 1.2243\n",
      "Episode 178 | Timesteps 153505 | Avg Loss: 1.2176 | Actor Loss: -0.0135 | Critic Loss: 1.2555 | Entropy: 1.2243\n",
      "Episode 179 | Timesteps 153626 | Avg Loss: 436.3148 | Actor Loss: -0.0060 | Critic Loss: 436.3452 | Entropy: 1.2239\n",
      "Episode 180 | Average Reward (last 10 episodes): 31.11 | Average Length: 1145.20\n",
      "Episode 180 | Timesteps 155226 | Avg Loss: 1.0509 | Actor Loss: -0.0211 | Critic Loss: 1.0965 | Entropy: 1.2222\n",
      "Episode 181 | Timesteps 156826 | Avg Loss: 0.7613 | Actor Loss: -0.0150 | Critic Loss: 0.8007 | Entropy: 1.2196\n",
      "Episode 182 | Timesteps 158426 | Avg Loss: 1.0871 | Actor Loss: -0.0110 | Critic Loss: 1.1225 | Entropy: 1.2171\n",
      "Episode 183 | Timesteps 160026 | Avg Loss: 0.6826 | Actor Loss: -0.0143 | Critic Loss: 0.7213 | Entropy: 1.2145\n",
      "Episode 184 | Timesteps 160124 | Avg Loss: 536.6450 | Actor Loss: -0.0022 | Critic Loss: 536.6715 | Entropy: 1.2148\n",
      "Episode 185 | Timesteps 161724 | Avg Loss: 0.8388 | Actor Loss: -0.0149 | Critic Loss: 0.8781 | Entropy: 1.2154\n",
      "Episode 186 | Timesteps 162387 | Avg Loss: 32.3792 | Actor Loss: -0.0118 | Critic Loss: 32.4153 | Entropy: 1.2157\n",
      "Episode 187 | Timesteps 162522 | Avg Loss: 161.7807 | Actor Loss: -0.0093 | Critic Loss: 161.8143 | Entropy: 1.2158\n",
      "Episode 188 | Timesteps 164044 | Avg Loss: 7.9246 | Actor Loss: -0.0169 | Critic Loss: 7.9658 | Entropy: 1.2159\n",
      "Episode 189 | Timesteps 165644 | Avg Loss: 1.6000 | Actor Loss: -0.0091 | Critic Loss: 1.6333 | Entropy: 1.2151\n",
      "Episode 190 | Average Reward (last 10 episodes): 44.41 | Average Length: 1201.80\n",
      "Episode 190 | Timesteps 166767 | Avg Loss: 18.9749 | Actor Loss: -0.0093 | Critic Loss: 19.0084 | Entropy: 1.2133\n",
      "Episode 191 | Timesteps 168367 | Avg Loss: 1.4743 | Actor Loss: -0.0132 | Critic Loss: 1.5117 | Entropy: 1.2144\n",
      "Episode 192 | Timesteps 169967 | Avg Loss: 0.7626 | Actor Loss: -0.0155 | Critic Loss: 0.8024 | Entropy: 1.2159\n",
      "Episode 193 | Timesteps 171567 | Avg Loss: 0.5415 | Actor Loss: -0.0149 | Critic Loss: 0.5807 | Entropy: 1.2140\n",
      "Episode 194 | Timesteps 171637 | Avg Loss: 1082.1770 | Actor Loss: 0.0061 | Critic Loss: 1082.1950 | Entropy: 1.2115\n",
      "Episode 195 | Timesteps 173237 | Avg Loss: 0.7058 | Actor Loss: -0.0110 | Critic Loss: 0.7410 | Entropy: 1.2099\n",
      "Episode 196 | Timesteps 174837 | Avg Loss: 0.7576 | Actor Loss: -0.0125 | Critic Loss: 0.7943 | Entropy: 1.2054\n",
      "Episode 197 | Timesteps 176437 | Avg Loss: 0.7631 | Actor Loss: -0.0163 | Critic Loss: 0.8034 | Entropy: 1.2011\n",
      "Episode 198 | Timesteps 178037 | Avg Loss: 0.5590 | Actor Loss: -0.0155 | Critic Loss: 0.5985 | Entropy: 1.1982\n",
      "Episode 199 | Timesteps 178113 | Avg Loss: 957.9972 | Actor Loss: 0.0130 | Critic Loss: 958.0081 | Entropy: 1.1971\n",
      "Episode 200 | Average Reward (last 10 episodes): 51.00 | Average Length: 1246.90\n",
      "\n",
      "Evaluating at episode 200...\n",
      "Evaluation over 3 episodes: Average Reward = 207.188299680445\n",
      "Best model saved with average reward 207.188299680445 at episode 200\n",
      "\n",
      "Episode 200 | Timesteps 179713 | Avg Loss: 0.7273 | Actor Loss: -0.0117 | Critic Loss: 0.7630 | Entropy: 1.1980\n",
      "Episode 201 | Timesteps 181313 | Avg Loss: 0.6716 | Actor Loss: -0.0133 | Critic Loss: 0.7088 | Entropy: 1.1973\n",
      "Episode 202 | Timesteps 182700 | Avg Loss: 7.0919 | Actor Loss: -0.0122 | Critic Loss: 7.1280 | Entropy: 1.1951\n",
      "Episode 203 | Timesteps 184300 | Avg Loss: 1.9443 | Actor Loss: -0.0134 | Critic Loss: 1.9816 | Entropy: 1.1946\n",
      "Episode 204 | Timesteps 185900 | Avg Loss: 0.9482 | Actor Loss: -0.0177 | Critic Loss: 0.9897 | Entropy: 1.1908\n",
      "Episode 205 | Timesteps 187500 | Avg Loss: 0.8661 | Actor Loss: -0.0111 | Critic Loss: 0.9009 | Entropy: 1.1865\n",
      "Episode 206 | Timesteps 187626 | Avg Loss: 181.3744 | Actor Loss: 0.0014 | Critic Loss: 181.3968 | Entropy: 1.1850\n",
      "Episode 207 | Timesteps 188638 | Avg Loss: 5.6751 | Actor Loss: -0.0156 | Critic Loss: 5.7143 | Entropy: 1.1851\n",
      "Episode 208 | Timesteps 190238 | Avg Loss: 1.2792 | Actor Loss: -0.0143 | Critic Loss: 1.3172 | Entropy: 1.1855\n",
      "Episode 209 | Timesteps 190297 | Avg Loss: 582.9904 | Actor Loss: 0.0079 | Critic Loss: 583.0062 | Entropy: 1.1854\n",
      "Episode 210 | Average Reward (last 10 episodes): 54.54 | Average Length: 1218.40\n",
      "Episode 210 | Timesteps 190876 | Avg Loss: 16.4896 | Actor Loss: -0.0129 | Critic Loss: 16.5262 | Entropy: 1.1854\n",
      "Episode 211 | Timesteps 190982 | Avg Loss: 497.4508 | Actor Loss: -0.0014 | Critic Loss: 497.4759 | Entropy: 1.1855\n",
      "Episode 212 | Timesteps 192582 | Avg Loss: 1.2809 | Actor Loss: -0.0105 | Critic Loss: 1.3151 | Entropy: 1.1856\n",
      "Episode 213 | Timesteps 194182 | Avg Loss: 1.7449 | Actor Loss: -0.0149 | Critic Loss: 1.7835 | Entropy: 1.1845\n",
      "Episode 214 | Timesteps 195782 | Avg Loss: 0.6416 | Actor Loss: -0.0145 | Critic Loss: 0.6798 | Entropy: 1.1802\n",
      "Episode 215 | Timesteps 197382 | Avg Loss: 0.9196 | Actor Loss: -0.0134 | Critic Loss: 0.9565 | Entropy: 1.1771\n",
      "Episode 216 | Timesteps 198982 | Avg Loss: 1.3232 | Actor Loss: -0.0123 | Critic Loss: 1.3590 | Entropy: 1.1768\n",
      "Episode 217 | Timesteps 200582 | Avg Loss: 1.1450 | Actor Loss: -0.0088 | Critic Loss: 1.1772 | Entropy: 1.1745\n",
      "Episode 218 | Timesteps 202182 | Avg Loss: 0.7472 | Actor Loss: -0.0133 | Critic Loss: 0.7839 | Entropy: 1.1731\n",
      "Episode 219 | Timesteps 203782 | Avg Loss: 1.0437 | Actor Loss: -0.0147 | Critic Loss: 1.0819 | Entropy: 1.1720\n",
      "Episode 220 | Average Reward (last 10 episodes): 86.68 | Average Length: 1348.50\n",
      "Episode 220 | Timesteps 205382 | Avg Loss: 1.4889 | Actor Loss: -0.0138 | Critic Loss: 1.5261 | Entropy: 1.1697\n",
      "Episode 221 | Timesteps 206982 | Avg Loss: 1.0879 | Actor Loss: -0.0169 | Critic Loss: 1.1282 | Entropy: 1.1660\n",
      "Episode 222 | Timesteps 208582 | Avg Loss: 0.7703 | Actor Loss: -0.0164 | Critic Loss: 0.8100 | Entropy: 1.1626\n",
      "Episode 223 | Timesteps 210182 | Avg Loss: 1.1202 | Actor Loss: -0.0120 | Critic Loss: 1.1555 | Entropy: 1.1624\n",
      "Episode 224 | Timesteps 210687 | Avg Loss: 22.3719 | Actor Loss: -0.0072 | Critic Loss: 22.4024 | Entropy: 1.1629\n",
      "Episode 225 | Timesteps 212287 | Avg Loss: 1.7974 | Actor Loss: -0.0133 | Critic Loss: 1.8339 | Entropy: 1.1619\n",
      "Episode 226 | Timesteps 213887 | Avg Loss: 0.8980 | Actor Loss: -0.0109 | Critic Loss: 0.9321 | Entropy: 1.1587\n",
      "Episode 227 | Timesteps 213958 | Avg Loss: 1258.1875 | Actor Loss: 0.0083 | Critic Loss: 1258.2024 | Entropy: 1.1576\n",
      "Episode 228 | Timesteps 215558 | Avg Loss: 1.0362 | Actor Loss: -0.0109 | Critic Loss: 1.0702 | Entropy: 1.1564\n",
      "Episode 229 | Timesteps 217158 | Avg Loss: 1.1472 | Actor Loss: -0.0141 | Critic Loss: 1.1844 | Entropy: 1.1536\n",
      "Episode 230 | Average Reward (last 10 episodes): 108.18 | Average Length: 1337.60\n",
      "Episode 230 | Timesteps 217249 | Avg Loss: 921.8473 | Actor Loss: -0.0006 | Critic Loss: 921.8710 | Entropy: 1.1524\n",
      "Episode 231 | Timesteps 218849 | Avg Loss: 1.8515 | Actor Loss: -0.0140 | Critic Loss: 1.8885 | Entropy: 1.1511\n",
      "Episode 232 | Timesteps 218916 | Avg Loss: 1055.0138 | Actor Loss: 0.0089 | Critic Loss: 1055.0278 | Entropy: 1.1494\n",
      "Episode 233 | Timesteps 220516 | Avg Loss: 1.2151 | Actor Loss: -0.0131 | Critic Loss: 1.2512 | Entropy: 1.1486\n",
      "Episode 234 | Timesteps 221529 | Avg Loss: 9.3135 | Actor Loss: -0.0101 | Critic Loss: 9.3465 | Entropy: 1.1469\n",
      "Episode 235 | Timesteps 221868 | Avg Loss: 105.5216 | Actor Loss: -0.0051 | Critic Loss: 105.5496 | Entropy: 1.1470\n",
      "Episode 236 | Timesteps 221944 | Avg Loss: 1044.0903 | Actor Loss: -0.0044 | Critic Loss: 1044.1177 | Entropy: 1.1470\n",
      "Episode 237 | Timesteps 223174 | Avg Loss: 35.6992 | Actor Loss: -0.0068 | Critic Loss: 35.7290 | Entropy: 1.1471\n",
      "Episode 238 | Timesteps 223239 | Avg Loss: 467.9136 | Actor Loss: -0.0054 | Critic Loss: 467.9420 | Entropy: 1.1472\n",
      "Episode 239 | Timesteps 223715 | Avg Loss: 58.9422 | Actor Loss: -0.0011 | Critic Loss: 58.9662 | Entropy: 1.1472\n",
      "Episode 240 | Average Reward (last 10 episodes): -27.45 | Average Length: 655.70\n",
      "Episode 240 | Timesteps 225315 | Avg Loss: 4.1685 | Actor Loss: -0.0059 | Critic Loss: 4.1973 | Entropy: 1.1464\n",
      "Episode 241 | Timesteps 226915 | Avg Loss: 1.6817 | Actor Loss: -0.0137 | Critic Loss: 1.7183 | Entropy: 1.1444\n",
      "Episode 242 | Timesteps 226994 | Avg Loss: 525.2748 | Actor Loss: -0.0012 | Critic Loss: 525.2989 | Entropy: 1.1428\n",
      "Episode 243 | Timesteps 228594 | Avg Loss: 0.8680 | Actor Loss: -0.0137 | Critic Loss: 0.9046 | Entropy: 1.1428\n",
      "Episode 244 | Timesteps 230194 | Avg Loss: 0.9048 | Actor Loss: -0.0138 | Critic Loss: 0.9413 | Entropy: 1.1401\n",
      "Episode 245 | Timesteps 231794 | Avg Loss: 1.0500 | Actor Loss: -0.0153 | Critic Loss: 1.0880 | Entropy: 1.1363\n",
      "Episode 246 | Timesteps 233394 | Avg Loss: 0.8348 | Actor Loss: -0.0143 | Critic Loss: 0.8718 | Entropy: 1.1341\n",
      "Episode 247 | Timesteps 234994 | Avg Loss: 0.8373 | Actor Loss: -0.0137 | Critic Loss: 0.8736 | Entropy: 1.1327\n",
      "Episode 248 | Timesteps 236509 | Avg Loss: 34.6487 | Actor Loss: 0.0005 | Critic Loss: 34.6708 | Entropy: 1.1321\n",
      "Episode 249 | Timesteps 238109 | Avg Loss: 2.7793 | Actor Loss: -0.0102 | Critic Loss: 2.8121 | Entropy: 1.1323\n",
      "Episode 250 | Average Reward (last 10 episodes): 116.54 | Average Length: 1439.40\n",
      "Episode 250 | Timesteps 239709 | Avg Loss: 2.0504 | Actor Loss: -0.0103 | Critic Loss: 2.0833 | Entropy: 1.1325\n",
      "Episode 251 | Timesteps 241309 | Avg Loss: 0.8744 | Actor Loss: -0.0146 | Critic Loss: 0.9117 | Entropy: 1.1319\n",
      "Episode 252 | Timesteps 241375 | Avg Loss: 1109.1057 | Actor Loss: -0.0031 | Critic Loss: 1109.1313 | Entropy: 1.1315\n",
      "Episode 253 | Timesteps 242975 | Avg Loss: 0.7153 | Actor Loss: -0.0197 | Critic Loss: 0.7576 | Entropy: 1.1305\n",
      "Episode 254 | Timesteps 244575 | Avg Loss: 0.8284 | Actor Loss: -0.0204 | Critic Loss: 0.8714 | Entropy: 1.1275\n",
      "Episode 255 | Timesteps 246175 | Avg Loss: 1.1744 | Actor Loss: -0.0172 | Critic Loss: 1.2141 | Entropy: 1.1260\n",
      "Episode 256 | Timesteps 247775 | Avg Loss: 1.1480 | Actor Loss: -0.0141 | Critic Loss: 1.1846 | Entropy: 1.1239\n",
      "Episode 257 | Timesteps 249375 | Avg Loss: 1.3326 | Actor Loss: -0.0123 | Critic Loss: 1.3673 | Entropy: 1.1211\n",
      "Episode 258 | Timesteps 250048 | Avg Loss: 14.6148 | Actor Loss: -0.0171 | Critic Loss: 14.6543 | Entropy: 1.1202\n",
      "Episode 259 | Timesteps 250198 | Avg Loss: 106.7427 | Actor Loss: -0.0086 | Critic Loss: 106.7738 | Entropy: 1.1201\n",
      "Episode 260 | Average Reward (last 10 episodes): 93.56 | Average Length: 1208.90\n",
      "Episode 260 | Timesteps 251798 | Avg Loss: 2.5670 | Actor Loss: -0.0119 | Critic Loss: 2.6013 | Entropy: 1.1193\n",
      "Episode 261 | Timesteps 253398 | Avg Loss: 1.0938 | Actor Loss: -0.0145 | Critic Loss: 1.1307 | Entropy: 1.1179\n",
      "Episode 262 | Timesteps 254998 | Avg Loss: 1.0500 | Actor Loss: -0.0176 | Critic Loss: 1.0899 | Entropy: 1.1152\n",
      "Episode 263 | Timesteps 255161 | Avg Loss: 321.6123 | Actor Loss: -0.0119 | Critic Loss: 321.6464 | Entropy: 1.1126\n",
      "Episode 264 | Timesteps 255263 | Avg Loss: 279.1435 | Actor Loss: -0.0018 | Critic Loss: 279.1676 | Entropy: 1.1125\n",
      "Episode 265 | Timesteps 256823 | Avg Loss: 7.2579 | Actor Loss: -0.0121 | Critic Loss: 7.2922 | Entropy: 1.1127\n",
      "Episode 266 | Timesteps 258423 | Avg Loss: 2.9369 | Actor Loss: -0.0143 | Critic Loss: 2.9735 | Entropy: 1.1123\n",
      "Episode 267 | Timesteps 260023 | Avg Loss: 2.2233 | Actor Loss: -0.0123 | Critic Loss: 2.2578 | Entropy: 1.1100\n",
      "Episode 268 | Timesteps 261623 | Avg Loss: 2.2461 | Actor Loss: -0.0144 | Critic Loss: 2.2826 | Entropy: 1.1073\n",
      "Episode 269 | Timesteps 262475 | Avg Loss: 25.9999 | Actor Loss: -0.0089 | Critic Loss: 26.0310 | Entropy: 1.1063\n",
      "Episode 270 | Average Reward (last 10 episodes): 101.33 | Average Length: 1227.70\n",
      "Episode 270 | Timesteps 263916 | Avg Loss: 15.7318 | Actor Loss: -0.0102 | Critic Loss: 15.7641 | Entropy: 1.1062\n",
      "Episode 271 | Timesteps 264276 | Avg Loss: 64.5211 | Actor Loss: -0.0058 | Critic Loss: 64.5490 | Entropy: 1.1061\n",
      "Episode 272 | Timesteps 265408 | Avg Loss: 26.1023 | Actor Loss: -0.0139 | Critic Loss: 26.1383 | Entropy: 1.1058\n",
      "Episode 273 | Timesteps 265473 | Avg Loss: 1143.1675 | Actor Loss: 0.0007 | Critic Loss: 1143.1890 | Entropy: 1.1054\n",
      "Episode 274 | Timesteps 267073 | Avg Loss: 3.8522 | Actor Loss: -0.0102 | Critic Loss: 3.8845 | Entropy: 1.1045\n",
      "Episode 275 | Timesteps 267186 | Avg Loss: 348.5852 | Actor Loss: 0.0001 | Critic Loss: 348.6072 | Entropy: 1.1029\n",
      "Episode 276 | Timesteps 268786 | Avg Loss: 1.0981 | Actor Loss: -0.0129 | Critic Loss: 1.1330 | Entropy: 1.1028\n",
      "Episode 277 | Timesteps 270386 | Avg Loss: 0.9593 | Actor Loss: -0.0164 | Critic Loss: 0.9977 | Entropy: 1.1004\n",
      "Episode 278 | Timesteps 271986 | Avg Loss: 1.3819 | Actor Loss: -0.0131 | Critic Loss: 1.4170 | Entropy: 1.0968\n",
      "Episode 279 | Timesteps 273586 | Avg Loss: 1.2973 | Actor Loss: -0.0118 | Critic Loss: 1.3310 | Entropy: 1.0956\n",
      "Episode 280 | Average Reward (last 10 episodes): 87.49 | Average Length: 1111.10\n",
      "Episode 280 | Timesteps 275079 | Avg Loss: 13.4424 | Actor Loss: -0.0075 | Critic Loss: 13.4718 | Entropy: 1.0951\n",
      "Episode 281 | Timesteps 276679 | Avg Loss: 1.6920 | Actor Loss: -0.0167 | Critic Loss: 1.7305 | Entropy: 1.0936\n",
      "Episode 282 | Timesteps 278279 | Avg Loss: 2.1840 | Actor Loss: -0.0129 | Critic Loss: 2.2187 | Entropy: 1.0933\n",
      "Episode 283 | Timesteps 278356 | Avg Loss: 1225.5645 | Actor Loss: 0.0044 | Critic Loss: 1225.5820 | Entropy: 1.0946\n",
      "Episode 284 | Timesteps 279956 | Avg Loss: 1.6501 | Actor Loss: -0.0158 | Critic Loss: 1.6877 | Entropy: 1.0936\n",
      "Episode 285 | Timesteps 281399 | Avg Loss: 14.5134 | Actor Loss: -0.0009 | Critic Loss: 14.5361 | Entropy: 1.0926\n",
      "Episode 286 | Timesteps 282999 | Avg Loss: 2.0877 | Actor Loss: -0.0117 | Critic Loss: 2.1213 | Entropy: 1.0928\n",
      "Episode 287 | Timesteps 284599 | Avg Loss: 1.5303 | Actor Loss: -0.0133 | Critic Loss: 1.5655 | Entropy: 1.0929\n",
      "Episode 288 | Timesteps 284878 | Avg Loss: 59.7973 | Actor Loss: 0.0137 | Critic Loss: 59.8055 | Entropy: 1.0940\n",
      "Episode 289 | Timesteps 286475 | Avg Loss: 1.8241 | Actor Loss: -0.0128 | Critic Loss: 1.8587 | Entropy: 1.0927\n",
      "Episode 290 | Average Reward (last 10 episodes): 126.63 | Average Length: 1288.90\n",
      "Episode 290 | Timesteps 286843 | Avg Loss: 40.5806 | Actor Loss: 0.0032 | Critic Loss: 40.5992 | Entropy: 1.0903\n",
      "Episode 291 | Timesteps 288443 | Avg Loss: 5.1003 | Actor Loss: -0.0134 | Critic Loss: 5.1355 | Entropy: 1.0899\n",
      "Episode 292 | Timesteps 288546 | Avg Loss: 555.1522 | Actor Loss: -0.0017 | Critic Loss: 555.1757 | Entropy: 1.0892\n",
      "Episode 293 | Timesteps 290146 | Avg Loss: 2.1191 | Actor Loss: -0.0142 | Critic Loss: 2.1550 | Entropy: 1.0873\n",
      "Episode 294 | Timesteps 290216 | Avg Loss: 666.7231 | Actor Loss: -0.0065 | Critic Loss: 666.7512 | Entropy: 1.0845\n",
      "Episode 295 | Timesteps 291816 | Avg Loss: 2.2779 | Actor Loss: -0.0157 | Critic Loss: 2.3153 | Entropy: 1.0842\n",
      "Episode 296 | Timesteps 293416 | Avg Loss: 1.7728 | Actor Loss: -0.0129 | Critic Loss: 1.8074 | Entropy: 1.0829\n",
      "Episode 297 | Timesteps 295016 | Avg Loss: 1.6355 | Actor Loss: -0.0157 | Critic Loss: 1.6728 | Entropy: 1.0826\n",
      "Episode 298 | Timesteps 295291 | Avg Loss: 51.6944 | Actor Loss: -0.0138 | Critic Loss: 51.7298 | Entropy: 1.0839\n",
      "Episode 299 | Timesteps 296879 | Avg Loss: 3.0401 | Actor Loss: -0.0124 | Critic Loss: 3.0742 | Entropy: 1.0839\n",
      "Episode 300 | Average Reward (last 10 episodes): 96.72 | Average Length: 1040.40\n",
      "\n",
      "Evaluating at episode 300...\n",
      "Evaluation over 3 episodes: Average Reward = 278.46545080643233\n",
      "Best model saved with average reward 278.46545080643233 at episode 300\n",
      "\n",
      "Episode 300 | Timesteps 298479 | Avg Loss: 4.1448 | Actor Loss: -0.0105 | Critic Loss: 4.1770 | Entropy: 1.0834\n",
      "Episode 301 | Timesteps 298538 | Avg Loss: 1161.6466 | Actor Loss: 0.0025 | Critic Loss: 1161.6657 | Entropy: 1.0831\n",
      "Episode 302 | Timesteps 300138 | Avg Loss: 1.6668 | Actor Loss: -0.0171 | Critic Loss: 1.7056 | Entropy: 1.0836\n",
      "Episode 303 | Timesteps 300435 | Avg Loss: 44.2917 | Actor Loss: -0.0012 | Critic Loss: 44.3145 | Entropy: 1.0843\n",
      "Episode 304 | Timesteps 302035 | Avg Loss: 1.7872 | Actor Loss: -0.0125 | Critic Loss: 1.8213 | Entropy: 1.0823\n",
      "Episode 305 | Timesteps 303635 | Avg Loss: 1.6222 | Actor Loss: -0.0199 | Critic Loss: 1.6636 | Entropy: 1.0804\n",
      "Episode 306 | Timesteps 305235 | Avg Loss: 1.6283 | Actor Loss: -0.0126 | Critic Loss: 1.6626 | Entropy: 1.0818\n",
      "Episode 307 | Timesteps 306835 | Avg Loss: 2.3435 | Actor Loss: -0.0149 | Critic Loss: 2.3801 | Entropy: 1.0818\n",
      "Episode 308 | Timesteps 307042 | Avg Loss: 102.7333 | Actor Loss: 0.0019 | Critic Loss: 102.7529 | Entropy: 1.0811\n",
      "Episode 309 | Timesteps 308588 | Avg Loss: 5.3020 | Actor Loss: -0.0077 | Critic Loss: 5.3314 | Entropy: 1.0810\n",
      "Episode 310 | Average Reward (last 10 episodes): 105.52 | Average Length: 1170.90\n",
      "Episode 310 | Timesteps 310178 | Avg Loss: 2.2740 | Actor Loss: -0.0137 | Critic Loss: 2.3093 | Entropy: 1.0806\n",
      "Episode 311 | Timesteps 311778 | Avg Loss: 1.3363 | Actor Loss: -0.0104 | Critic Loss: 1.3682 | Entropy: 1.0779\n",
      "Episode 312 | Timesteps 313378 | Avg Loss: 1.5172 | Actor Loss: -0.0120 | Critic Loss: 1.5508 | Entropy: 1.0757\n",
      "Episode 313 | Timesteps 314978 | Avg Loss: 1.8208 | Actor Loss: -0.0125 | Critic Loss: 1.8549 | Entropy: 1.0758\n",
      "Episode 314 | Timesteps 316578 | Avg Loss: 1.6609 | Actor Loss: -0.0138 | Critic Loss: 1.6962 | Entropy: 1.0750\n",
      "Episode 315 | Timesteps 317979 | Avg Loss: 8.0349 | Actor Loss: -0.0124 | Critic Loss: 8.0688 | Entropy: 1.0746\n",
      "Episode 316 | Timesteps 318665 | Avg Loss: 28.8092 | Actor Loss: -0.0203 | Critic Loss: 28.8510 | Entropy: 1.0751\n",
      "Episode 317 | Timesteps 320215 | Avg Loss: 2.1330 | Actor Loss: -0.0121 | Critic Loss: 2.1666 | Entropy: 1.0763\n",
      "Episode 318 | Timesteps 321815 | Avg Loss: 1.5578 | Actor Loss: -0.0123 | Critic Loss: 1.5916 | Entropy: 1.0774\n",
      "Episode 319 | Timesteps 323415 | Avg Loss: 1.4133 | Actor Loss: -0.0164 | Critic Loss: 1.4513 | Entropy: 1.0766\n",
      "Episode 320 | Average Reward (last 10 episodes): 189.34 | Average Length: 1482.70\n",
      "Episode 320 | Timesteps 323736 | Avg Loss: 42.4824 | Actor Loss: -0.0008 | Critic Loss: 42.5047 | Entropy: 1.0767\n",
      "Episode 321 | Timesteps 325205 | Avg Loss: 1.3215 | Actor Loss: -0.0060 | Critic Loss: 1.3490 | Entropy: 1.0750\n",
      "Episode 322 | Timesteps 325297 | Avg Loss: 575.4810 | Actor Loss: 0.0054 | Critic Loss: 575.4971 | Entropy: 1.0726\n",
      "Episode 323 | Timesteps 326897 | Avg Loss: 1.9918 | Actor Loss: -0.0128 | Critic Loss: 2.0261 | Entropy: 1.0729\n",
      "Episode 324 | Timesteps 328497 | Avg Loss: 3.0339 | Actor Loss: -0.0108 | Critic Loss: 3.0661 | Entropy: 1.0722\n",
      "Episode 325 | Timesteps 330097 | Avg Loss: 1.6062 | Actor Loss: -0.0137 | Critic Loss: 1.6414 | Entropy: 1.0718\n",
      "Episode 326 | Timesteps 331697 | Avg Loss: 2.1650 | Actor Loss: -0.0115 | Critic Loss: 2.1980 | Entropy: 1.0712\n",
      "Episode 327 | Timesteps 333297 | Avg Loss: 1.4275 | Actor Loss: -0.0140 | Critic Loss: 1.4628 | Entropy: 1.0680\n",
      "Episode 328 | Timesteps 334897 | Avg Loss: 2.8138 | Actor Loss: -0.0138 | Critic Loss: 2.8489 | Entropy: 1.0651\n",
      "Episode 329 | Timesteps 336497 | Avg Loss: 2.0330 | Actor Loss: -0.0140 | Critic Loss: 2.0683 | Entropy: 1.0648\n",
      "Episode 330 | Average Reward (last 10 episodes): 159.99 | Average Length: 1308.20\n",
      "Episode 330 | Timesteps 338097 | Avg Loss: 1.7472 | Actor Loss: -0.0148 | Critic Loss: 1.7832 | Entropy: 1.0624\n",
      "Episode 331 | Timesteps 338527 | Avg Loss: 51.7392 | Actor Loss: 0.0036 | Critic Loss: 51.7568 | Entropy: 1.0600\n",
      "Episode 332 | Timesteps 339564 | Avg Loss: 33.9712 | Actor Loss: -0.0191 | Critic Loss: 34.0116 | Entropy: 1.0600\n",
      "Episode 333 | Timesteps 340714 | Avg Loss: 9.1986 | Actor Loss: -0.0114 | Critic Loss: 9.2312 | Entropy: 1.0604\n",
      "Episode 334 | Timesteps 342232 | Avg Loss: 22.1722 | Actor Loss: -0.0073 | Critic Loss: 22.2007 | Entropy: 1.0610\n",
      "Episode 335 | Timesteps 343832 | Avg Loss: 2.7701 | Actor Loss: -0.0112 | Critic Loss: 2.8025 | Entropy: 1.0611\n",
      "Episode 336 | Timesteps 345432 | Avg Loss: 2.2460 | Actor Loss: -0.0109 | Critic Loss: 2.2782 | Entropy: 1.0624\n",
      "Episode 337 | Timesteps 346486 | Avg Loss: 7.2206 | Actor Loss: -0.0055 | Critic Loss: 7.2474 | Entropy: 1.0646\n",
      "Episode 338 | Timesteps 348077 | Avg Loss: 1.8623 | Actor Loss: -0.0118 | Critic Loss: 1.8954 | Entropy: 1.0648\n",
      "Episode 339 | Timesteps 348177 | Avg Loss: 622.1961 | Actor Loss: 0.0117 | Critic Loss: 622.2058 | Entropy: 1.0643\n",
      "Episode 340 | Average Reward (last 10 episodes): 102.44 | Average Length: 1168.00\n",
      "Episode 340 | Timesteps 349777 | Avg Loss: 1.5335 | Actor Loss: -0.0124 | Critic Loss: 1.5671 | Entropy: 1.0648\n",
      "Episode 341 | Timesteps 351377 | Avg Loss: 3.0987 | Actor Loss: -0.0134 | Critic Loss: 3.1334 | Entropy: 1.0656\n",
      "Episode 342 | Timesteps 352977 | Avg Loss: 1.8940 | Actor Loss: -0.0123 | Critic Loss: 1.9276 | Entropy: 1.0636\n",
      "Episode 343 | Timesteps 354577 | Avg Loss: 1.4573 | Actor Loss: -0.0164 | Critic Loss: 1.4949 | Entropy: 1.0600\n",
      "Episode 344 | Timesteps 356176 | Avg Loss: 1.9179 | Actor Loss: -0.0118 | Critic Loss: 1.9509 | Entropy: 1.0552\n",
      "Episode 345 | Timesteps 356377 | Avg Loss: 66.1656 | Actor Loss: 0.0152 | Critic Loss: 66.1713 | Entropy: 1.0517\n",
      "Episode 346 | Timesteps 357977 | Avg Loss: 1.2667 | Actor Loss: -0.0112 | Critic Loss: 1.2990 | Entropy: 1.0506\n",
      "Episode 347 | Timesteps 359577 | Avg Loss: 1.3072 | Actor Loss: -0.0131 | Critic Loss: 1.3413 | Entropy: 1.0487\n",
      "Episode 348 | Timesteps 359675 | Avg Loss: 467.5992 | Actor Loss: -0.0054 | Critic Loss: 467.6255 | Entropy: 1.0479\n",
      "Episode 349 | Timesteps 360480 | Avg Loss: 24.5808 | Actor Loss: -0.0019 | Critic Loss: 24.6036 | Entropy: 1.0479\n",
      "Episode 350 | Average Reward (last 10 episodes): 118.72 | Average Length: 1230.30\n",
      "Episode 350 | Timesteps 362080 | Avg Loss: 2.2969 | Actor Loss: -0.0053 | Critic Loss: 2.3231 | Entropy: 1.0473\n",
      "Episode 351 | Timesteps 363680 | Avg Loss: 1.4562 | Actor Loss: -0.0118 | Critic Loss: 1.4889 | Entropy: 1.0462\n",
      "Episode 352 | Timesteps 365223 | Avg Loss: 1.4651 | Actor Loss: -0.0072 | Critic Loss: 1.4932 | Entropy: 1.0450\n",
      "Episode 353 | Timesteps 366823 | Avg Loss: 1.3330 | Actor Loss: -0.0137 | Critic Loss: 1.3676 | Entropy: 1.0420\n",
      "Episode 354 | Timesteps 368423 | Avg Loss: 1.4542 | Actor Loss: -0.0107 | Critic Loss: 1.4858 | Entropy: 1.0408\n",
      "Episode 355 | Timesteps 370023 | Avg Loss: 2.0037 | Actor Loss: -0.0161 | Critic Loss: 2.0406 | Entropy: 1.0396\n",
      "Episode 356 | Timesteps 371623 | Avg Loss: 1.4167 | Actor Loss: -0.0159 | Critic Loss: 1.4534 | Entropy: 1.0377\n",
      "Episode 357 | Timesteps 373223 | Avg Loss: 1.4918 | Actor Loss: -0.0151 | Critic Loss: 1.5276 | Entropy: 1.0357\n",
      "Episode 358 | Timesteps 374823 | Avg Loss: 2.4513 | Actor Loss: -0.0172 | Critic Loss: 2.4892 | Entropy: 1.0343\n",
      "Episode 359 | Timesteps 376423 | Avg Loss: 1.4057 | Actor Loss: -0.0111 | Critic Loss: 1.4375 | Entropy: 1.0324\n",
      "Episode 360 | Average Reward (last 10 episodes): 217.42 | Average Length: 1594.30\n",
      "Episode 360 | Timesteps 378023 | Avg Loss: 1.6657 | Actor Loss: -0.0122 | Critic Loss: 1.6985 | Entropy: 1.0300\n",
      "Episode 361 | Timesteps 379623 | Avg Loss: 2.1289 | Actor Loss: -0.0118 | Critic Loss: 2.1612 | Entropy: 1.0286\n",
      "Episode 362 | Timesteps 381223 | Avg Loss: 2.4570 | Actor Loss: -0.0198 | Critic Loss: 2.4974 | Entropy: 1.0266\n",
      "Episode 363 | Timesteps 382715 | Avg Loss: 13.3607 | Actor Loss: -0.0118 | Critic Loss: 13.3931 | Entropy: 1.0256\n",
      "Episode 364 | Timesteps 384287 | Avg Loss: 1.9697 | Actor Loss: -0.0124 | Critic Loss: 2.0025 | Entropy: 1.0257\n",
      "Episode 365 | Timesteps 385351 | Avg Loss: 19.0282 | Actor Loss: -0.0096 | Critic Loss: 19.0583 | Entropy: 1.0258\n",
      "Episode 366 | Timesteps 385430 | Avg Loss: 717.4057 | Actor Loss: 0.0035 | Critic Loss: 717.4227 | Entropy: 1.0263\n",
      "Episode 367 | Timesteps 387015 | Avg Loss: 2.2573 | Actor Loss: -0.0060 | Critic Loss: 2.2838 | Entropy: 1.0246\n",
      "Episode 368 | Timesteps 388590 | Avg Loss: 2.1302 | Actor Loss: -0.0126 | Critic Loss: 2.1632 | Entropy: 1.0203\n",
      "Episode 369 | Timesteps 390190 | Avg Loss: 1.8175 | Actor Loss: -0.0120 | Critic Loss: 1.8498 | Entropy: 1.0175\n",
      "Episode 370 | Average Reward (last 10 episodes): 165.70 | Average Length: 1376.70\n",
      "Episode 370 | Timesteps 391777 | Avg Loss: 1.8244 | Actor Loss: -0.0144 | Critic Loss: 1.8591 | Entropy: 1.0129\n",
      "Episode 371 | Timesteps 392647 | Avg Loss: 18.1757 | Actor Loss: -0.0101 | Critic Loss: 18.2060 | Entropy: 1.0092\n",
      "Episode 372 | Timesteps 394247 | Avg Loss: 2.3200 | Actor Loss: -0.0166 | Critic Loss: 2.3568 | Entropy: 1.0081\n",
      "Episode 373 | Timesteps 395367 | Avg Loss: 15.0197 | Actor Loss: -0.0106 | Critic Loss: 15.0505 | Entropy: 1.0067\n",
      "Episode 374 | Timesteps 396592 | Avg Loss: 10.4431 | Actor Loss: -0.0158 | Critic Loss: 10.4790 | Entropy: 1.0069\n",
      "Episode 375 | Timesteps 398192 | Avg Loss: 3.4292 | Actor Loss: -0.0088 | Critic Loss: 3.4581 | Entropy: 1.0072\n",
      "Episode 376 | Timesteps 399792 | Avg Loss: 1.5165 | Actor Loss: -0.0146 | Critic Loss: 1.5512 | Entropy: 1.0068\n",
      "Episode 377 | Timesteps 401182 | Avg Loss: 4.1652 | Actor Loss: -0.0153 | Critic Loss: 4.2006 | Entropy: 1.0060\n",
      "Episode 378 | Timesteps 401324 | Avg Loss: 148.7001 | Actor Loss: -0.0234 | Critic Loss: 148.7436 | Entropy: 1.0061\n",
      "Episode 379 | Timesteps 402924 | Avg Loss: 1.7368 | Actor Loss: -0.0117 | Critic Loss: 1.7686 | Entropy: 1.0057\n",
      "Episode 380 | Average Reward (last 10 episodes): 121.28 | Average Length: 1273.40\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ENV_NAME = 'BipedalWalker-v3'\n",
    "HIDDEN_SIZE = 256\n",
    "LEARNING_RATE = 1e-4\n",
    "GAMMA = 0.99\n",
    "LAMBDA = 0.97\n",
    "CLIP_EPSILON = 0.2\n",
    "ENTROPY_COEF = 0.02\n",
    "VALUE_LOSS_COEF = 0.8\n",
    "MAX_GRAD_NORM = 1.0\n",
    "PPO_EPOCHS = 15\n",
    "MINI_BATCH_SIZE = 128\n",
    "TOTAL_EPISODES = 2000\n",
    "ROLLOUT_LENGTH = 2048  # or 4096 if memory allows\n",
    "EVAL_INTERVAL = 100\n",
    "EVAL_EPISODES = 3\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 15\n",
    "min_delta = 1e-2\n",
    "\n",
    "# Device configuration (CPU or GPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Initialize the environment with render_mode for evaluation\n",
    "env = gym.make(ENV_NAME)\n",
    "eval_env = gym.make(ENV_NAME, render_mode='human')  # Set render_mode to 'human' for rendering\n",
    "\n",
    "env.action_space.seed(seed)\n",
    "eval_env.action_space.seed(seed + 1)\n",
    "\n",
    "obs_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.shape[0]\n",
    "action_high = torch.tensor(env.action_space.high).to(device)\n",
    "action_low = torch.tensor(env.action_space.low).to(device)\n",
    "\n",
    "# Define the Actor-Critic Network\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, obs_size, action_size):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        # Common network\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(obs_size, HIDDEN_SIZE),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        # Actor network\n",
    "        self.actor_mean = nn.Sequential(\n",
    "            nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(HIDDEN_SIZE, action_size),\n",
    "            nn.Tanh()  # Assuming action space is bounded between -1 and 1\n",
    "        )\n",
    "        # Actor log_std (learned)\n",
    "        self.actor_log_std = nn.Parameter(torch.zeros(action_size))\n",
    "        # Critic network\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(HIDDEN_SIZE, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        shared_out = self.shared(x)\n",
    "        # Actor\n",
    "        mean = self.actor_mean(shared_out)\n",
    "        std = self.actor_log_std.exp().expand_as(mean)\n",
    "        dist = Normal(mean, std)\n",
    "        # Critic\n",
    "        value = self.critic(shared_out)\n",
    "        return dist, value\n",
    "\n",
    "# Initialize the network and optimizer\n",
    "model = ActorCritic(obs_size, action_size).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Storage for rollouts\n",
    "class RolloutBuffer:\n",
    "    def __init__(self):\n",
    "        self.obs = []\n",
    "        self.actions = []\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        self.values = []\n",
    "\n",
    "    def clear(self):\n",
    "        self.obs = []\n",
    "        self.actions = []\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        self.values = []\n",
    "\n",
    "buffer = RolloutBuffer()\n",
    "\n",
    "# Function to compute Generalized Advantage Estimation (GAE)\n",
    "def compute_gae(next_value, rewards, dones, values):\n",
    "    values = values + [next_value]\n",
    "    gae = 0\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        delta = rewards[step] + GAMMA * values[step + 1] * (1 - dones[step]) - values[step]\n",
    "        gae = delta + GAMMA * LAMBDA * (1 - dones[step]) * gae\n",
    "        returns.insert(0, gae + values[step])\n",
    "    return returns\n",
    "\n",
    "# Function to evaluate the agent\n",
    "def evaluate_policy(model, eval_env, episodes=5):\n",
    "    model.eval()\n",
    "    total_rewards = []\n",
    "    for episode in range(episodes):\n",
    "        state, info = eval_env.reset(seed=seed + episode)\n",
    "        state = torch.FloatTensor(state).to(device)\n",
    "        terminated = truncated = False\n",
    "        episode_reward = 0\n",
    "        while not (terminated or truncated):\n",
    "            with torch.no_grad():\n",
    "                dist, _ = model(state)\n",
    "                action = dist.mean\n",
    "            action_clipped = torch.clamp(action, action_low, action_high)\n",
    "            next_state, reward, terminated, truncated, _ = eval_env.step(action_clipped.cpu().numpy())\n",
    "            # Render the environment\n",
    "            eval_env.render()\n",
    "            state = torch.FloatTensor(next_state).to(device)\n",
    "            episode_reward += reward\n",
    "            time.sleep(0.01)  # Slow down the rendering\n",
    "        total_rewards.append(episode_reward)\n",
    "    model.train()\n",
    "    avg_reward = np.mean(total_rewards)\n",
    "    print(f\"Evaluation over {episodes} episodes: Average Reward = {avg_reward}\")\n",
    "    return avg_reward\n",
    "\n",
    "# Initialize variables for early stopping and tracking\n",
    "best_avg_reward = -np.inf\n",
    "no_improvement_counter = 0\n",
    "all_episode_rewards = []\n",
    "all_episode_lengths = []\n",
    "all_losses = []\n",
    "all_actor_losses = []\n",
    "all_critic_losses = []\n",
    "all_entropies = []\n",
    "all_avg_rewards = []\n",
    "episode_rewards = []\n",
    "episode_lengths = []\n",
    "total_timesteps = 0\n",
    "next_eval = EVAL_INTERVAL\n",
    "episode_count = 0\n",
    "\n",
    "while episode_count < TOTAL_EPISODES:\n",
    "    state, info = env.reset(seed=seed + episode_count)\n",
    "    state = torch.FloatTensor(state).to(device)\n",
    "    episode_reward = 0\n",
    "    episode_length = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        dist, value = model(state)\n",
    "        action = dist.sample()\n",
    "        action_clipped = torch.clamp(action, action_low, action_high)\n",
    "        log_prob = dist.log_prob(action).sum(dim=-1)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action_clipped.cpu().numpy())\n",
    "        next_state = torch.FloatTensor(next_state).to(device)\n",
    "        done = terminated or truncated\n",
    "        # Store in buffer (detach tensors to prevent retaining computational graph)\n",
    "        buffer.obs.append(state)\n",
    "        buffer.actions.append(action.detach())  # Detach action\n",
    "        buffer.log_probs.append(log_prob.detach())  # Detach log_prob\n",
    "        buffer.rewards.append(reward)\n",
    "        buffer.dones.append(done)\n",
    "        buffer.values.append(value.detach().squeeze())\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "        episode_length += 1\n",
    "        total_timesteps += 1\n",
    "\n",
    "        # Check if it's time to update the policy\n",
    "        if len(buffer.rewards) >= ROLLOUT_LENGTH or done:\n",
    "            # Compute next value\n",
    "            with torch.no_grad():\n",
    "                _, next_value = model(state)\n",
    "            next_value = next_value.detach().squeeze()\n",
    "            # Compute returns and advantages\n",
    "            returns = compute_gae(next_value, buffer.rewards, buffer.dones, buffer.values)\n",
    "            advantages = [ret - val for ret, val in zip(returns, buffer.values)]\n",
    "            advantages = torch.tensor(advantages, dtype=torch.float32).to(device)\n",
    "            returns = torch.tensor(returns, dtype=torch.float32).to(device)\n",
    "\n",
    "            # Flatten the buffers\n",
    "            obs_tensor = torch.stack(buffer.obs)\n",
    "            actions_tensor = torch.stack(buffer.actions)\n",
    "            log_probs_tensor = torch.stack(buffer.log_probs)\n",
    "            values_tensor = torch.stack(buffer.values).to(device)\n",
    "            # Clear buffer\n",
    "            buffer.clear()\n",
    "\n",
    "            # Normalize advantages\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "            # PPO Optimization step\n",
    "            total_loss = 0\n",
    "            total_actor_loss = 0\n",
    "            total_critic_loss = 0\n",
    "            total_entropy = 0\n",
    "            num_updates = 0\n",
    "            for _ in range(PPO_EPOCHS):\n",
    "                # Create mini-batches\n",
    "                indices = np.arange(len(obs_tensor))\n",
    "                np.random.shuffle(indices)\n",
    "                for start in range(0, len(obs_tensor), MINI_BATCH_SIZE):\n",
    "                    end = start + MINI_BATCH_SIZE\n",
    "                    mini_batch_indices = indices[start:end]\n",
    "                    mb_obs = obs_tensor[mini_batch_indices]\n",
    "                    mb_actions = actions_tensor[mini_batch_indices]\n",
    "                    mb_log_probs = log_probs_tensor[mini_batch_indices]\n",
    "                    mb_returns = returns[mini_batch_indices]\n",
    "                    mb_advantages = advantages[mini_batch_indices]\n",
    "                    # Forward pass\n",
    "                    dist, value = model(mb_obs)\n",
    "                    entropy = dist.entropy().mean()\n",
    "                    new_log_probs = dist.log_prob(mb_actions).sum(dim=-1)\n",
    "                    # Ratio for clipping\n",
    "                    ratio = (new_log_probs - mb_log_probs).exp()\n",
    "                    surr1 = ratio * mb_advantages\n",
    "                    surr2 = torch.clamp(ratio, 1.0 - CLIP_EPSILON, 1.0 + CLIP_EPSILON) * mb_advantages\n",
    "                    actor_loss = -torch.min(surr1, surr2).mean()\n",
    "                    critic_loss = VALUE_LOSS_COEF * (mb_returns - value.squeeze()).pow(2).mean()\n",
    "                    loss = actor_loss + critic_loss - ENTROPY_COEF * entropy\n",
    "                    # Backpropagation\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
    "                    optimizer.step()\n",
    "                    # Accumulate losses\n",
    "                    total_loss += loss.item()\n",
    "                    total_actor_loss += actor_loss.item()\n",
    "                    total_critic_loss += critic_loss.item()\n",
    "                    total_entropy += entropy.item()\n",
    "                    num_updates += 1\n",
    "\n",
    "            # Compute average losses\n",
    "            avg_loss = total_loss / num_updates\n",
    "            avg_actor_loss = total_actor_loss / num_updates\n",
    "            avg_critic_loss = total_critic_loss / num_updates\n",
    "            avg_entropy = total_entropy / num_updates\n",
    "\n",
    "            # Store losses\n",
    "            all_losses.append(avg_loss)\n",
    "            all_actor_losses.append(avg_actor_loss)\n",
    "            all_critic_losses.append(avg_critic_loss)\n",
    "            all_entropies.append(avg_entropy)\n",
    "\n",
    "            # Verbose logging\n",
    "            print(f\"Episode {episode_count} | Timesteps {total_timesteps} | Avg Loss: {avg_loss:.4f} | \"\n",
    "                  f\"Actor Loss: {avg_actor_loss:.4f} | Critic Loss: {avg_critic_loss:.4f} | \"\n",
    "                  f\"Entropy: {avg_entropy:.4f}\")\n",
    "\n",
    "    episode_rewards.append(episode_reward)\n",
    "    episode_lengths.append(episode_length)\n",
    "    episode_count += 1\n",
    "\n",
    "    # Print average reward every 10 episodes\n",
    "    if episode_count % 10 == 0:\n",
    "        avg_reward = np.mean(episode_rewards[-10:])\n",
    "        avg_length = np.mean(episode_lengths[-10:])\n",
    "        print(f\"Episode {episode_count} | Average Reward (last 10 episodes): {avg_reward:.2f} | \"\n",
    "              f\"Average Length: {avg_length:.2f}\")\n",
    "\n",
    "    # Evaluate the agent periodically\n",
    "    if episode_count % EVAL_INTERVAL == 0:\n",
    "        print(f\"\\nEvaluating at episode {episode_count}...\")\n",
    "        avg_reward = evaluate_policy(model, eval_env, episodes=EVAL_EPISODES)\n",
    "        all_avg_rewards.append(avg_reward)\n",
    "\n",
    "        # Early stopping and model saving\n",
    "        if avg_reward > best_avg_reward + min_delta:\n",
    "            best_avg_reward = avg_reward\n",
    "            no_improvement_counter = 0\n",
    "            # Save the model\n",
    "            torch.save(model.state_dict(), f'best_model_episode_{episode_count}.pth')\n",
    "            print(f\"Best model saved with average reward {best_avg_reward} at episode {episode_count}\")\n",
    "        else:\n",
    "            no_improvement_counter += 1\n",
    "            print(f\"No improvement for {no_improvement_counter} evaluation(s)\")\n",
    "\n",
    "        if no_improvement_counter >= patience:\n",
    "            print(f\"Early stopping at episode {episode_count} due to no improvement in average reward\")\n",
    "            break\n",
    "        print()\n",
    "\n",
    "env.close()\n",
    "eval_env.close()\n",
    "\n",
    "# Function to compute moving average\n",
    "def moving_average(data, window_size):\n",
    "    return np.convolve(data, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "# Plotting the results\n",
    "window_size = 10  # You can adjust the window size for averaging\n",
    "episodes = np.arange(len(episode_rewards))\n",
    "updates = np.arange(len(all_losses))\n",
    "\n",
    "# Compute moving averages\n",
    "avg_episode_rewards = moving_average(episode_rewards, window_size)\n",
    "avg_losses = moving_average(all_losses, window_size)\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "\n",
    "# Plot episode rewards and average rewards\n",
    "ax1.plot(episodes, episode_rewards, label='Episode Reward')\n",
    "ax1.plot(episodes[window_size-1:], avg_episode_rewards, label='Average Reward', linewidth=2)\n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_ylabel('Reward')\n",
    "ax1.set_title('Episode Reward Over Time')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot losses and average losses\n",
    "ax2.plot(updates, all_losses, label='Loss')\n",
    "ax2.plot(updates[window_size-1:], avg_losses, label='Average Loss', linewidth=2)\n",
    "ax2.set_xlabel('Update')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.set_title('Loss Over Time')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('rewards_and_losses.png')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
