{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "ename": "DependencyNotInstalled",
     "evalue": "Box2D is not installed, you can install it by run `pip install swig` followed by `pip install \"gymnasium[box2d]\"`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\gymnasium\\envs\\box2d\\bipedal_walker.py:15\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 15\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mBox2D\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mBox2D\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mb2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     17\u001b[0m         circleShape,\n\u001b[0;32m     18\u001b[0m         contactListener,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m         revoluteJointDef,\n\u001b[0;32m     23\u001b[0m     )\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'Box2D'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mDependencyNotInstalled\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 264\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;66;03m# Train agent\u001b[39;00m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 264\u001b[0m agent \u001b[38;5;241m=\u001b[39m train(render_every\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)  \u001b[38;5;66;03m# Show every 20 episodes\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# Evaluate agent\u001b[39;00m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEvaluating trained agent...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 138\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(render_every, total_episodes)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(render_every\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, total_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m):\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;66;03m# Create environment\u001b[39;00m\n\u001b[1;32m--> 138\u001b[0m     env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBipedalWalker-v3\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;66;03m# Initialize agent\u001b[39;00m\n\u001b[0;32m    141\u001b[0m     state_dim \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\gymnasium\\envs\\registration.py:702\u001b[0m, in \u001b[0;36mmake\u001b[1;34m(id, max_episode_steps, disable_env_checker, **kwargs)\u001b[0m\n\u001b[0;32m    699\u001b[0m     env_creator \u001b[38;5;241m=\u001b[39m env_spec\u001b[38;5;241m.\u001b[39mentry_point\n\u001b[0;32m    700\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    701\u001b[0m     \u001b[38;5;66;03m# Assume it's a string\u001b[39;00m\n\u001b[1;32m--> 702\u001b[0m     env_creator \u001b[38;5;241m=\u001b[39m load_env_creator(env_spec\u001b[38;5;241m.\u001b[39mentry_point)\n\u001b[0;32m    704\u001b[0m \u001b[38;5;66;03m# Determine if to use the rendering\u001b[39;00m\n\u001b[0;32m    705\u001b[0m render_modes: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\gymnasium\\envs\\registration.py:549\u001b[0m, in \u001b[0;36mload_env_creator\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Loads an environment with name of style ``\"(import path):(environment name)\"`` and returns the environment creation function, normally the environment class type.\u001b[39;00m\n\u001b[0;32m    541\u001b[0m \n\u001b[0;32m    542\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    546\u001b[0m \u001b[38;5;124;03m    The environment constructor for the given environment name.\u001b[39;00m\n\u001b[0;32m    547\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    548\u001b[0m mod_name, attr_name \u001b[38;5;241m=\u001b[39m name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 549\u001b[0m mod \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(mod_name)\n\u001b[0;32m    550\u001b[0m fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(mod, attr_name)\n\u001b[0;32m    551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\importlib\\__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _bootstrap\u001b[38;5;241m.\u001b[39m_gcd_import(name[level:], package, level)\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1310\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:488\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:995\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:488\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\gymnasium\\envs\\box2d\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbox2d\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbipedal_walker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BipedalWalker, BipedalWalkerHardcore\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbox2d\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcar_racing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CarRacing\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbox2d\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlunar_lander\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LunarLander, LunarLanderContinuous\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\gymnasium\\envs\\box2d\\bipedal_walker.py:25\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mBox2D\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mb2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     17\u001b[0m         circleShape,\n\u001b[0;32m     18\u001b[0m         contactListener,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m         revoluteJointDef,\n\u001b[0;32m     23\u001b[0m     )\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 25\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DependencyNotInstalled(\n\u001b[0;32m     26\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBox2D is not installed, you can install it by run `pip install swig` followed by `pip install \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgymnasium[box2d]\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     27\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpygame\u001b[39;00m\n",
      "\u001b[1;31mDependencyNotInstalled\u001b[0m: Box2D is not installed, you can install it by run `pip install swig` followed by `pip install \"gymnasium[box2d]\"`"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "# Set environment variables to handle PyGame display better\n",
    "os.environ['SDL_VIDEODRIVER'] = 'windib'  # Use Windows driver\n",
    "os.environ['SDL_WINDOW_CENTERED'] = '1'\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
    "        return (np.array(state), np.array(action), np.array(reward), \n",
    "                np.array(next_state), np.array(done))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, action_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        return self.net(state)\n",
    "\n",
    "class DQN:\n",
    "    def __init__(self, state_dim, action_dim, discretization=5):\n",
    "        self.action_dim = action_dim\n",
    "        self.discretization = discretization\n",
    "        self.discrete_actions = self._create_discrete_actions()\n",
    "        \n",
    "        # Calculate total number of discrete actions\n",
    "        self.total_actions = discretization ** action_dim\n",
    "        \n",
    "        # Networks\n",
    "        self.q_network = QNetwork(state_dim, self.total_actions)\n",
    "        self.target_network = QNetwork(state_dim, self.total_actions)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = torch.optim.Adam(self.q_network.parameters(), lr=1e-3)\n",
    "        \n",
    "        # Replay buffer\n",
    "        self.replay_buffer = ReplayBuffer(100000)\n",
    "        \n",
    "        # Parameters\n",
    "        self.batch_size = 128\n",
    "        self.gamma = 0.99\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.target_update_freq = 10\n",
    "        self.learning_starts = 1000\n",
    "        \n",
    "        # Training history\n",
    "        self.rewards_history = []\n",
    "        self.avg_rewards_history = []\n",
    "    \n",
    "    def _create_discrete_actions(self):\n",
    "        \"\"\"Create a discrete action space by discretizing each continuous dimension\"\"\"\n",
    "        action_space = np.linspace(-1, 1, self.discretization)\n",
    "        # Create all combinations of discrete actions\n",
    "        action_combinations = np.array(np.meshgrid(*[action_space] * self.action_dim))\n",
    "        return action_combinations.T.reshape(-1, self.action_dim)\n",
    "    \n",
    "    def _discrete_to_continuous(self, discrete_action):\n",
    "        \"\"\"Convert discrete action index to continuous action vector\"\"\"\n",
    "        return self.discrete_actions[discrete_action]\n",
    "    \n",
    "    def get_action(self, state, training=True):\n",
    "        if training and random.random() < self.epsilon:\n",
    "            action_idx = random.randrange(self.total_actions)\n",
    "        else:\n",
    "            state = torch.FloatTensor(state).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                q_values = self.q_network(state)\n",
    "                action_idx = q_values.argmax().item()\n",
    "        \n",
    "        return self._discrete_to_continuous(action_idx), action_idx\n",
    "    \n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        # Store transition in replay buffer\n",
    "        self.replay_buffer.push(state, action, reward, next_state, done)\n",
    "        \n",
    "        if len(self.replay_buffer) < self.learning_starts:\n",
    "            return\n",
    "        \n",
    "        # Sample from replay buffer\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
    "        \n",
    "        # Convert to torch tensors\n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.LongTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        next_states = torch.FloatTensor(next_states)\n",
    "        dones = torch.FloatTensor(dones)\n",
    "        \n",
    "        # Compute current Q values\n",
    "        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))\n",
    "        \n",
    "        # Compute next Q values\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_network(next_states).max(1)[0]\n",
    "            target_q_values = rewards + (1 - dones) * self.gamma * next_q_values\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = nn.MSELoss()(current_q_values.squeeze(), target_q_values)\n",
    "        \n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Update epsilon\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "def train(render_every=20, total_episodes=1000):\n",
    "    # Create environment\n",
    "    env = gym.make('BipedalWalker-v3')\n",
    "    \n",
    "    # Initialize agent\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    agent = DQN(state_dim, action_dim)\n",
    "    \n",
    "    # Training loop\n",
    "    best_reward = float('-inf')\n",
    "    episode_rewards = []\n",
    "    update_count = 0\n",
    "    \n",
    "    try:\n",
    "        for episode in range(total_episodes):\n",
    "            state, _ = env.reset()\n",
    "            episode_reward = 0\n",
    "            \n",
    "            # Create display environment if needed\n",
    "            if episode % render_every == 0:\n",
    "                display_env = gym.make('BipedalWalker-v3', render_mode='human')\n",
    "                display_state, _ = display_env.reset()\n",
    "            \n",
    "            while True:\n",
    "                # Get action from agent\n",
    "                action, action_idx = agent.get_action(state)\n",
    "                \n",
    "                # Take step in environment\n",
    "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "                done = terminated or truncated\n",
    "                \n",
    "                # Update agent\n",
    "                agent.update(state, action_idx, reward, next_state, done)\n",
    "                update_count += 1\n",
    "                \n",
    "                # Update target network\n",
    "                if update_count % agent.target_update_freq == 0:\n",
    "                    agent.target_network.load_state_dict(agent.q_network.state_dict())\n",
    "                \n",
    "                episode_reward += reward\n",
    "                state = next_state\n",
    "                \n",
    "                # Display if needed\n",
    "                if episode % render_every == 0:\n",
    "                    display_action, _ = agent.get_action(display_state, training=False)\n",
    "                    display_state, _, terminated, truncated, _ = display_env.step(display_action)\n",
    "                    if terminated or truncated:\n",
    "                        display_env.close()\n",
    "                        break\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            # Store reward\n",
    "            episode_rewards.append(episode_reward)\n",
    "            avg_reward = np.mean(episode_rewards[-100:])\n",
    "            \n",
    "            # Print progress\n",
    "            print(f\"Episode {episode + 1}\")\n",
    "            print(f\"Reward: {episode_reward:.2f}\")\n",
    "            print(f\"Average Reward (last 100): {avg_reward:.2f}\")\n",
    "            print(f\"Epsilon: {agent.epsilon:.3f}\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            # Plot progress\n",
    "            if (episode + 1) % 10 == 0:\n",
    "                plt.figure(figsize=(10, 5))\n",
    "                plt.plot(episode_rewards)\n",
    "                plt.title(\"Training Progress\")\n",
    "                plt.xlabel(\"Episode\")\n",
    "                plt.ylabel(\"Reward\")\n",
    "                plt.savefig(\"training_progress.png\")\n",
    "                plt.close()\n",
    "            \n",
    "            # Save best model\n",
    "            if avg_reward > best_reward:\n",
    "                best_reward = avg_reward\n",
    "                torch.save({\n",
    "                    'q_network_state_dict': agent.q_network.state_dict(),\n",
    "                    'target_network_state_dict': agent.target_network.state_dict(),\n",
    "                    'reward': best_reward\n",
    "                }, 'best_model.pth')\n",
    "            \n",
    "            # Early stopping\n",
    "            if avg_reward >= 300:\n",
    "                print(\"Environment solved!\")\n",
    "                break\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nTraining interrupted by user\")\n",
    "    \n",
    "    finally:\n",
    "        env.close()\n",
    "        if 'display_env' in locals():\n",
    "            display_env.close()\n",
    "    \n",
    "    return agent\n",
    "\n",
    "def evaluate(agent, episodes=5):\n",
    "    env = gym.make('BipedalWalker-v3', render_mode='human')\n",
    "    \n",
    "    try:\n",
    "        for episode in range(episodes):\n",
    "            state, _ = env.reset()\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "            \n",
    "            while not done:\n",
    "                action, _ = agent.get_action(state, training=False)\n",
    "                state, reward, terminated, truncated, _ = env.step(action)\n",
    "                done = terminated or truncated\n",
    "                total_reward += reward\n",
    "                time.sleep(0.01)  # Slow down visualization\n",
    "            \n",
    "            print(f\"Episode {episode + 1} Reward: {total_reward:.2f}\")\n",
    "    \n",
    "    finally:\n",
    "        env.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set random seeds\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    random.seed(42)\n",
    "    \n",
    "    # Train agent\n",
    "    print(\"Starting training...\")\n",
    "    agent = train(render_every=20)  # Show every 20 episodes\n",
    "    \n",
    "    # Evaluate agent\n",
    "    print(\"\\nEvaluating trained agent...\")\n",
    "    evaluate(agent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
